---
title: "Ensemble Methods for Regression"
subtitle: "IN2004B: Generation of Value with Data Analytics"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: false
    footer: "Tecnologico de Monterrey"
    logo: IN2004B_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Agenda

</br>

1.  Introduction
2.  Bagging
3.  Random Forests
4.  Boosting
5.  Ensemble Methods for Time Series

# Introduction 

## Load the libraries

</br>

Before we start, let's import the data science libraries into Python.

```{python}
#| echo: true
#| output: false

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.ensemble import  GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.inspection import permutation_importance
```

Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.

## Decision trees

</br>

:::::: columns
:::: {.column width="45%"}
::: {style="font-size: 90%;"}
-   Simple and useful for interpretations.

-   Can handle continuous and categorical predictors and responses. So, they can be applied to both [**classification**]{style="color:blue;"} and [**regression**]{style="color:green;"} problems.

-   Computationally efficient.
:::
::::

::: {.column width="55%"}
</br>

![](images/clipboard-2280895928.png){fig-align="center"}
:::
::::::

## Decision trees for regression

</br></br>

- The procedure for constructing a decision tree for regression is the same as for a classification tree. 

- That is, we use the CART algorithm for building large trees and cost complexity pruning to reduce them, if desired.

- However, instead of using impurity to evaluate splits, we use the mean squared error.

##

</br></br>

To compute the mean squared error we use

  - The predicted response is the average response calculated in the node or region.

  - The actual responses are those of the observations in the region. 

We refer to decision trees for regression as **regression trees**.

## Example 1

</br></br>

The "Hitters.xlsx" dataset contains data collected from Major League Baseball players. It includes data on various statistics such as number of hits, years in the league, and salary.

The **goal** is to predict a player's salary based on performance statistics.

##

</br></br>

The [response]{style="color:darkred;"} is the player's salary (in \$), contained in the column `Salary`. 

For this example, we will focus on two predictors: 

- `Hits`: Number of hits in the season  
- `Years`: Number of years the player has been in the league


## Read the dataset

We read the dataset

```{python}
#| echo: true
#| output: true

Hitters_data = pd.read_excel('Hitters.xlsx')
Hitters_data.head()
```

##

The data set has some missing observations. So, we remove the observations with at least one missing value in a predictor.

```{python}
#| echo: true
#| output: true

Hitters_data_clean = Hitters_data.dropna()
Hitters_data_clean.head()
```

## Generating predictors in Python

</br></br>

Select the predictors and response

```{python}
#| echo: true
#| output: true

X_full = Hitters_data_clean.filter(["Years", "Hits"])
Y_full = Hitters_data_clean.filter(["Salary"])
```

We partition the full dataset into 70% for training and the other 30% for validation.

```{python}
#| echo: true
#| output: true

# Split the dataset into training and validation.
X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, 
                                                      test_size = 0.3,
                                                      random_state = 507134)
```


## Regression trees in Python

</br></br>

We train a regression tree using the `DecisionTreeRegressor()` function from regression.

```{python}
#| echo: true
#| output: false

# We tell Python we want a regression tree
reg_tree = DecisionTreeRegressor(min_samples_leaf = 5, max_depth=3, 
                              random_state=507134)

# We train the regression tree using the training data.
reg_tree.fit(X_train, Y_train)
```

We use `max_depth = 3` to have a small tree as an example.

## Plotting the tree

</br>

To see the decision tree, we use the `plot_tree` function from **scikit-learn** and some commands from **matplotlib**. 

```{python}
#| echo: true
#| output: false
#| fig-align: center

plt.figure(figsize=(6, 6))
plot_tree(reg_tree, feature_names = X_train.columns, filled=True, 
          rounded=True)
plt.show()
```

## 

</br>

```{python}
#| echo: false
#| output: true
#| fig-align: center

plt.figure(figsize=(14, 7))
plot_tree(reg_tree, feature_names = X_train.columns, filled=True, 
          rounded=True)
plt.show()
```

## Visualize the regions

```{python}
#| fig-align: center
#| 
# Set plot style
sns.set(style="whitegrid")


train_data = pd.concat([X_train, Y_train], axis = 1)
# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6, 6))
sns.scatterplot(
    data=train_data,
    x='Years',
    y='Hits',
    s=15,
    edgecolor=None,
    legend='full'
)

# Add decision boundaries
plt.axhline(y=122.5, color='black', linewidth=3)     # Horizontal line
plt.axhline(y=72, xmin=0.230, color='black', linewidth=3)  # Left vertical
plt.axvline(x=5.5, ymax=0.538, color='black', linewidth=3)  # Left vertical
plt.axvline(x=3.5, color='black', linewidth=3)  # Right vertical
plt.axvline(x=12.5, ymin=0.538, color='black', linewidth=3)  # Right vertical

# Add region labels
plt.text(1, 200, r"$R_1$", fontsize=30, fontweight='bold')
plt.text(1, 50, r"$R_2$", fontsize=30, fontweight='bold')
plt.text(6, 200, r"$R_3$", fontsize=30, fontweight='bold')
plt.text(20, 200, r"$R_4$", fontsize=30, fontweight='bold')
plt.text(20, 100, r"$R_5$", fontsize=30, fontweight='bold')
plt.text(20, 20, r"$R_6$", fontsize=30, fontweight='bold')
plt.text(4.0, 3, r"$R_7$", fontsize=20, fontweight='bold')

# Clean layout
plt.tight_layout()
plt.show()
```


## Visualize the predictions

```{python}
#| fig-align: center
#| 
# Set plot style
sns.set(style="whitegrid")


train_data = pd.concat([X_train, Y_train], axis = 1)
# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6, 6))
sns.scatterplot(
    data=train_data,
    x='Years',
    y='Hits',
    s=15,
    edgecolor=None,
    legend='full'
)

# Add decision boundaries
plt.axhline(y=122.5, color='black', linewidth=3)     # Horizontal line
plt.axhline(y=72, xmin=0.230, color='black', linewidth=3)  # Left vertical
plt.axvline(x=5.5, ymax=0.538, color='black', linewidth=3)  # Left vertical
plt.axvline(x=3.5, color='black', linewidth=3)  # Right vertical
plt.axvline(x=12.5, ymin=0.538, color='black', linewidth=3)  # Right vertical

# Add region labels
plt.text(1, 200, r"233.3", fontsize=15, fontweight='bold')
plt.text(1, 50, r"140.0", fontsize=15, fontweight='bold')
plt.text(6, 200, r"917.8", fontsize=15, fontweight='bold')
plt.text(20, 200, r"1190.4", fontsize=15, fontweight='bold')
plt.text(20, 100, r"601.9", fontsize=15, fontweight='bold')
plt.text(20, 20, r"371.6", fontsize=15, fontweight='bold')
plt.text(4.0, 3, r"286.9", fontsize=8, fontweight='bold')

# Clean layout
plt.tight_layout()
plt.show()
```


##  Predictions on the validation data

To predict the responses on the validation data, we use the function `.predict()` using the predictor values in the validation dataset contained in `X_valid`.

```{python}
#| echo: true
#| output: true

Y_pred = reg_tree.predict(X_valid)
Y_pred
```

## Validation RMSE

</br></br>

We compute the *root mean squared error* on the validation data using the `mse()` function. Recall that the responses from the validation dataset are in `Y_valid`, and the model predictions are in `Y_pred`.

```{python}
#| echo: true
#| output: true
#| fig-align: center
#| code-fold: false

mse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)
print(round(mse**(1/2), 2)) # RMSE
```


## Limitations of decision trees

</br></br></br>

-   In general, decision trees do not work well for classification and regression problems.

-   However, they can be combined to build effective algorithms for these problems.

## Ensamble methods

</br></br>

Ensemble methods are frameworks to combine decision trees.

Here, we will cover two popular ensamble methods:

-   [**Bagging**]{style="color:purple;"}. Ensemble many deep trees.

    -   Quintessential method: [Random Forests]{style="color:purple;"}.

-   [**Boosting**]{style="color:brown;"}. Ensemble small trees sequentially.


# Bagging

## Bootstrap samples

::: {style="font-size: 90%;"}
Bootstrap samples are samples obtained *with replacement* from the original sample. So, an observation can occur more than one in a bootstrap sample.

Bootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.
:::

![](images/clipboard-2003681619.png){fig-align="center"}

## Bagging

Given a training dataset, [**bagging**]{style="color:purple;"} averages the predictions from decision trees over a collection of ***bootstrap*** samples.

![](images/bagging_scheme.png){fig-align="center"}

## Predictions

Let $\boldsymbol{x} = (x_1, x_2, \ldots, x_p)$ be a vector of new predictor values.

For [**regression**]{style="color:darkgreen;"} problems:

1.  Each regression tree outputs the average value depending on the region $\boldsymbol{x}$ falls in. For the b-th tree, denote such average as $\hat{T}_{b}(\boldsymbol{x})$.

2.  Predict using the average of all regression trees

$$\hat{f}_{bag}(\boldsymbol{x}) = \frac{1}{B} \sum_{b=1}^{B} \hat{T}_{b}(\boldsymbol{x}).  $$

## Implementation

</br></br>

::: incremental
-   How many trees? No risk of overfitting, so use plenty.

-   No pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.

-   The size of bootstrap samples is the same as the size of the training dataset, but we can use a different size.
:::

## Example 2

</br>

The "BostonHousing.xlsx" contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.

The **goal** is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.

The [response]{style="color:darkred;"} is the median value of owner-occupied homes in \$1000s, contained in the column `MEDV`.

## The predictors

::: {style="font-size: 70%;"}
-   `CRIM`: per capita crime rate by town.
-   `ZN`: proportion of residential land zoned for lots over 25,000 sq.ft.
-   `INDUS`: proportion of non-retail business acres per town.
-   `CHAS`: Charles River ('Yes' if tract bounds river; 'No' otherwise).
-   `NOX`: nitrogen oxides concentration (parts per 10 million).
-   `RM`: average number of rooms per dwelling.
-   `AGE`: proportion of owner-occupied units built prior to 1940.
-   `DIS`: weighted mean of distances to five Boston employment centers
-   `RAD`: index of accessibility to radial highways ('Low', 'Medium', 'High').
-   `TAX`: full-value property-tax rate per \$10,000.
-   `PTRATIO`: pupil-teacher ratio by town.
-   `LSTAT`: lower status of the population (percent).
:::

## Read the dataset

</br>

We read the dataset and set the variable `CHAS` and `RAD` as categorical.

```{python}
#| echo: true
#| output: true

Boston_data = pd.read_excel('BostonHousing.xlsx')

Boston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])
Boston_data['RAD'] = pd.Categorical(Boston_data['RAD'], 
                                      categories=["Low", "Medium", "High"], 
                                      ordered=True)
```

## 

</br>

```{python}
#| echo: true
#| output: true

Boston_data.head()
```

## Generating predictors in Python

First, we use the function `.drop()` from **pandas** to drop the response column `MEDV` and store the result in `X_full`.

```{python}
#| echo: true
#| output: true

# Set full matrix of predictors.
X_full = Boston_data.drop(columns = ['MEDV']) 
```

Unfortunately, bagging does not work with categorical predictors such as `CHAS` and `RAD`. So, we must transform them into dummy variables using the code below.

```{python}
#| echo: true
#| output: false

# Turn categorical predictors into dummy variables.
X_dummies = pd.get_dummies(X_full[['CHAS', 'RAD']], drop_first = True, dtype = 'int')

# Drop original predictors from the dataset.
X_other = X_full.drop(['CHAS', 'RAD'], axis=1)

# Update the predictor matrix.
X_full = pd.concat([X_other, X_dummies], axis=1)
```

## 

Next, we use the function `.filter()` from **pandas** to extract the column `MEDV` from the data frame. We store the result in `Y_full`.

```{python}
#| echo: true
#| output: true

# Set full matrix of responses.
Y_full = Boston_data.filter(['MEDV'])
```

We partition the full dataset into 80% for training and the other 20% for validation.

```{python}
#| echo: true
#| output: true

# Split the dataset into training and validation.
X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, 
                                                      test_size = 0.2,
                                                      random_state = 507134)
```




## Bagging in Python

</br>

We define a bagging algorithm for regression using the `BaggingRegressor` function from **scikit-learn**.

The `n_estimators` argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.

```{python}
#| echo: true
#| output: false

# Set the bagging algorithm.
Baggingalgorithm = BaggingRegressor(n_estimators = 500, 
                                     random_state = 59227)

# Train the bagging algorithm.
Baggingalgorithm.fit(X_train, Y_train)
```

`random_state` allows us to obtain the same bagging algorithm in different runs of the algorithm.

## Predictions on the validation data

To predict the responses on the validation data, we use the function `.predict()` using the predictor values in the validation dataset contained in `X_valid`.

```{python}
#| echo: true
#| output: true

Y_pred = Baggingalgorithm.predict(X_valid)
Y_pred
```

## Validation RMSE

</br></br>

We compute the *root mean squared error* on the validation data using the `mse()` function. 

```{python}
#| echo: true
#| output: true
#| fig-align: center
#| code-fold: false

mse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)
print(round(mse**(1/2), 2)) # RMSE
```


## A single deep tree

To compare the bagging, let's use a single deep tree.

```{python}
#| echo: true
#| output: false
#| code-fold: false

# We tell Python that we want a classification tree
reg_tree = DecisionTreeRegressor(ccp_alpha=0.0,
                                 random_state=507134)

# We train the classification tree using the training data.
reg_tree.fit(X_train, Y_train)
```

Let's compute the validation RMSE of the tree.

```{python}
#| echo: true
#| output: true

reg_tree_Y_pred = reg_tree.predict(X_valid)
mse = mean_squared_error(Y_valid, reg_tree_Y_pred) 
print(round(mse**(1/2), 2)) 
```

## Advantages

</br></br>

-   Bagging will have lower prediction errors than a single regression tree.

-   The fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification. This estimate is called the [out-of-bag error]{.underline} estimate which is an estimate of the RMSE on the test dataset.

## Limitations

</br>

-   *Loss of interpretability*: the final bagged classifier is [not a tree]{style="color:darkred;"}, and so we forfeit the clear interpretative ability of a classification tree.

-   *Computational complexity*: we are essentially multiplying the work of growing (and possibly pruning) a single tree by $B$.

-   *Fundamental issue*: bagging a good model can improve predictive performance, but bagging a bad one can seriously degrade it.

## Other issues

</br></br>

-   Suppose a variable is very important and decisive.

    -   It will probably appear near the top of a large number of trees.

    -   And these trees will tend to vote the same way.

    -   In some sense, then, many of the trees are “correlated”.

    -   This will degrade the performance of bagging.

## 

-   Bagging is unable to capture simple decision boundaries

![](images/bagging_decision_boundary.png){fig-align="center"}

# Random Forest

## Random Forest

</br>

Exactly as bagging, but...

-   When splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a [*random sample of a small number of variables* $m$]{style="color:brown;"}, where $m < p$.

Random forests can improve the performance of bagging.

## Why does it work?

</br>

-   Not so dominant predictors will get a chance to appear by themselves and show “their stuff”.

-   This adds more diversity to the trees.

-   The fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall.

## Tuning parameter

</br>

How do we set $m$?

-   For regression, can use $m = \sqrt{p}$ and a minimum leaf node size of 5.

In practice, sometimes the best values for these parameters will depend on the problem. So, we can treat $m$ as a tuning parameter.

> Note that if $m = p$, we get bagging.

## The final product is a black box

![](images/clipboard-2376484681.png){fig-align="center"}

-   A black box. Inside the box are several hundred trees, each slightly different.

-   You put an observation into the black box, and the black box classifies it or predicts it for you.

## Random Forest in Python

In Python, we define a RandomForest algorithm for classification using the `RandomForestRegressor` function from **scikit-learn**. 

```{python}
#| echo: true
#| output: false

# Set the random forest algorithm.
RFalgorithm = RandomForestRegressor(n_estimators = 500,
                                    min_samples_leaf = 5,
                                    max_features = "sqrt",
                                    random_state = 59227)

# Train the random forest algorithm.
RFalgorithm.fit(X_train, Y_train)
```

`n_estimators` sets the number of decision trees to use, `min_samples_leaf` sets the minimum size for the terminal nodes, and `max_features` sets the maximum number of predictors to try in each split.

## Validation RMSE

</br></br>

Evaluate the performance of random forest.

```{python}
#| echo: true
#| output: true
#| code-fold: false
#| fig-align: center

# Predict responses in validation data.
RF_predicted = RFalgorithm.predict(X_valid)

# Compute RMSE.
mse = mean_squared_error(Y_valid, RF_predicted) 
print(round(mse**(1/2), 2))
```

## Predictor importance

</br></br>

- Random Forests and bagged trees are a black box. But “Importance” is a statistic that attempts to give us some insight.

- The importance statistic assumes that we have trained a random forest and we have validation data.

## Calculation of importance

</br>

1. We record the prediction error of the trained algorithm on the validation data. This serves as a baseline.

2. Next, a variable is “taken out” by having all of its values permuted in the validation dataset. We then compute the prediction error of the algorithm on the perturbed validation data.

3. The importance statistic is the difference between the error due to the perturbation and the baseline. A larger difference means an important predictor for the algorithm.

##

</br></br>

In Python, we use compute the predictor importance using the `permutation_importance()` function.

```{python}
#| echo: true
#| output: true
#| code-fold: false
#| fig-align: center

# Compute permutation importance
result = permutation_importance(RFalgorithm, X_valid, Y_valid, n_repeats=5, random_state=59227)
```

##

```{python}
#| echo: true
#| output: true
#| code-fold: true
#| fig-align: center

# Create DataFrame
importance_df = pd.DataFrame({
    'Feature': X_valid.columns,
    'Importance': result.importances_mean
}).sort_values(by='Importance', ascending=False)

# Plot bar chart
plt.figure(figsize=(8, 5))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xticks(rotation=90)
plt.title('Permutation Feature Importance')
plt.show()
```

# Boosting

## Boosting

</br></br>

-   In boosting, we also grow multiple decision trees. But instead of growing trees randomly, each new tree depends on the previous one.

-   Boosting is easier to understand in the context of regression, rather than classification.

-   [Idea]{.underline}: Explore [**cooperation**]{style="color:brown;"} between desicion trees, rather than [diversity]{style="color:#4682B4;"} as in Bagging and Random Forest.

## Boosting for regression

-   Boosting creates a sequence of trees, each one building upon the previous.

-   Earlier trees are small, and the next tree is created with the **residuals** of the previous tree.

-   In other words, at each step, we try to explain the information that we didn't explain in previous steps.

-   Gradually, the sequence "learns" to predict.

-   Something a little odd: earlier trees are deliberately "held back" to keep them from explaining too much. This creates "slow learning".

## 

![](images/clipboard-720176022.png)

<https://pythongeeks.org/gradient-boosting-algorithm-in-machine-learning/>


##

::: {style="font-size: 95%;"}
Initially, the boosted tree is $\hat{f}(\boldsymbol{x}) = 0$ and the residuals of this tree are $r_i = y_i - f(\boldsymbol{x}_i)$ for all $i$ in the training data.

At each step $b$ in the process ($b = 1, \ldots, B$), we

1. **Build** a regression tree $\hat{T}_b$ with $d$ splits to the training data $(\boldsymbol{X}, \boldsymbol{r})$. This tree has $d+1$ terminal nodes.
2. **Update** the boosted tree $\hat{f}$ by adding in a [***shrunken version***]{style="color:orange;"} of the new tree: $\hat{f}(\boldsymbol{x}) \leftarrow \hat{f}(\boldsymbol{x}) + \lambda \hat{T}_b(\boldsymbol{x})$.
3. **Update** the residuals using shrunken tree, $r_i \leftarrow r_i - \lambda \hat{T}_b(x_i)$.

The final boosted tree is: $\hat{f}(\boldsymbol{x}) = \sum_{b=1}^{B} \lambda \hat{T}_b (\boldsymbol{x}).$
:::

::: notes
D is 4 or 5, sometimes 1.
Lambda is 0.001 or 0.01
This algorithm is a gradient boosting method for regression.
:::

## Why does this work?

::: {style="font-size: 90%;"}
- By using a small tree, we are deliberately leaving information out of the first round of the model. So what gets fit is the “easy” stuff.

- The residuals have all of the information that we haven’t yet explained. We continue iterating on the residuals, fitting them with small trees, so that we slowly explain the bits of the variation that are harder to explain.

- This process is called “learning”: at each iteration, we get a better fit.

- By multiplying by $\lambda <1$, we “slow down” the learning (by making it harder to fit all of the variation), and there is research that says that slower learning is better.
:::

## Tuning parameters

</br></br>

The [**number of trees**]{style="color:brown;"} $B$. Unlike bagging and random forest, boosting can overfit if $B$ is too large. We use so-called *K-fold cross-validation* to select $B$.

The [**shrinkage parameter**]{style="color:brown;"} $\lambda$, a small positive number. Typical values are 0.01 or 0.001. Very small $\lambda$ can require using a very large value of B to achieve good performance.

The [**number of splits**]{style="color:brown;"} $d$ in each tree. Common choices are 1, 4 or 5. Often $d = 1$, in which case each tree is a stump.

## Boosting in Python

In Python, we define a Boosting algorithm for classification using the `GradientBoostingRegressor` function from **scikit-learn**. 

```{python}
#| echo: true
#| output: false

# Set the boosting algorithm.
GBalgorithm = GradientBoostingRegressor(learning_rate = 0.1,
                                    n_estimators = 1000,
                                    max_depth = 3,
                                    random_state = 59227)

# Train the algorithm.
GBalgorithm.fit(X_train, Y_train)
```

`n_estimators` sets the number of decision trees to use, `learning_rate` sets the value of $\lambda$, and `max_depth` sets the depth of the tree.

## Validation RMSE for boosting

</br></br>

Evaluate the performance of the boosting algorithm.

```{python}
#| echo: true
#| output: true
#| code-fold: false
#| fig-align: center

# Predict responses in validation data.
GB_predicted = GBalgorithm.predict(X_valid)

# Compute RMSE.
mse = mean_squared_error(Y_valid, GB_predicted) 
print(round(mse**(1/2), 2))
```

## Predictor importance

We can also apply predictor importance to the gradient boosting algorithm.

```{python}
#| echo: true
#| output: true
#| code-fold: true
#| fig-align: center

# Compute permutation importance
result = permutation_importance(GBalgorithm, X_valid, Y_valid, n_repeats=5, random_state=59227)
# Create DataFrame
importance_df = pd.DataFrame({
    'Feature': X_valid.columns,
    'Importance': result.importances_mean
}).sort_values(by='Importance', ascending=False)

# Plot bar chart
plt.figure(figsize=(5, 3))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xticks(rotation=90)
plt.title('Permutation Feature Importance')
plt.show()
```


## Issues with boosting

</br></br>

- Loss of interpretability: the final boosted model is a weighted sum of trees, which we cannot interpret easily. 

- Computational complexity: since it uses slow learners, it can be time consuming. However, we are growing small trees, each step can be done relatively quickly in some cases (e.g. AdaBoost). 

# Ensemble methods for Time Series


# [Return to main page](https://alanrvazquez.github.io/TEC-IN2004B/)

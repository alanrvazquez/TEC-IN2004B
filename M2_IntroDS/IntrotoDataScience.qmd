---
title: "Introduction to Data Science"
subtitle: "IN2004B: Generation of Value with Data Analytics"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: false
    footer: "Tecnologico de Monterrey"
    logo: IN1002b_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Agenda

</br>

1.  ¿Qué es la ciencia de datos?
2.  Tipos de aprendizaje: supervisado y no supervisado

# ¿Qué es la ciencia de datos?

## 

La ciencia de datos es un campo interdisciplinario que utiliza métodos, procesos, algoritmos y sistemas científicos para extraer conocimientos e ideas de muchos [datos]{style="color:purple;"} estructurados y no estructurados.

. . .

:::::: center
::::: columns
::: {.column width="40%"}
![](images/clipboard-2707460623.png){fig-align="center" width="377"}
:::

::: {.column width="50%"}
![](images/clipboard-3242954590.png){fig-align="center" width="485"}
:::
:::::
::::::

## Otros conceptos parecidos

La \[minería de datos\]\[datos\] es un proceso de descubrimiento de patrones en grandes conjuntos de datos utilizando métodos en la intersección de estadísticas y sistemas de bases de [datos]{style="color:purple;"}

El \[modelado predictivo\]\[datos\] es el proceso de desarrollar un modelo de manera que podamos comprender y cuantificar la precisión de la predicción del modelo en [datos]{style="color:purple;"} futuros aún por ver.

El **aprendizaje estadístico** se refiere a un conjunto de herramientas (modelos estadísticos y métodos de minería de datos) para modelar y comprender conjuntos de [datos]{style="color:purple;"} complejos.

## En 2004 ...

El huracán Frances azotaba el Caribe y amenazaba con afectar directamente la costa atlántica de Florida.

. . .

::::::: center
:::::: columns
::: {.column width="30%"}
![](images/clipboard-123225538.png){width="194"}
:::

::: {.column width="30%"}
![](https://c8.alamy.com/compes/ccn9gk/ft-pierce-9-6-04-un-clubouse-danados-por-el-huracan-frances-en-ocean-village-en-hutchinson-island-el-lunes-el-complejo-tambien-recibio-algunos-danos-a-techos-pisos-de-tierra-y-algunas-unidades-fueron-danadas-por-las-tormentas-foto-por-aguas-lannis-el-palm-beach-post-no-para-su-distribucion-fuera-de-cox-ccn9gk.jpg){width="268"}
:::

::: {.column width="30%"}
![](images/clipboard-679480994.png){width="280"}
:::
::::::
:::::::

. . .

Los residentes se dirigieron a terrenos más altos, pero en Arkansas, los ejecutivos de Wal-Mart vieron una gran oportunidad para una de sus más nuevas armas basadas en datos: la ***tecnología predictiva***.

## 

::::::: center
:::::: columns
:::: {.column width="70%"}
::: {style="font-size: 90%;"}
Una semana antes de que la tormenta tocara tierra, Linda M. Dillman, directora de información de Wal-Mart, presionó a su personal para que creara pronósticos basados en lo que había sucedido cuando el huracán Charley azotó la zona varias semanas antes.

<br/>

Con el respaldo de billones de bytes de historial de compras almacenados en el almacén de datos de Wal-Mart, dijo, la empresa podría "comenzar a predecir lo que va a suceder, en lugar de esperar a que suceda", como lo expresó.
:::
::::

::: {.column width="30%"}
![](images/clipboard-3960378011.png){width="224"}

<br/>

![](images/clipboard-4213549697.png){width="516"}
:::
::::::
:::::::

## The result

::::::: columns
:::: {.column width="50%"}
El New York Times informó

::: {style="font-size: 80%;"}
> *"... Los expertos analizaron los datos y descubrieron que las tiendas efectivamente necesitarían ciertos productos, y no solo las típicas linternas.*
:::
::::

:::: {.column width="50%"}
Dillman dijo

::: {style="font-size: 80%;"}
> *"No sabíamos en el pasado que las Pop-Tarts de fresa aumentan sus ventas, como siete veces su tasa de ventas normal, antes de un huracán".*
:::
::::
:::::::

[![](images/clipboard-3670330051.png){fig-align="center" width="529"}](https://www.nytimes.com/2004/11/14/business/yourmoney/what-walmart-knows-about-customers-habits.html)

## Cross-Industry Standard Process (CRISP) for Data Science

![](images/clipboard-4096324521.png){fig-align="center"}

## Modelo CRISP

**Comprensión empresarial**: ¿qué necesita la empresa?

**Comprensión de datos**: ¿Qué datos tenemos o necesitamos? ¿Está limpio?

**Preparación de datos**: ¿cómo organizamos los datos para el modelado?

**Modelado**: ¿Qué técnicas de modelado debemos aplicar?

**Evaluación**: ¿Qué modelo se adapta mejor a los objetivos comerciales?

**Implementación**: ¿Cómo acceden las partes interesadas a los resultados?

## Comprensión empresarial

-   La comprensión del negocio se refiere a definir el problema empresarial que se busca resolver.

-   El objetivo es replantear el problema empresarial como un problema de ciencia de datos.

-   Replantear el problema y diseñar una solución suele ser un proceso iterativo.

## Problemas en Ciencia de Datos

La [**clasificación**]{style="color:blue;"} (o estimación de probabilidad de clase) intenta predecir, para cada individuo de una población, a cuál de un (pequeño) conjunto de clases pertenece este individuo. Por ejemplo, "Entre todos los clientes de T-Mobile, ¿cuáles probablemente responderán a una oferta determinada?”

. . .

La [**regresión intenta**]{style="color:green;"} estimar o predecir, para cada individuo, el valor numérico de alguna variable para ese individuo. Por ejemplo, "¿Cuánto utilizará el servicio un cliente determinado?"

## 

La agrupación intenta agrupar a los individuos de una población por su similitud, pero no por ningún propósito específico. Por ejemplo, "¿Nuestros clientes forman grupos o segmentos naturales?"

# Discusión

-   A menudo, reformular el problema y diseñar una solución es un proceso iterativo.

-   La formulación inicial puede no ser completa u óptima, por lo que pueden ser necesarias múltiples iteraciones para una formulación de solución aceptable.

-   [La clave para un gran éxito es la formulación creativa del problema por parte de algún analista sobre cómo plantear el problema empresarial como uno o más problemas de ciencia de datos.]{style="color:lightblue;"}

# Comprensión de Datos

## Comprensión de datos I

</br>

::: incremental
-   Si el objetivo es resolver un problema de negocio, los datos constituyen la materia prima disponible a partir de la cual se construirá la solución.

-   Los datos disponibles rara vez coinciden con el problema.

-   Por ejemplo, los datos históricos a menudo se recopilan con fines no relacionados con el problema de negocio actual o sin ningún propósito explícito.
:::

## Comprensión de datos II

</br>

-   Los costos de los datos varían. Algunos datos estarán disponibles de forma gratuita, mientras que otros requerirán esfuerzo para obtenerlos.

:: incremental - Una parte fundamental de la fase de comprensión de datos es estimar los costos y beneficios de cada fuente de datos y decidir si se justifica una mayor inversión.

-   Incluso después de adquirir todos los conjuntos de datos, recopilarlos puede requerir un esfuerzo adicional. :::

## Ejemplo

En la década de 1980, las tarjetas de crédito tenían esencialmente precios uniformes porque las empresas no tenían sistemas de información adecuados para hacer frente a precios diferenciales a escala masiva.

Alrededor de 1990, Richard Fairbanks y Nigel Morris se dieron cuenta de que la tecnología de la información era lo suficientemente poderosa como para poder realizar modelos predictivos más sofisticados y ofrecer condiciones diferentes (hoy en día: precios, límites de crédito, transferencias de saldo con tasa inicial baja, devolución de efectivo, y puntos de fidelidad).

## 

La dirección de Signet Bank estaba convencida de que modelar la rentabilidad, y no sólo la probabilidad de incumplimiento, era la estrategia correcta.

Sabían que una pequeña proporción de clientes en realidad representa más del 100% de las ganancias de un banco por operaciones con tarjetas de crédito (porque el resto está en equilibrio o pierde dinero).

Si pudieran modelar la rentabilidad, podrían hacer mejores ofertas a los mejores clientes y “desnatar la crema” de la clientela de los grandes bancos.

## 

Pero Signet Bank tuvo un problema realmente grande al implementar esta estrategia.

No tenían los datos adecuados para modelar la rentabilidad con el objetivo de ofrecer diferentes condiciones a diferentes clientes!

Dado que el banco ofrecía crédito con un conjunto específico de términos y un modelo de incumplimiento específico, tenía los datos para modelar la rentabilidad (1) para los términos que realmente ofrecieron en el pasado, y (2) para el tipo de cliente que realmente se le ofreció crédito.

## 

¿Qué podría hacer Signet Bank? Pusieron en juego una estrategia fundamental de la ciencia de datos: **adquirir los datos necesarios a un costo**.

En este caso, se podrían generar datos sobre la rentabilidad de los clientes con diferentes condiciones de crédito mediante la realización de experimentos. Se ofrecieron diferentes términos al azar a diferentes clientes.

Esto puede parecer una tontería fuera del contexto del pensamiento analítico de datos: ¡es probable que pierda dinero!

Esto es cierto. En este caso, las pérdidas son el costo de adquisición de datos.

## ¿Qué sucedió?

Como era de esperar, el número de cuentas malas de Signet se disparó.

Las pérdidas continuaron durante algunos años mientras los científicos de datos trabajaban para construir modelos predictivos a partir de los datos, evaluarlos e implementarlos para mejorar las ganancias.

Como la empresa consideraba estas pérdidas como inversiones en datos, persistieron a pesar de las quejas de las partes interesadas.

Con el tiempo, la operación de tarjetas de crédito de Signet dio un giro y se volvió tan rentable que se escindió para separarla de las otras operaciones del banco, que ahora estaban eclipsando el éxito del crédito al consumo.

## Richard Fairbanks y Nigel Morris

![](images/clipboard-4138827509.png)

## Herramientas de Ciencia de Datos más utilizadas

1.  Python
2.  R
3.  SAS
4.  Excel
5.  Power BI
6.  Tableau
7.  Apache Spark

<https://hackr.io/blog/top-data-analytics-tools>

## Otras herramientas usadas

-   RapidMiner (<https://rapidminer.com/products/studio/>)

-   JMP (<https://www.jmp.com/es_mx/home.html>)

-   Minitab (<https://www.minitab.com/es-mx/products/minitab/>)

-   Trifacta (<https://www.trifacta.com/>)

-   BigML (<https://bigml.com/>)

-   MLBase (<http://www.mlbase.org/>)

-   Google Cloud AutoML (<https://cloud.google.com/automl/>)

# Tipos de aprendizaje: supervisado y no supervisado

## Terminología

-   [**Predictores**]{style="color:darkblue;"}. Se representan usando la notación $X_1$ para el primer predictor, $X_p$ para el segundo predictor, ..., y $X_p$ para el p-ésimo predictor.

-   [**Respuesta**]{style="color:darkred;"}. $Y$ representa la variable de respuesta, que intentaremos predecir.

En terminos matemáticos, queremos establecer la siguiente relación

$$ 
Y = f(X_1, X_2, \ldots, X_p) + \epsilon
$$

Donde $f$ es una función de los predictores y $\epsilon$ es un error natural (aleatorio).

## Tipos de Aprendizaje

</br></br>

En ciencia de datos (y machine learning), existen dos tipos principales de aprendizaje:

-   [Aprendizaje supervisado (*supervised learning*)]{style="color:blue;"}

-   [Aprendizaje sin supervisión (*unsupervised learning*)]{style="color:green;"}

## 

![](images/clipboard-2564234206.png)

## Aprendizaje supervisado

Comprende algoritmos que aprendan a través de ejemplos. El usuario proporciona al algoritmo de aprendizaje automático un conjunto de datos conocidos que incluye las entradas y salidas conocidas correspondientes. El algoritmo debe encontrar un método para determinar cómo llegar a esas entradas y salidas.

Mientras el operador conoce las respuestas correctas al problema, el algoritmo identifica patrones en los datos, aprende de las observaciones y hace predicciones. El algoritmo realiza predicciones y puede ser corregido por el operador, y este proceso sigue hasta que el algoritmo alcanza un alto nivel de precisión y rendimiento.

## Algoritmos Supervisados Populares

![](images/clipboard-4052653044.png)

## 

![](images/clipboard-2622609818.png)

## Algoritmos no supervisados populares

![](images/clipboard-1531600765.png)

## Aprendizaje sin supervisión

El algoritmo de aprendizaje automático estudia los datos para identificar patrones. No hay una clave de respuesta o un operador humano para proporcionar instrucción. La máquina determina las correlaciones y las relaciones mediante el análisis de los datos disponibles.

En este proceso, se deja que el algoritmo de aprendizaje automático interprete grandes conjuntos de datos. El algoritmo intenta organizar esos datos de alguna manera para describir su estructura. A medida que evalúa más datos, su capacidad para tomar decisiones sobre los mismos mejora gradualmente y se vuelve más refinada.

## Dos conjuntos de datos

-   En el aprendizaje supervisado, existen varios tipos de datos.

-   Los datos de entrenamiento son los que se utilizan para construir $\hat{f}(\boldsymbol{X})$.

-   Los datos de prueba son los que NO se utilizaron en el proceso de ajuste, pero que se utilizan para comprobar el rendimiento del modelo con datos no analizados.

![](images/training_test.jpeg){fig-align="center"}

## Yogi Berra

</br></br></br>

> It’s though to make predictions, especially about the future.

## Let's Play

Vamos a jugar con modelos supervisados

1.  <https://quickdraw.withgoogle.com/>

2.  <https://tenso.rs/demos/rock-paper-scissors/>

3.  <https://teachablemachine.withgoogle.com/>

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)

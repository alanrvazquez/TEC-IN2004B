[
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#agenda",
    "href": "Module2/IntrotoDataScience.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nData Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "href": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data Science is …",
    "text": "Data Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#similar-concepts",
    "href": "Module2/IntrotoDataScience.slides.html#similar-concepts",
    "title": "Introduction to Data Science",
    "section": "Similar concepts",
    "text": "Similar concepts\n\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#in-2004",
    "href": "Module2/IntrotoDataScience.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004…",
    "text": "In 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section",
    "href": "Module2/IntrotoDataScience.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#the-result",
    "href": "Module2/IntrotoDataScience.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "title": "Introduction to Data Science",
    "section": "Cross-Industry Standard Process (CRISP) for Data Science",
    "text": "Cross-Industry Standard Process (CRISP) for Data Science"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "href": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "title": "Introduction to Data Science",
    "section": "CRISP Model",
    "text": "CRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nDeployment: How do stakeholders access the results?"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "href": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business understanding",
    "text": "Business understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "title": "Introduction to Data Science",
    "section": "Problems in Data Science",
    "text": "Problems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-1",
    "href": "Module2/IntrotoDataScience.slides.html#section-1",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Clustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#discussion",
    "href": "Module2/IntrotoDataScience.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data understanding I",
    "text": "Data understanding I\n\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data understanding II",
    "text": "Data understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#example",
    "href": "Module2/IntrotoDataScience.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\n\nIn the 1980s, credit cards had uniform pricing — companies lacked the systems for mass differential pricing.\nBy 1990, Richard Fairbanks and Nigel Morris saw that IT could power predictive models to customize offers (pricing, credit limits, low introductory rates, cash back, loyalty points).\nSignet Bank’s strategy: model profitability, not just probability of default, since a small fraction of customers generate most profits."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-2",
    "href": "Module2/IntrotoDataScience.slides.html#section-2",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Problem: They lacked data on how different credit terms affected profitability.\n\n\n\nSolution: Acquire data at a cost — run experiments offering varied terms to different customers. Losses from these offers were considered investments in data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#what-happened",
    "href": "Module2/IntrotoDataScience.slides.html#what-happened",
    "title": "Introduction to Data Science",
    "section": "What happened?",
    "text": "What happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "href": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "title": "Introduction to Data Science",
    "section": "Richard Fairbanks and Nigel Morris",
    "text": "Richard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#terminology",
    "href": "Module2/IntrotoDataScience.slides.html#terminology",
    "title": "Introduction to Data Science",
    "section": "Terminology",
    "text": "Terminology\n\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a whole collection of \\(p\\) predictors.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "href": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "title": "Introduction to Data Science",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised learning…",
    "text": "Supervised learning…\n\nIncludes algorithms that learn by example. That is, we provide the supervised algorithm with a data set with known predictor and response values. The algorithm must find a way to determine the responses from the predictors.\nSince we have the correct (true) responses, the algorithm can identify patterns in the data, learn from its mistakes, and make better predictions of the responses.\nThe algorithm is trained to reach a high level of accuracy and performance for predicting the responses."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#mathematically",
    "href": "Module2/IntrotoDataScience.slides.html#mathematically",
    "title": "Introduction to Data Science",
    "section": "Mathematically",
    "text": "Mathematically\n\nWe want to establish the following relationship\n\\[\nY = f(\\boldsymbol{X}) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error.\n\n\n\\(f(\\boldsymbol{X})\\) represents the true relationship between the response (\\(Y\\)) and predictors (\\(\\boldsymbol{X}\\)).\n\n\n\n\nHowever, \\(f(\\boldsymbol{X})\\) is unknown and very complex!"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-4",
    "href": "Module2/IntrotoDataScience.slides.html#section-4",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A supervised algorithm attempts to construct an approximation \\(\\hat{f}(\\boldsymbol{X})\\) to the true function \\(f(\\boldsymbol{X})\\) using available data on the predictors and response.\nIdeally, the algorithm builds an \\(\\hat{f}(\\boldsymbol{X})\\) that is interpretable, but not necessarily."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "href": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "title": "Introduction to Data Science",
    "section": "Two data sets",
    "text": "Two data sets\nIn supervised learning, there are two main types of data:\n\nTraining data is data used by the supervised algorithm to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data NOT used in the algorithm’s training process, but is used to evaluate the quality of \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular supervised algorithms",
    "text": "Popular supervised algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Unsupervised learning…",
    "text": "Unsupervised learning…\n\nstudies data of the predictors (\\(\\boldsymbol{X}\\)) to identify patterns. There are no responses.\nAn unsupervised algorithm identifies correlations and relationships by analyzing available training data. So, the unsupervised algorithm is left to interpret the data set and organize it in some way to describe its structure.\nIn technical terms, we want the algorithm to say something about the joint probability distribution of the predictors \\(P(X_1, X_2, \\ldots, X_p)\\)."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Unsupervised Algorithms",
    "text": "Popular Unsupervised Algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#lets-play-with-supervised-models.",
    "href": "Module2/IntrotoDataScience.slides.html#lets-play-with-supervised-models.",
    "title": "Introduction to Data Science",
    "section": "Let’s play with supervised models.",
    "text": "Let’s play with supervised models.\n\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "Module2/Classification.slides.html#agenda",
    "href": "Module2/Classification.slides.html#agenda",
    "title": "Classification Trees",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nTraining, Validation, and Test Data\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Module2/Classification.slides.html#load-the-libraries",
    "href": "Module2/Classification.slides.html#load-the-libraries",
    "title": "Classification Trees",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/Classification.slides.html#scikit-learn-library",
    "href": "Module2/Classification.slides.html#scikit-learn-library",
    "title": "Classification Trees",
    "section": "scikit-learn library",
    "text": "scikit-learn library\n\nscikit-learn is a robust and popular library for machine learning in Python\nIt provides simple, efficient tools for data mining and data analysis\nIt is built on top of libraries such as NumPy, SciPy, and Matplotlib\nhttps://scikit-learn.org/stable/"
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems",
    "href": "Module2/Classification.slides.html#main-data-science-problems",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems-1",
    "href": "Module2/Classification.slides.html#main-data-science-problems-1",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#terminology",
    "href": "Module2/Classification.slides.html#terminology",
    "title": "Classification Trees",
    "section": "Terminology",
    "text": "Terminology\n\nRecall that\n\n\\(X\\) represents a predictor or explanatory variable.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors."
  },
  {
    "objectID": "Module2/Classification.slides.html#section",
    "href": "Module2/Classification.slides.html#section",
    "title": "Classification Trees",
    "section": "",
    "text": "Response:\n\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-algorithms",
    "href": "Module2/Classification.slides.html#classification-algorithms",
    "title": "Classification Trees",
    "section": "Classification algorithms",
    "text": "Classification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (either target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not."
  },
  {
    "objectID": "Module2/Classification.slides.html#the-goal-of-classification-algorithms",
    "href": "Module2/Classification.slides.html#the-goal-of-classification-algorithms",
    "title": "Classification Trees",
    "section": "The goal of classification algorithms",
    "text": "The goal of classification algorithms\n\nGoal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n\n\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-1",
    "href": "Module2/Classification.slides.html#example-1",
    "title": "Classification Trees",
    "section": "Example 1",
    "text": "Example 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n\n\n\n\n\n\n\n\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-1",
    "href": "Module2/Classification.slides.html#section-1",
    "title": "Classification Trees",
    "section": "",
    "text": "Technically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\n\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]"
  },
  {
    "objectID": "Module2/Classification.slides.html#bayes-classifier",
    "href": "Module2/Classification.slides.html#bayes-classifier",
    "title": "Classification Trees",
    "section": "Bayes classifier",
    "text": "Bayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-2",
    "href": "Module2/Classification.slides.html#section-2",
    "title": "Classification Trees",
    "section": "",
    "text": "HOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n\nTo overcome this issue, we several methods:\n\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits)."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-3",
    "href": "Module2/Classification.slides.html#section-3",
    "title": "Classification Trees",
    "section": "",
    "text": "Once we estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) using one of these methods, we plug it into the Bayes classifier:\n\\[\\hat{C}(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nwhere\n\n\\(\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) is an estimate of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\).\n\\(\\hat{C}(\\boldsymbol{X})\\) is an estimate of the true Bayes Classifier \\(C(\\boldsymbol{X})\\)"
  },
  {
    "objectID": "Module2/Classification.slides.html#two-datasets",
    "href": "Module2/Classification.slides.html#two-datasets",
    "title": "Classification Trees",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science algorithms needs two data sets:\n\n\nTraining data is data that we use to train or construct the approximation \\(\\hat{C}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the classification performance of \\(\\hat{C}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-4",
    "href": "Module2/Classification.slides.html#section-4",
    "title": "Classification Trees",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{C}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{C}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#validation-dataset",
    "href": "Module2/Classification.slides.html#validation-dataset",
    "title": "Classification Trees",
    "section": "Validation dataset",
    "text": "Validation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "href": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "title": "Classification Trees",
    "section": "Example 2: Identifying Counterfeit Banknotes",
    "text": "Example 2: Identifying Counterfeit Banknotes"
  },
  {
    "objectID": "Module2/Classification.slides.html#dataset",
    "href": "Module2/Classification.slides.html#dataset",
    "title": "Classification Trees",
    "section": "Dataset",
    "text": "Dataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "href": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "title": "Classification Trees",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "href": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "title": "Classification Trees",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .filter() from pandas to select two predictors: Top and Bottom.\n\n# Set full matrix of predictors.\nX_full = bank_data.filter(['Top', 'Bottom'])\nX_full.head(4)\n\n\n\n\n\n\n\n\nTop\nBottom\n\n\n\n\n0\n9.7\n9.0\n\n\n1\n9.5\n8.1\n\n\n2\n9.6\n8.7\n\n\n3\n10.4\n7.5"
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-response-column",
    "href": "Module2/Classification.slides.html#create-the-response-column",
    "title": "Classification Trees",
    "section": "Create the response column",
    "text": "Create the response column\nWe do the same to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data['Status']\nY_full.head(4)\n\n0    genuine\n1    genuine\n2    genuine\n3    genuine\nName: Status, dtype: category\nCategories (2, object): ['counterfeit', 'genuine']"
  },
  {
    "objectID": "Module2/Classification.slides.html#set-the-target-category",
    "href": "Module2/Classification.slides.html#set-the-target-category",
    "title": "Classification Trees",
    "section": "Set the target category",
    "text": "Set the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: counterfeit, dtype: int64"
  },
  {
    "objectID": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "href": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "title": "Classification Trees",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3, \n                                                      stratify = Y_target_full,\n                                                      random_state=507134)\n\n\nThanks to the stratify parameter, the function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-5",
    "href": "Module2/Classification.slides.html#section-5",
    "title": "Classification Trees",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nTop\nBottom\ncounterfeit\n\n\n\n\n187\n10.6\n11.4\n1\n\n\n54\n10.0\n7.9\n0\n\n\n33\n10.3\n8.4\n0\n\n\n180\n10.7\n11.4\n1"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-6",
    "href": "Module2/Classification.slides.html#section-6",
    "title": "Classification Trees",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nTop\nBottom\ncounterfeit\n\n\n\n\n56\n10.4\n9.2\n0\n\n\n17\n9.0\n9.0\n0\n\n\n24\n10.8\n7.4\n0\n\n\n20\n10.0\n8.4\n0\n\n\n139\n10.2\n11.8\n1"
  },
  {
    "objectID": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "href": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "title": "Classification Trees",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\n\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nAlgorithm development.\n\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your algorithm!"
  },
  {
    "objectID": "Module2/Classification.slides.html#decision-tree",
    "href": "Module2/Classification.slides.html#decision-tree",
    "title": "Classification Trees",
    "section": "Decision tree",
    "text": "Decision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique."
  },
  {
    "objectID": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "href": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "title": "Classification Trees",
    "section": "Basic idea of a decision tree",
    "text": "Basic idea of a decision tree\nStratify or segment the predictor space into several simpler regions."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "href": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "title": "Classification Trees",
    "section": "How do you build a decision tree?",
    "text": "How do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "href": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "title": "Classification Trees",
    "section": "How do we grow a tree?",
    "text": "How do we grow a tree?\n\n\nUsing the CART algorithm!\n\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-25",
    "href": "Module2/Classification.slides.html#section-25",
    "title": "Classification Trees",
    "section": "",
    "text": "We repeat the partitioning process until:\n\nimpurity does not improve in any of the terminal nodes, or\neach terminal node has no less than, say, 5 observations."
  },
  {
    "objectID": "Module2/Classification.slides.html#what-is-impurity",
    "href": "Module2/Classification.slides.html#what-is-impurity",
    "title": "Classification Trees",
    "section": "What is impurity?",
    "text": "What is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "href": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "title": "Classification Trees",
    "section": "How do we measure impurity?",
    "text": "How do we measure impurity?\n\n\n\n\nThere are three different metrics for impurity:\n\nMisclassification risk.\nCross entropy.\nGini impurity index.\n\n\n \n\np: Proportion of elements in the target class"
  },
  {
    "objectID": "Module2/Classification.slides.html#mathematically",
    "href": "Module2/Classification.slides.html#mathematically",
    "title": "Classification Trees",
    "section": "Mathematically",
    "text": "Mathematically\n\nLet \\(p_1\\) and \\(p_2\\) be the proportion of observations in the target and reference class, respectively, in a node.\n\nMisclassification risk: \\(1 - \\max\\{p_1, p_2\\}\\)\nCross entropy: \\(- (p_1 \\log(p_1) + p_2 \\log(p_2))\\)\nGini impurity index: \\(p_1(1 - p_1) + p_2(1 - p_2)\\)"
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python",
    "href": "Module2/Classification.slides.html#in-python",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\nIn Python, we use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a classification tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(min_samples_leaf= 5, ccp_alpha=0, \n                              random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameter min_samples_leaf controls the minimum number of observations in a terminal node, and the cc_alpha controls the tree complexity (to be described later). The parameter random_state allows you to reproduce the same tree in different runs of the Python code."
  },
  {
    "objectID": "Module2/Classification.slides.html#plotting-the-tree",
    "href": "Module2/Classification.slides.html#plotting-the-tree",
    "title": "Classification Trees",
    "section": "Plotting the tree",
    "text": "Plotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, feature_names = X_train.columns,\n    class_names=[\"genuine\", \"counterfeit\"], filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#the-tree-in-the-predictor-space",
    "href": "Module2/Classification.slides.html#the-tree-in-the-predictor-space",
    "title": "Classification Trees",
    "section": "The tree in the predictor space",
    "text": "The tree in the predictor space"
  },
  {
    "objectID": "Module2/Classification.slides.html#estimated-conditional-probabilities",
    "href": "Module2/Classification.slides.html#estimated-conditional-probabilities",
    "title": "Classification Trees",
    "section": "Estimated conditional probabilities",
    "text": "Estimated conditional probabilities\n\nAfter training a classification tree, we can calculate the estimated conditional probability \\(\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\).\nTo this end, let\n\n\\(\\hat{p}_1(\\boldsymbol{x}) = \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)\n\\(\\hat{p}_0(\\boldsymbol{x}) = 1 - \\hat{p}_1(\\boldsymbol{x})\\)\n\nbe the estimated probabilities that \\(\\boldsymbol{x}\\) belongs to class 1 or 0."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-27",
    "href": "Module2/Classification.slides.html#section-27",
    "title": "Classification Trees",
    "section": "",
    "text": "Essentially, we calculate \\(\\hat{p}_1(\\boldsymbol{x})\\) and \\(\\hat{p}_0(\\boldsymbol{x})\\) as follows:\n\nSelect the region or terminal node where \\(\\boldsymbol{x}\\) belongs.\n\\(\\hat{p}_1(\\boldsymbol{x})\\) is the proportion of observations in that terminal node that belong to class 1. \\(\\hat{p}_0(\\boldsymbol{x})\\) is the proportion of observations that belong to class 0."
  },
  {
    "objectID": "Module2/Classification.slides.html#visually",
    "href": "Module2/Classification.slides.html#visually",
    "title": "Classification Trees",
    "section": "Visually",
    "text": "Visually\nLet \\((\\hat{p}_0(\\boldsymbol{x}), \\hat{p}_1(\\boldsymbol{x}))\\) be the estimated probabilities."
  },
  {
    "objectID": "Module2/Classification.slides.html#estimated-bayes-classifier",
    "href": "Module2/Classification.slides.html#estimated-bayes-classifier",
    "title": "Classification Trees",
    "section": "Estimated Bayes Classifier",
    "text": "Estimated Bayes Classifier\n\nOnce we have estimated the conditional probability using the classification tree, we plug it into the Bayes classifier to have our approximated function:\n\\[\\hat{C}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases},\\]\nwhere \\(\\hat{p}_1(\\boldsymbol{x})\\) depends on the region or terminal node \\(\\boldsymbol{x}\\) falls in."
  },
  {
    "objectID": "Module2/Classification.slides.html#simplified-decision-boundary",
    "href": "Module2/Classification.slides.html#simplified-decision-boundary",
    "title": "Classification Trees",
    "section": "Simplified decision boundary",
    "text": "Simplified decision boundary"
  },
  {
    "objectID": "Module2/Classification.slides.html#decision-boundary-logistic-regression",
    "href": "Module2/Classification.slides.html#decision-boundary-logistic-regression",
    "title": "Classification Trees",
    "section": "Decision boundary logistic regression",
    "text": "Decision boundary logistic regression"
  },
  {
    "objectID": "Module2/Classification.slides.html#implementation-details",
    "href": "Module2/Classification.slides.html#implementation-details",
    "title": "Classification Trees",
    "section": "Implementation details",
    "text": "Implementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability."
  },
  {
    "objectID": "Module2/Classification.slides.html#evaluation",
    "href": "Module2/Classification.slides.html#evaluation",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-28",
    "href": "Module2/Classification.slides.html#section-28",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1])"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-30",
    "href": "Module2/Classification.slides.html#section-30",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ]])"
  },
  {
    "objectID": "Module2/Classification.slides.html#confusion-matrix",
    "href": "Module2/Classification.slides.html#confusion-matrix",
    "title": "Classification Trees",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems."
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python-1",
    "href": "Module2/Classification.slides.html#in-python-1",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[29  1]\n [ 1 29]]"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-31",
    "href": "Module2/Classification.slides.html#section-31",
    "title": "Classification Trees",
    "section": "",
    "text": "We can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nCMplot = ConfusionMatrixDisplay(cm, display_labels = [\"genuine\", \"counterfeit\"])\nCMplot.plot()"
  },
  {
    "objectID": "Module2/Classification.slides.html#accuracy",
    "href": "Module2/Classification.slides.html#accuracy",
    "title": "Classification Trees",
    "section": "Accuracy",
    "text": "Accuracy\nIt is a simple metric for summarizing the information in the confusion matrix. It is the proportion of correct classifications for both classes, out of the total classifications performed.\n\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.97\n\n\nThe higher the accuracy, the better the performance of the classification algorithm."
  },
  {
    "objectID": "Module2/Classification.slides.html#pruning-the-tree",
    "href": "Module2/Classification.slides.html#pruning-the-tree",
    "title": "Classification Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\nIn some cases, we can optimize the performance of the tree by pruning it. That is, we collapse two internal (non-terminal) nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-34",
    "href": "Module2/Classification.slides.html#section-34",
    "title": "Classification Trees",
    "section": "",
    "text": "To prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThis algorithm minimizes the following function:\n\n\\[\\text{Missclassification rate of tree} + \\alpha (\\text{\\# terminal nodes}),\\]\n\nwhere \\(\\alpha\\) is a tuning parameter that places greater weight on the number of tree nodes (or size).\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "href": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "title": "Classification Trees",
    "section": "Apply penalty for large trees",
    "text": "Apply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of \\(\\alpha\\), which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas = path.ccp_alphas"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-35",
    "href": "Module2/Classification.slides.html#section-35",
    "title": "Classification Trees",
    "section": "",
    "text": "The ccp_alphas object contains the different values of \\(\\alpha\\) used. To train a decision tree using different \\(\\alpha\\) values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf_alpha = DecisionTreeClassifier(min_samples_leaf= 5, \n                                       ccp_alpha=ccp_alpha, random_state=507134)\n    clf_alpha.fit(X_train, Y_train)\n    clfs.append(clf_alpha)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-36",
    "href": "Module2/Classification.slides.html#section-36",
    "title": "Classification Trees",
    "section": "",
    "text": "Now, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]\n\nThe .score() function computes the accuracy of a classification tree."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-37",
    "href": "Module2/Classification.slides.html#section-37",
    "title": "Classification Trees",
    "section": "",
    "text": "We visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#choosing-the-best-tree",
    "href": "Module2/Classification.slides.html#choosing-the-best-tree",
    "title": "Classification Trees",
    "section": "Choosing the best tree",
    "text": "Choosing the best tree\n\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0, min_samples_leaf= 5)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-38",
    "href": "Module2/Classification.slides.html#section-38",
    "title": "Classification Trees",
    "section": "",
    "text": "Once this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, feature_names = X_train.columns,\n    class_names=[\"genuine\", \"counterfeit\"], filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-39",
    "href": "Module2/Classification.slides.html#section-39",
    "title": "Classification Trees",
    "section": "",
    "text": "The accuracy of the pruned tree is:\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.97"
  },
  {
    "objectID": "Module2/Classification.slides.html#comments-on-accuracy",
    "href": "Module2/Classification.slides.html#comments-on-accuracy",
    "title": "Classification Trees",
    "section": "Comments on accuracy",
    "text": "Comments on accuracy\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading."
  },
  {
    "objectID": "Module2/Classification.slides.html#an-example",
    "href": "Module2/Classification.slides.html#an-example",
    "title": "Classification Trees",
    "section": "An example",
    "text": "An example\n\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-40",
    "href": "Module2/Classification.slides.html#section-40",
    "title": "Classification Trees",
    "section": "",
    "text": "In other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway."
  },
  {
    "objectID": "Module2/Classification.slides.html#another-example",
    "href": "Module2/Classification.slides.html#another-example",
    "title": "Classification Trees",
    "section": "Another example",
    "text": "Another example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-specific-metrics",
    "href": "Module2/Classification.slides.html#classification-specific-metrics",
    "title": "Classification Trees",
    "section": "Classification-specific metrics",
    "text": "Classification-specific metrics\n\nTo overcome this limitation of accuracy, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-41",
    "href": "Module2/Classification.slides.html#section-41",
    "title": "Classification Trees",
    "section": "",
    "text": "Sensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-42",
    "href": "Module2/Classification.slides.html#section-42",
    "title": "Classification Trees",
    "section": "",
    "text": "Precision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-43",
    "href": "Module2/Classification.slides.html#section-43",
    "title": "Classification Trees",
    "section": "",
    "text": "In Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n0.97\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.97"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-44",
    "href": "Module2/Classification.slides.html#section-44",
    "title": "Classification Trees",
    "section": "",
    "text": "Type I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-45",
    "href": "Module2/Classification.slides.html#section-45",
    "title": "Classification Trees",
    "section": "",
    "text": "Unfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n0.03"
  },
  {
    "objectID": "Module2/Classification.slides.html#discussion",
    "href": "Module2/Classification.slides.html#discussion",
    "title": "Classification Trees",
    "section": "Discussion",
    "text": "Discussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error."
  },
  {
    "objectID": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "href": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "title": "Classification Trees",
    "section": "Disadvantages of decision trees",
    "text": "Disadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "",
    "text": "Course topics\n\nModule 1\n\nIndicators (slides)\nSelection of Indicators (slides)\n\n\n\nModule 2\n\nIntroduction to Data Science (slides)\nIntroduction to Python and pandas (slides) (colab)\nData wrangling and visualization (slides) (colab) (pandas cheat sheet)\nDecision trees for classification (slides) (colab)\nEnsemble methods for classification (slides) (colab)\nK-nearest neighbours (slides) (colab)\n\n\n\nModule 3\n\nIntroduction to predictive models (slides) (colab)\nIntroduction to time series (slides) (colab)\nAutocorrelation models (slides) (colab)\nEnsemble methods for regression (slides) (colab)\n\n\n\nModule 4\n\nClustering methods (slides) (colab)\nPrincipal component analysis (slides) (colab)\n\n\n\n\nAbout the author\nAlan R. Vazquez is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\n\nLicense\nIN2004B Generation of Value with Data Analytics © 2025 by Alan R Vazquez is licensed under CC BY-NC-SA 4.0",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Module1/BSC.html",
    "href": "Module1/BSC.html",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Models for Defining Indicators\nIndicator Documentation\nDashboards for Visualizing Indicators"
  },
  {
    "objectID": "Module1/BSC.html#models-for-defining-indicators",
    "href": "Module1/BSC.html#models-for-defining-indicators",
    "title": "Selection of Indicators",
    "section": "Models for Defining Indicators",
    "text": "Models for Defining Indicators\n\nIntroduction\n\nOnce we know what indicators are, their role in the management process, and the types of indicators that exist, we ask ourselves:\n\n\nWhich indicators should we use?\nHow many indicators should we have?\nWhat is an appropriate set of indicators?\nHow do we document and share them?\n\n\n\n\n\n\nAlthough there are some indicators that could be generally applied to any company, each company has its own strategy, its own priorities, and its particular competitive environment\nTherefore, the most appropriate set of indicators depends on each organization.\n. . .\n\nHere we describe models for finding the appropriate number and set of indicators.\n\n\nModels for defining indicators\n\nThe models for defining indicators depend on the type of strategic planning.\nThree strategic planning frameworks are:\n\nManagement by Objectives (MBO; Peter Drucker, 1954).\nHoshin Kanri.\nBalanced Score Card (BSC; Kaplan & Norton, 1992, 1996).\n\n\n\nModels for defining indicators\n\nThe models for defining indicators depend on the type of strategic planning.\nThree strategic planning frameworks are:\n\nManagement by Objectives (MBO; Peter Drucker, 1954).\nHoshin Kanri.\nBalanced Scorecard (BSC; Kaplan & Norton, 1992, 1996).\n\n\n\nBalanced Scorecard (BSC)\n\n\n\nIt’s a model that helps organizations translate strategy into operational (measurable) objectives, resulting in actions, behaviors, and performance.\n\nThe BSC includes all critical success factors in a measurement system, giving organizations a better chance of achieving their goals.\n\n\n\n\n\n\n\n\n\n\nBSC’s objectives\n\n\n\nTranslate the strategy into measurable objectives.\nAlign the strategy components: objectives, indicators, and initiatives.\nCommunicate the strategy to the organization.\nCreate the basis for strategic management.\n\n\n\n\n\n\nBSC transforms strategy into an integrated system defined through four perspectives:\n\n\n\n\n\nFinancial\nClients\nInternal Processes\nLearning and Growth\n\n\n\n\n\n\n\n\n\n\n\n\nThe four perspectives\n\n\n\n\nFinancial. Includes objectives related to profitability, productivity, profits, stock price, etc.; these are the objectives that the organization must achieve from the shareholders’ perspective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomers. Includes objectives related to the company’s value proposition, are market-oriented, and are established from the customer’s perspective. Includes customer perception objectives regarding service, delivery time, quality, and value/price.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInternal Processes. Includes objectives related to the performance of processes that are critical to meeting customer perspective objectives. Performance objectives for the business’s primary value chain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning and growth. These are the objectives related to the enablers for achieving the objectives of the other perspectives. These are objectives related to competency development, the work environment, the physical environment, technological infrastructure, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlanning your birthday party\n\n\n\n\n\n\n\nModel components\n\nFor each perspective, the following should be defined:\n\nA small set of strategic objectives\nFor each objective, one (or more if necessary) metric as a performance indicator\nFor each indicator, establish long- and short-term goals\nInitiatives (programs, projects, actions) to close the gaps between current and desired performance according to the goals.\n\n\n\nStrategy map\n\n\nThe strategy map displays the strategic objectives within each perspective using a matrix.\nPossible causal relationships between objectives are also shown using arrows.\nIf the organization’s strategic charter (Vision and Mission) is reviewed, Strategic Themes can be included to show whether the organization is explicitly addressing these themes in its planning.\n\n\n\nStrategy map structure\n\n\n\n\n\n\n\nSeguimos la filosofia de BSC y lo definimos de arriba abajo. Pero se ejecuta de abajo hacia arriba. Los niveles superiores son consecuencia de los niveles inferiores. No necesariamente todos están ligados.\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore on strategy maps\n\n\n\nActivity 1.2 (cooperative mode)\n\n\nForm teams of three.\nBuild a strategy map as part of the Balanced Scorecard model application.\nRead the introduction to the Muebles Finos MF mini-case study in CANVAS.\nThe activity consists of mapping the company’s objectives to the perspectives of the BSC model and to the business’s strategic lines.\n\n\n\n\n\n\nOnce you have placed the objectives on the map, link them to each other, establishing cause-and-effect relationships between them.\nYou are also asked to select three of the objectives and propose at least one suggested metric for each.\nFinally, you will write the justification for the established causal relationships.\n\n\n\nFinal comments\n\nOnce the BSC model has been designed for the company, specific models are developed by functional area.\nIn the functional areas, work is based on the strategic objectives, identifying objectives specific to the functional area.\nIt is more common to use performance indicators than impact indicators at the departmental level.\nNot all strategic themes are necessarily maintained across functional areas, nor do all areas necessarily have objectives from all BSC perspectives.\n\n\n\n\n\n\nIt is essential that functional managers fully understand the company’s BSC and their area’s contribution to the overall objectives.\nThe person responsible for BSC deployment must ensure the consistency and alignment of the functional BSCs.\nSome organizational-level indicators are simply aggregations of departmental indicators."
  },
  {
    "objectID": "Module1/BSC.html#indicator-documentation",
    "href": "Module1/BSC.html#indicator-documentation",
    "title": "Selection of Indicators",
    "section": "Indicator Documentation",
    "text": "Indicator Documentation\n\nIndicator documentation\n\nWhen selecting and/or designing indicators, it is necessary to formally document the definition of each indicator.\nDocumentation is very useful because:\n\nIt helps clarify the meaning of the indicators.\nIt facilitates communication between users and indicator creators.\nIt serves as a future reference when revising the system.\n\n\n\nBasic documenting format"
  },
  {
    "objectID": "Module1/BSC.html#dashboards-for-visualizing-indicators",
    "href": "Module1/BSC.html#dashboards-for-visualizing-indicators",
    "title": "Selection of Indicators",
    "section": "Dashboards for Visualizing Indicators",
    "text": "Dashboards for Visualizing Indicators\n\nIndicator visualization\nOnce the indicators are defined using the Balanced Scorecard (BSC) and the documentation format, we design dashboards with:\n\n\nScreens.\nChart and table styles.\nAggregation levels.\nDefault reports.\nDrill-down requirements.\n\n\nThis applies to both the overall organizational level and the functional BSCs.\n\n\nDashboard deployment\nVarious strategies can be followed to implement the dashboards, for example:\n\nDevelopment on a specialized platform for indicator systems (https://www.predictiveanalyticstoday.com/open-source-balanced-scorecard-software/)\nDevelopment with generic OLAP Online Analytical Processing tools (https://www.softwaretestinghelp.com/best-olap-tools/)\nShort-term, isolated implementations in spreadsheets.\n\n\n\nTableu dashboard example\n\n\n\n\n\nhttps://www.tableau.com/es-mx\n\n\nAnother example\n\n\n\n\n\n\n\nGoogle Looker Studio\nFree online tool by Google for building interactive dashboards and reports. Works directly in the web browser and it connects to many data sources:\n\nGoogle Sheets\nCSV files\nBigQuery\nOther cloud and database services\n\n\n\n\n\n\n\n\nGetting started\n\nGo to https://lookerstudio.google.com/\nClick Blank Report\n\n\n\n\n\n\n\n\n\n\n\nSelect Microsoft Excel as your data source\n\n\n\n\n\n\n\n\n\n\nAdd the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe variables in the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graphics available.\n\n\n\n\n\n\n\n\n\n\nExample\n\nThe dataset “McDonalds Financial Statements.xlsx” includes McDonald’s annual financial data over two decades, offering insights into its economic performance. The dataset contains records of the following indicators:\n\n\n\nMarket cap ($B)\nRevenue ($B)\nEarnings ($B)\nP/E ratio\nP/S ratio\nP/B ratio\nOperating Margin\nEPS ($)\nShares Outstanding\n\n\n\n\n\nLet’s load the data into looker studio\n\n\n\n\n\nWe rename the report.\n\n\n\n\n\n\n\n\nLet’s insert our first table\n\n\n\n\n\nWe can place the table anywhere in the dashboard canvas.\n\n\n\n\n\n\n\nThe default table\n\n\n\n\n\n\n\n\nWe change the graphic type by going to its properties.\n\n\n\n\n\n\n\nSelect another variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can sort the table according to date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImproving the table\nWe can change the layout of the table by changing the metrics in the style of the table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange it to bars.\n\n\n\n\n\n\nShow the number too.\n\n\n\n\n\n\n\n\n\nThe result\n\n\n\n\n\n\n\nLet’s create a bar chart\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s change the variable or metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also add a box\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can edit the properties or layout of the number.\n\n\n\n\n\n\n\n\n\nSelect the mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can increase the font size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a text or a header to the report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe report\n\n\n\n\n\n\n\nTutorials on looker studio\n\n\n\nTips for effective dashboards\n\n\nKeep it simple — focus on the key indicators.\nUse color to draw attention, not decorate.\nProvide context: compare current value vs. target.\nTest your dashboard on different screen sizes."
  },
  {
    "objectID": "Module1/BSC.html#return-to-main-page",
    "href": "Module1/BSC.html#return-to-main-page",
    "title": "Selection of Indicators",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n[1] 2"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html",
    "href": "Module2/IntrotoDataScience.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Data Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#data-science",
    "href": "Module2/IntrotoDataScience.html#data-science",
    "title": "Introduction to Data Science",
    "section": "Data Science",
    "text": "Data Science\n\nData Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data.\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar concepts\n\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets.\n\n\n\n\nIn 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n. . .\n\n\n\n\n\n\n\n\n\n\n\n. . .\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology.\n\n\n\n\n\n\n\n\nA week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it.\n\n\n\n\n\n\n\n\n\n\nThe result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”\n\n\n\n\n\n\n\n\n\n\n\nCross-Industry Standard Process (CRISP) for Data Science\n\n\n\n\n\n\n\nCRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nDeployment: How do stakeholders access the results?\n\n\n\nBusiness understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process.\n\n\n\nProblems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n. . .\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”\n\n\n\n\nClustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”\n\n\nDiscussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems.\n\n\n\nData understanding I\n\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose.\n\n\n\n\nData understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort.\n\n\n\n\nExample\n\n\nIn the 1980s, credit cards had uniform pricing — companies lacked the systems for mass differential pricing.\nBy 1990, Richard Fairbanks and Nigel Morris saw that IT could power predictive models to customize offers (pricing, credit limits, low introductory rates, cash back, loyalty points).\nSignet Bank’s strategy: model profitability, not just probability of default, since a small fraction of customers generate most profits.\n\n\n\n\n\n\nProblem: They lacked data on how different credit terms affected profitability.\n\n. . .\n\nSolution: Acquire data at a cost — run experiments offering varied terms to different customers. Losses from these offers were considered investments in data.\n\n\n\nWhat happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business.\n\n\nRichard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#supervised-and-unsupervised-learning",
    "href": "Module2/IntrotoDataScience.html#supervised-and-unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised and Unsupervised Learning",
    "text": "Supervised and Unsupervised Learning\n\nTerminology\n\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a whole collection of \\(p\\) predictors.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict.\n\n\n\nTypes of learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning\n\n\n\n\n\n\n\nSupervised learning…\n\nIncludes algorithms that learn by example. That is, we provide the supervised algorithm with a data set with known predictor and response values. The algorithm must find a way to determine the responses from the predictors.\nSince we have the correct (true) responses, the algorithm can identify patterns in the data, learn from its mistakes, and make better predictions of the responses.\nThe algorithm is trained to reach a high level of accuracy and performance for predicting the responses.\n\n\nMathematically\n\nWe want to establish the following relationship\n\\[\nY = f(\\boldsymbol{X}) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error.\n. . .\n\n\\(f(\\boldsymbol{X})\\) represents the true relationship between the response (\\(Y\\)) and predictors (\\(\\boldsymbol{X}\\)).\n\n. . .\n\nHowever, \\(f(\\boldsymbol{X})\\) is unknown and very complex!\n\n\n\n\n\nA supervised algorithm attempts to construct an approximation \\(\\hat{f}(\\boldsymbol{X})\\) to the true function \\(f(\\boldsymbol{X})\\) using available data on the predictors and response.\nIdeally, the algorithm builds an \\(\\hat{f}(\\boldsymbol{X})\\) that is interpretable, but not necessarily.\n\n\nTwo data sets\nIn supervised learning, there are two main types of data:\n\nTraining data is data used by the supervised algorithm to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data NOT used in the algorithm’s training process, but is used to evaluate the quality of \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\n\n\n\nPopular supervised algorithms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised learning…\n\nstudies data of the predictors (\\(\\boldsymbol{X}\\)) to identify patterns. There are no responses.\nAn unsupervised algorithm identifies correlations and relationships by analyzing available training data. So, the unsupervised algorithm is left to interpret the data set and organize it in some way to describe its structure.\nIn technical terms, we want the algorithm to say something about the joint probability distribution of the predictors \\(P(X_1, X_2, \\ldots, X_p)\\).\n\n\nPopular Unsupervised Algorithms\n\n\n\n\n\n\n\nLet’s play with supervised models.\n\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#return-to-main-page",
    "href": "Module2/IntrotoDataScience.html#return-to-main-page",
    "title": "Introduction to Data Science",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/EnsembleMethods.html",
    "href": "Module2/EnsembleMethods.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Introduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#ensamble-methods",
    "href": "Module2/EnsembleMethods.html#ensamble-methods",
    "title": "Ensemble Methods",
    "section": "Ensamble Methods",
    "text": "Ensamble Methods\n\nLoad the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nDecision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, they can be combined to build effective algorithms for these problems.\n\n\n\nEnsamble methods\n\nEnsemble methods are frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Module2/EnsembleMethods.html#bagging",
    "href": "Module2/EnsembleMethods.html#bagging",
    "title": "Ensemble Methods",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\n\n\n\n\n\n\n\n\nBagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples.\n\n\n\n\n\n\n\nPredictions\n\nLet \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values.\n\nFor classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\boldsymbol{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively.\n\n\n\n\n\n\nUsing the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]\n\n\n\n\nCompute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} (\\text{\\#} \\hat{T}_{b}\\text{'s that output a } 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B} (\\text{\\#} \\hat{T}_{b}\\text{'s that output a } 1).\\]\n\n\n\n\n\nClassify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\).\n\n\n\nImplementation\n\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size.\n\n\n\n\nExample 1\n\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records.\nIn this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc.\nThe data contains 7,508 records.\n\n\n\nRead the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall\n\n\n\n\n\n\n\n\n\nSelected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nX_full = Adult_data.filter(['age', 'sex', 'race', 'education.num', \n                            'hours.per.week'])\n\n\n\nPre-processing categorical predictors\n\nUnfortunately, bagging does not work with categorical predictors. We must transform them into dummy variables using the code below.\n\n# Turn categorical predictors into dummy variables.\nX_dummies = pd.get_dummies(X_full[['sex', 'race']])\n\n# Drop original predictors from the dataset.\nX_other = X_full.drop(['sex', 'race'], axis=1)\n\n# Update the predictor matrix.\nX_full = pd.concat([X_other, X_dummies], axis=1)\n\n\n\nSet the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_data.filter(['income'])\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nincome_large\nincome_small\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1\n\n\n\n\n\n\n\n\n\n\n\nHere we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['income_large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: income_large, dtype: int64\n\n\n\n\nTraining and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2,\n                                                      stratify = Y_target,\n                                                      random_state=507134)\n\nWe use 80% of the dataset for training and the rest for validation.\n\n\nBagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm.\n\n\nPredictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 1, ..., 0, 0, 0])\n\n\n\n\nConfusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()\n\n\n\n\n\n\n\n\n\n\nAccuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77\n\n\n\n\nA single deep tree\nTo compare the bagging, let’s use a single deep tree.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.76\n\n\n\n\nAdvantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification.\n\n\n\nLimitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy.\n\n\n\nOther issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging.\n\n\n\n\n\n\nBagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#random-forest",
    "href": "Module2/EnsembleMethods.html#random-forest",
    "title": "Ensemble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further.\n\n\nWhy does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall.\n\n\n\nTuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging.\n\n\n\nThe final product is a black box\n\n\n\n\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you.\n\n\n\nRandom Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)\n\n\n\nConfusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#return-to-main-page",
    "href": "Module2/EnsembleMethods.html#return-to-main-page",
    "title": "Ensemble Methods",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/TimeSeries.html",
    "href": "Module3/TimeSeries.html",
    "title": "Introduction to Time Series",
    "section": "",
    "text": "Time Series\nLinear Regression Model for Time Series\nModels with Seasonality"
  },
  {
    "objectID": "Module3/TimeSeries.html#time-series",
    "href": "Module3/TimeSeries.html#time-series",
    "title": "Introduction to Time Series",
    "section": "Time Series",
    "text": "Time Series\n\nLoad the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nWhat is a time series?\n\n\nIt is a sequence of observations collected at successive time intervals.\nTime series data is commonly used in finance, economics, weather forecasting, signal processing, and many others.\nAnalyzing time series data helps us understand patterns, trends, and behaviors over time, enabling prediction, anomaly detection, and decision-making.\n\n\n\nExample 1: Tesla’s stock price\n\n\n\n\n\n\n\n\n\nTechnically, a time series is a set of observations about a (discrete) predictor \\(T\\) and a response \\(Y\\).\nObservations of \\(Y\\) are recorded at the moments or times given by the predictor \\(T\\).\nThe special feature of the time series is that the observations of \\(Y\\) are not independent!\n\n\n\n\n\nDay\nT\nTemperature (Y)\n\n\n\n\nMonday\n1\n10\n\n\nTuesday\n2\n12\n\n\nWednesday\n3\n15\n\n\nThursday\n4\n14\n\n\nFriday\n5\n18\n\n\n\n\n\n\n\n\nExample 2: Amtrak data\n\n\nThe Amtrak train company in the USA collects data on the number of passengers traveling on its trains.\nRecords are available from January 1991 to March 2004.\nThe data is available in “Amtrak.xlsx” on Canvas.\n\n\nAmtrak_data = pd.read_excel('Amtrak.xlsx')\n\n\n\n\n\n\nAmtrak_data.head()\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay\n\n\n\n\n\n\n\n\n\nTime series plot in Python\nWe can create a line graph to visualize the evolution of Amtrak train ridership over time using lineplot from seaborn.\n\n\nCode\nplt.figure(figsize=(6, 4))\nsns.lineplot(x='Month', y='Ridership (in 000s)', data = Amtrak_data)\nplt.xlabel('Month')\nplt.ylabel('Ridership')\nplt.title('Amtrak Ridership Over Time')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInformative Series\n\nAn informative time series is a series that contains patterns that we can use to predict future values of the series.\nThe three possible patterns are:\n\n\nTrend: the series has an increasing/decreasing behavior.\nSeasonality: the series has a repeating cyclical pattern in its values.\nAutocorrelation: the series follows a pattern that can be described by previous values of the series.\n\n\n\n\nExample 3: Airline data\n\n\n\n\n\nThis series has an upper trend.\nThis series has cyclical patterns in its values.\nAlthough not immediately visible, we can use the previous values of the series to describe the future ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-informative series: White noise\n\n\n\n\n\nWhite noise is a series whose values, on average, are 0 and have a constant variation.\nIts values are also independent of each other.\nIt is used to describe random or natural error."
  },
  {
    "objectID": "Module3/TimeSeries.html#linear-regression-model-for-time-series",
    "href": "Module3/TimeSeries.html#linear-regression-model-for-time-series",
    "title": "Introduction to Time Series",
    "section": "Linear Regression Model for Time Series",
    "text": "Linear Regression Model for Time Series\n\nLinear regression model\n\nThe linear regression model is useful for capturing patterns in a time series. In this context, the model takes the form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) at time \\(T_i\\).\n\n\n\nTrend\n\nThe trend of the time series is captured by the value of \\(\\hat{\\beta}_1\\) at\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nIf \\(\\hat{\\beta}_1\\) is positive, the series has an upward trend.\nIf \\(\\hat{\\beta}_1\\) is negative, the series has a downward trend.\n\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the least squares method.\n\n\nModel evaluation\n\nRemember that the errors of the linear regression model (\\(e_i = Y_i - \\hat{Y}_i\\)) must meet three conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nIn the context of time series, this means that the model errors \\(e_i\\) must behave like white noise that contains no patterns.\n\n\nExample 2: Amtrak data (cont.)\n\nLet’s fit a linear regression model to the ridership data from Amtrak.\n\n\n\n\n\n\n\n\n\n\n\nCreating a train and a validation data\n\n\nIn time series, the order of the data matters because each observation is tied to a specific point in time.\nBecause of this, we cannot randomly split the data using a function like train_test_split().\nDoing so might result in a situation where the model learns from future values to predict past ones—which doesn’t make sense and would lead to overly optimistic performance.\n\n\n\n\nInstead, we want to train the model on earlier time periods and test it on later ones.\nTo this end, we use the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% validation\nsplit_point = int(len(Amtrak_data) * split_ratio)\n\n# Split the data\nAmtrak_train = Amtrak_data[:split_point]\nAmtrak_validation = Amtrak_data[split_point:]\n\nThis code ensures that the training data always comes before the validation data in time, preserving the temporal order. The proportion of data that goes to training is set using split_ratio.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear regression model\nWe first set the predictor and response.\n\n# Set predictor.\nX_train = Amtrak_train.filter(['t'])\n\n# Set response.\nY_train = Amtrak_train.filter(['Ridership (in 000s)'])\n\n\nNext, we fit the model using LinearRegression() and fit() from scikit-learn.\n\n# 1. Create linear regression model.\nLRmodelAmtrak = LinearRegression()\n\n# 2. Fit the model to the training data.\nLRmodelAmtrak.fit(X_train, Y_train)\n\n\n\n\n\nLet’s inspect the estimated coefficient for the predictor (time).\n\nprint(LRmodelAmtrak.coef_)\n\n[[-1.2818326]]\n\n\nAnd the intercept.\n\nprint(LRmodelAmtrak.intercept_)\n\n[1810.77686661]\n\n\nThe estimated model then is:\n\\[\\hat{Y}_i = 1810.777 - 1.281 T_i.\\]\n\n\nResidual analysis\n\nWe can validate the model using a residual analysis on the training data. To this end, we first compute the predicted values and residuals of the model.\n\nfitted = LRmodelAmtrak.predict(X_train) + Y_train*0\nresiduals = Y_train - fitted\n\n# Construct a pandas data.frame\nresidual_data = pd.DataFrame()\nresidual_data[\"Fitted\"] = fitted\nresidual_data[\"Residuals\"] = residuals\nresidual_data[\"Time\"] = residuals.index\n\nRemember that we are using + Y_train*0 to visualize the objects fitted and residuals effectively.\n\n\n\n\n\n\n\n\nCode\n# Residual vs Fitted Values Plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = residual_data, x = \"Fitted\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = residual_data, x = \"Time\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model is more flexible than that\n\nIf necessary, the linear regression model can be extended to capture quadratic relationships. For this, the model takes the following form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i + \\hat{\\beta}_2 T^{2}_i \\]\n\nWhere \\(T^{2}_i\\) is the squared value of the time index.\n\\(\\hat{\\beta}_2\\) is a term that captures possible curvature in the time series.\n\n\n\nIn Python\nTo include a quadratic term, we must augment our predictor matrix with an additional column. The following code shows how to augment X_full by the square of the Amtrak_data['t'] column. This is done using the pandas .concat() function. The resulting matrix is stored in X_quad.\n\nX_quad = pd.concat([X_train, Amtrak_train['t']**2], axis = 1)\n\nNext, we follow the same steps to fit this model.\n\n# 1. Create linear regression model\nQuadmodelAmtrak = LinearRegression()\n\n# 2. Fit linear regression model\nQuadmodelAmtrak.fit(X_quad, Y_train)\n\n\n\n\n\nWe show the estimated coefficients in Python.\n\nprint(\"Intercept = \", QuadmodelAmtrak.intercept_)\nprint(\"Coefficients = \", QuadmodelAmtrak.coef_)\n\nIntercept =  [1866.84019635]\nCoefficients =  [[-4.64563238  0.03397778]]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 1866.84 - 4.65 T_i + 0.03 T^2_i.\\]\n\n\nResidual analysis\n\n\n\n\n\nCode\n# Remember to use the same `X_quad`\nY_pred_quad = QuadmodelAmtrak.predict(X_quad) + Y_train*0\nresiduals_quad = Y_train - Y_pred_quad\n# Construct a pandas data.frame\nresidual_data_quad = pd.DataFrame()\nresidual_data_quad[\"Fitted\"] = Y_pred_quad\nresidual_data_quad[\"Residuals\"] = residuals_quad\nresidual_data_quad[\"Time\"] = residuals_quad.index\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = residual_data_quad, x = \"Fitted\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data = residual_data_quad, x = \"Time\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel evaluation using validation data\n\nRemember that another way to evaluate the performance of a model is using the \\(\\text{MSE}_v\\) or \\(\\text{RMSE}_v\\) on the validation data.\n\nTo this end, we need some Python objects.\n\n# Set predictor.\nX_valid = Amtrak_validation.filter(['t'])\n\n# Set response.\nY_valid = Amtrak_validation.filter(['Ridership (in 000s)'])\n\n\n\n\nLet’s compute the \\(\\text{RMSE}_v\\) for the linear regression model.\n\nY_val_pred_lin = LRmodelAmtrak.predict(X_valid)\nmse = mean_squared_error(Y_valid, Y_val_pred_lin)  \nprint(round(mse**(1/2), 2))\n\n250.03\n\n\nLet’s do the same for the the linear regression model with a quadratic term.\n\nX_quad_valid = pd.concat([X_valid, Amtrak_validation['t']**2], axis = 1)\nY_val_pred_quad = QuadmodelAmtrak.predict(X_quad_valid)\nmse_quad = mean_squared_error(Y_valid, Y_val_pred_quad)  # Mean Squared Error (MSE)\nprint(round(mse_quad**(1/2), 2))\n\n173.99\n\n\nWe conclude that the linear model with a quadratic term is better than the linear regression model because the \\(\\text{RMSE}_v\\) of the former is smaller than for the latter.\n\n\n\nPredictions of linear model.\n\n\n\n\n\n\n\n\n\n\n\n\nPredictions of linear model with quadratic trend.\n\n\n\n\n\n\n\n\n\n\n\nActivity (solo mode)\nThe Office of Transportation Statistics of the Innovative Research and Technology Administration conducted a study to assess the impact of the September 11, 2001, terrorist attack on U.S. transportation. The report analyzes monthly passenger movement data from January 1990 to May 2004. Time series data are provided for (1) Real Revenue Passenger Miles Traveled (Air RPM), (2) Rail Passenger Miles Traveled (Rail PM), and (3) Car Miles Traveled (VMT).\nIn this activity, you will fit different linear regression models to data in the file “Sept11Travel.xlsx” in CANVAS.\n\n\nIdentifying Heteroskedasticity\n\nHeteroskedasticity arises when the dispersion of model errors is not constant over time.\nTo see it, let’s go back to the Airline data, which contains the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\n\n\n\n\n\n\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121\n\n\n\n\n\n\n\n\n\n\nFor illustrative purposes, we will not split the time series into training and validation datasets.\n\n\nCode\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_data)\nplt.xlabel('Time')\nplt.ylabel('Number of passengers')\nplt.title('Number of passengers across time')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model.\n\n# Set predictor.\nX_full = Airline_data.filter(['T'])\n\n# Set response.\nY_full = Airline_data.filter(['Number of passengers'])\n\n# 1. Create linear regression\nLRmodelAirline = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirline.fit(X_full, Y_full)\n\n\n\nResidual analysis\n\nHeteroskedasticity: Dispersion of the residuals increases with the predicted value.\n\n\n\nCode\n# Remember to use the same `X_quad`\nY_pred = LRmodelAirline.predict(X_full) + Y_full*0\nresiduals = Y_full - Y_pred\n\n# Construct a pandas data.frame\nresidual_data = pd.DataFrame()\nresidual_data[\"Fitted\"] = Y_pred\nresidual_data[\"Residuals\"] = residuals\nresidual_data[\"Time\"] = residuals.index\n\n\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data = residual_data, x = \"Fitted\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nIf we identify heteroskedasticity in the regression model errors, we have several transformation options for our original series.\n\nA common transformations to the time series \\(Y_i\\) is the Natural Logarithm\nIf the original time series contains negative values, it can be lagged by adding the negative of its minimum value.\n\n\n\nIn Python\nThe easiest way to apply the logarithm in Python is to use the log() function from the numpy library\n\nlog_Y_full = np.log( Y_full )\n\nNow, the response to use is in log_Y_full.\n\nThe steps to fit a linear regression model are similar.\n\n# 1. Create linear regression\nLRmodelAirlineTransformed = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirlineTransformed.fit(X_full, log_Y_full)\n\n\n\nResidual analysis\n\n\n\nWith transformation\n\n\nCode\n# Remember to use the same `X_quad`\nY_pred_log = LRmodelAirlineTransformed.predict(X_full) + log_Y_full*0\nresiduals_log = log_Y_full - Y_pred_log\n\n# Construct a pandas data.frame\nresidual_data_log = pd.DataFrame()\nresidual_data_log[\"Fitted\"] = Y_pred_log\nresidual_data_log[\"Residuals\"] = residuals_log\nresidual_data_log[\"Time\"] = residuals_log.index\n\n\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data = residual_data_log, x = \"Fitted\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWithout transformation\n\n\nCode\n# Remember to use the same `X_quad`\nY_pred = LRmodelAirline.predict(X_full) + Y_full*0\nresiduals = Y_full - Y_pred\n\n# Construct a pandas data.frame\nresidual_data = pd.DataFrame()\nresidual_data[\"Fitted\"] = Y_pred\nresidual_data[\"Residuals\"] = residuals\nresidual_data[\"Time\"] = residuals.index\n\n\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data = residual_data, x = \"Fitted\", y = \"Residuals\")\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do I do if the transformation doesn’t work?\n\n\nIf the log transformation doesn’t significantly reduce heteroskedasticity, there are models for modeling variance called GARCH.\nYou can find literature on these models and their software implementations in a time series textbook such as Time Series Analysis with Applications in R by Cryer and Chan."
  },
  {
    "objectID": "Module3/TimeSeries.html#models-with-seasonality",
    "href": "Module3/TimeSeries.html#models-with-seasonality",
    "title": "Introduction to Time Series",
    "section": "Models with Seasonality",
    "text": "Models with Seasonality\n\nSeasonality\nSeasonality refers to repetitive or cyclical behavior that occurs with a constant frequency.\n\n\n\n\nExamples:\n\nDemand for winter clothing\nDemand for tourist travel\nAmount of rainfall throughout the year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing seasonality\n\n\n\nThe linear regression model can be extended to capture seasonal patterns in the time series.\nTo do this, an additional categorical predictor is created that indicates the season to which each data item belongs.\nThe additional categorical predictor is transformed into several auxiliary numerical predictors.\n\n\n\n\n\n\n\n\nAnalyzing seasonal series in Python\nConsider the data in Amtrak_train with the additional predictor of Season to model seasonality.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay\n\n\n\n\n\n\n\n\n\n\nTo fit a linear regression model with a categorical variable like Season, we must transform the text categories into numbers. To do this, we use dummy variables constructed using the following commands.\n\ndummy_data = pd.get_dummies(Amtrak_train['Season'], dtype = 'int')\ndummy_data.head(4)\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\nThe matrix above contains one column for each month. Each column indicates the observations that belong to the month of the column. For example, the column Apr has the values 0 and 1. The value 1 indicates that the corresponding observation belongs to the month of April. A 0 indicates otherwise.\n\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nUnfortunately, we cannot use the matrix as is in the linear regression model due to multicollinearity issues. Technically, this happens because if you add all the columns, the resulting column is a column of 1s, which is already used by the intercept. Therefore, you cannot fit a model with the intercept and all the columns of the dummy variables.\n\nTo solve this problem, we arbitrarily remove a column from the matrix above. For example, let’s remove Dec.\n\ndummy_data = dummy_data.drop([\"Dec\"], axis = 1)\n\n\n\n\nNow, let’s build the complete matrix of predictors, including the column for time, time squared, and the dummy variables.\n\nX_quad_season = pd.concat([Amtrak_train['t'], Amtrak_train['t']**2, dummy_data], \n                          axis = 1)\n\n\nNext, we fit the model with all the terms in the matrix above.\n\n# 0. Ensure that we have the response in `Y_full`.\nY_train = Amtrak_train['Ridership (in 000s)']\n\n# 1. Create linear regression model.\nSeasonmodelAmtrak = LinearRegression()\n\n# 2. Fit the linear regression model.\nSeasonmodelAmtrak.fit(X_quad_season, Y_train)\n\n\n\nEstimated model coefficients\n\n\nprint(\"Intercept = \", SeasonmodelAmtrak.intercept_)\nprint(\"Coefficients = \", SeasonmodelAmtrak.coef_)\n\nIntercept =  1924.3205889003025\nCoefficients =  [-6.12171259e+00  4.96085283e-02  1.22309969e+01  1.77720172e+02\n -2.83088245e+02 -2.25659158e+02  1.15685204e+02 -7.31735621e+00\n  1.59876604e+01  5.17037414e+01 -4.77796001e+01 -3.19949172e+01\n -1.45366826e+02]\n\n\n\n\nResidual analysis\n\n\nCode\nY_pred = SeasonmodelAmtrak.predict(X_quad_season)\nresiduals = Y_train - Y_pred\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=Amtrak_data['t'], y=residuals)\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPredictions on the validation dataset\nPrepare the validation data using dummy variables.\n\nY_valid = Amtrak_validation['Ridership (in 000s)']\n\ndummy_valid = pd.get_dummies(Amtrak_validation['Season'], dtype = 'int')\ndummy_valid = dummy_valid.drop([\"Dec\"], axis = 1)\n\nX_qs_valid = pd.concat([Amtrak_validation['t'], Amtrak_validation['t']**2, \n                        dummy_valid], axis = 1)\n\n\nNow, we compute the validation \\(\\text{RMSE}_v\\).\n\nY_pred_valid = SeasonmodelAmtrak.predict(X_qs_valid)\n\nmse_season = mean_squared_error(Y_valid, Y_pred_valid) \nprint(round(mse_season**(1/2), 2))\n\n69.93\n\n\n\n\n\nPredictions of the linear model with seasonality.\n\n\n\n\n\n\n\n\n\n\n\nDisadvantages of linear regression\n\n\nDespite their simplicity and versatility, linear regression models are not the best for describing a time series.\nThis is because they do not assume a dependency between consecutive values in the time series. That is, they do not use the fact that, for example, \\(Y_1\\) can help us predict \\(Y_2\\), and \\(Y_2\\) can help us predict \\(Y_3\\), etc.\nModels that help us use past observations to predict future values of the response variable \\(Y\\) are autoregressive models.\n\n\n\nYogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "Module3/TimeSeries.html#return-to-main-page",
    "href": "Module3/TimeSeries.html#return-to-main-page",
    "title": "Introduction to Time Series",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/PredictiveModels.html",
    "href": "Module3/PredictiveModels.html",
    "title": "Introduction to Predictive Models",
    "section": "",
    "text": "Before we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module3/PredictiveModels.html#return-to-main-page",
    "href": "Module3/PredictiveModels.html#return-to-main-page",
    "title": "Introduction to Predictive Models",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module4/Clustering.html",
    "href": "Module4/Clustering.html",
    "title": "Clustering Methods",
    "section": "",
    "text": "Unsupervised Learning\nClustering Methods\nK-Means Method\nHierarchical Clustering"
  },
  {
    "objectID": "Module4/Clustering.html#unsupervised-learning",
    "href": "Module4/Clustering.html#unsupervised-learning",
    "title": "Clustering Methods",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nLoad the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn, and scipy libraries in Python.\n\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nUnsupervised learning\nGoal: organize or group data to gain insights. It answers questions like these\n\nIs there an informative way to visualize the data?\nCan we discover subgroups among variables or observations?\n\n. . .\nUnsupervised learning is more challenging than supervised learning because it is subjective and there is no simple objective for the analysis, such as predicting a response.\n. . .\nIt is also known as exploratory data analysis.\n\n\nExamples of unsupervised learning\n\n\nMarketing. Identify a segment of customers with a high tendency to purchase a specific product.\nRetail. Group customers based on their preferences, style, clothing choices, and store preferences.\nMedical Science. Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.\nSociology. Classify people based on their demographics, lifestyle, socioeconomic status, etc.\n\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical.\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.html#clustering-methods",
    "href": "Module4/Clustering.html#clustering-methods",
    "title": "Clustering Methods",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\nClustering methods\nThey group data in different ways to discover groups with common traits.\n\n\n\n\n\n\n\nClustering methods\n\nTwo classic clustering methods are:\n\n\nK-Means Method. We seek to divide the observations into K groups.\nHierarchical Clustering. We divide the n observations into 1 group, 2 groups, 3 groups, …, up to n groups. We visualize the divisions using a graph called a dendrogram.\n\n\n\n\nExample 1\nThe “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:\n\n\n\n\nBill length in millimeters.\nBill depth in millimeters.\nFlipper length in millimeters.\nBody mass in grams.\n\n\n\n\n\n\n\n\n\n\n\n\nData\n\npenguins_data = pd.read_excel(\"penguins.xlsx\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\nData visualization\nCan we group penguins based on their characteristics?\n\n\nCode\nplt.figure(figsize=(8, 5)) # Set figure size.\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\") # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.html#k-means-method",
    "href": "Module4/Clustering.html#k-means-method",
    "title": "Clustering Methods",
    "section": "K-Means Method",
    "text": "K-Means Method\n\nThe K-Means method\n\nGoal: Find K groups of observations such that each observation is in a different group.\n\n\n\n\n\nFor this, the method requires two elements:\n\n\nA measure of “closeness” between observations.\nAn algorithm that groups observations that are close to each other.\n\n\n. . .\nGood clustering is one in which observations within a group are close together and observations in different groups are far apart.\n\n\nDistance between observations\n\nFor quantitative predictors, we use the Euclidean distance.\nFor example, let’s consider two predictors \\(X_1\\) and \\(X_2\\) with observations given in the table:\n\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\n\n\n\n\nEuclidean distance\n\n\n\n\n\n\n\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }\\]\n\n\n\n\nWe can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, let’s consider three predictors:\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\\(X_{1,3}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\\(X_{2,3}\\)\n\n\n\n\nThe Euclidean distance is\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }\\]\n\n\nProblem with Euclidean distance\n\n\nThe Euclidean distance depends on the units of measurement of the predictors!\nPredictors with certain units have greater importance in calculating the distance.\nThis is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.\nThe solution is to standardize the units of the predictors.\n\n\n\nK-Means algorithm\n\n\n\n\nChoose a value for K, the number of groups.\n\nRandomly assign observations to one of the K groups.\nFind the centroids (average points) of each group.\nReassign observations to the group with the closest centroid.\nRepeat steps 3 and 4 until there are no more changes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (cont.)\nLet’s apply the algorithm to the predictors bill_depth_mm and bill_length_mm of the penguins dataset.\n\n\nCode\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm'])\nX_penguins.head()\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\n\n\n\n\n0\n18.7\n39.1\n\n\n1\n17.4\n39.5\n\n\n2\n18.0\n40.3\n\n\n3\n19.3\n36.7\n\n\n4\n20.6\n39.3\n\n\n\n\n\n\n\n\n\nStandarization\n\nSince the K-means algorithm works with Euclidean distance, we must standardize the predictors before we start. In this way, all of them will be equally informative in the process.\n\nWe do this using the function StandardScaler() and fit_transform().\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n\n\n\n\nIn Python, we use the KMeans() function of sklearn to apply K-means clustering. KMeans() tells Python we want to train a K-means clustering algorithm and .fit_predict() actually trains it using the data.\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\nThe argument n_clusters sets the desired number of clusters and random_state allows us to reproduce the analysis.\n\n\n\nThe clusters created are contained in the clusters object.\n\n\nCode\nclusters\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n       0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2], dtype=int32)\n\n\n\n\n\nTo visualize the clusters, we augment the original dataset X_penguins (without standarization) with the clusters object. usign the code below.\n\nclustered_penguins = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\nclustered_penguins.head(4)\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\nCluster\n\n\n\n\n0\n18.7\n39.1\n1\n\n\n1\n17.4\n39.5\n1\n\n\n2\n18.0\n40.3\n1\n\n\n3\n19.3\n36.7\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data = clustered_penguins, y = 'bill_length_mm', x = 'bill_depth_mm', \n                hue = 'Cluster', palette = 'Set1')\nplt.title('K-means Clustering of Penguins')\nplt.ylabel('Bill Length (mm)')\nplt.xlabel('Bill Depth (mm)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe truth: 3 groups of penguins\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\",\n                hue=\"species\", palette = 'Set1') # Define type of plot.\nplt.show() # Display the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s try using more predictors\n\n\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', \n                          'flipper_length_mm', 'body_mass_g'])\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\n# Save new clusters into the original data\nclustered_X = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are the three species\n\n\n\nAdelie\n\n\nGentoo\n\n\nChinstrap\n\n\n\n\n\n\nDetermining the number of clusters\n\nA simple way to determine the number of clusters (K) is recording the quality of clustering for different numbers of clusters.\nIn sklearn, we can record the inertia of a partition into clusters. Technically, the inertia is the sum of squared distances of observations to their closest cluster center.\nThe lower the intertia the better because this means that all observations are close to their cluster centers overall.\n\n\n\n\nTo record the intertias for different numbers of clusters, we use the code below.\n\ninertias = []\nk_max = 11 # Maximum number of clusters to test.\n\nfor i in range(1,k_max):\n    kmeans = KMeans(n_clusters=i) # Define K-means.\n    kmeans.fit(Xs_penguins) # Fit K-means to data.\n    inertias.append(kmeans.inertia_) # Save the inertia.\n\n\n\n\n\n\n\nNext, we plot the intertias and look for the elbow in the plot.\nThe elbow represents a number of clusters for which there is no significant improvement in the quality of the clustering.\nIn this case, the number of clusters recommended by this elbow method is 3.\n\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(range(1,k_max), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\n\nSelecting the number of clusters K is more of an art than a science. You’d better get K right, or you’ll be detecting patterns where none really exist.\nWe need to standardize all predictors.\nThe performance of K-means clustering is affected by the presence of outliers.\nThe algorithm’s solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported."
  },
  {
    "objectID": "Module4/Clustering.html#hierarchical-clustering",
    "href": "Module4/Clustering.html#hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nHierarchical clustering\n\n\n\n\n\n\nStart with each observation standing alone in its own group.\nThen, gradually merge the groups that are close together.\nContinue this process until all the observations are in one large group.\nFinally, step back and see which grouping works best.\n\n\n\n\n\n\n\n\n\n\nEssential elements\n\n\n\nDistance between two observations.\n\nWe use Euclidean distance.\nWe must standardize the predictors!\n\nDistance between two groups.\n\n\n\n\nDistance between two groups\n\n\n\n\nThe distance between two groups of observations is called linkage.\nThere are several types of linking. The most commonly used are:\n\nComplete linkage\nAverage linkage\n\n\n\n\n\n\n\n\nComplete linkage\nThe distance between groups is measured using the largest distance between observations.\n\n\n\n\n\n\n\nAverage linkage\nThe distance between groups is the average of all the distances between observations.\n\n\n\n\n\n\n\nHierarchical clustering algorithm\n\nThe steps of the algorithm are as follows:\n\n\nAssign each observation to a cluster.\nMeasure the linkage between all clusters.\nMerge the two most similar clusters.\nThen, merge the next two most similar clusters.\nContinue until all clusters have been merged.\n\n\n\n\nExample 2\n\nLet’s consider a dataset called “Cereals.xlsx.” The data includes nutritional information for 77 cereals, among other data.\n\ncereal_data = pd.read_excel(\"cereals.xlsx\")\n\n\n\n\nHere, we will restrict to 7 numeric predictors.\n\nX_cereal = cereal_data.filter(['calories', 'protein', 'fat', 'sodium', 'fiber',\n                              'carbo', 'sugars', 'potass', 'vitamins'])\nX_cereal.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n\n\n\n\n\n\n\n\n\nDo not forget to standardize\n\nSince the hierarchical clustering algorithm also works with distances, we must standardize the predictors to have an accurate analysis.\n\nscaler = StandardScaler()\nXs_cereal = scaler.fit_transform(X_cereal)\n\n\n\n\n\nUnfortunately, the Agglomerative() function in sklearn is not as user friendly compared to other available functions in Python. In particular, the scipy library has a function called linkage() for hierarchical clustering that works as follows.\n\nClust_Cereal = linkage(Xs_cereal, method = 'complete')\n\nThe argument method sets the type of linkage to be used.\n\n\nResults: Dendrogram\n\n\n\n\n\n\nA dendrogram is a tree diagram that summarizes and visualizes the clustering process.\nObservations are on the horizontal axis and at the bottom of the diagram.\nThe vertical axis shows the distance between groups.\nIt is read from top to bottom.\n\n\n\n\n\n\n\n\n\nWhat to do with a dendrogram?\n\n\n\n\nWe draw a horizontal line at a specific height to define the groups.\nThis line defines three groups.\n\n\n\n\n\n\n\n\n\n\n\n\nThis line defines 5 groups.\n\n\n\n\n\n\n\nDendrogram in Python\n\nTo produce a nice dendrogram in Python, we use the function dendrogram from scipy.\n\nplt.figure(figsize=(8, 4))\ndendrogram(Clust_Cereal)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColoring clusters\n\nThe dendrogram function colors a specific number of groups by default, but we can alter this behavior and color two, three, or more groups. To this end, w must determine the threshold to cut the dendrogram using the code below.\n\n# Define the number of clusters\nnum_clusters = 4\n\n# Compute the color threshold to form exactly 3 clusters\n# Find the distance at which to cut the dendrogram\ndistance_threshold = Clust_Cereal[-(num_clusters - 1), 2]\n\n\n\n\nWe specify the threshold to cut the dendrogram and set the groups using the color_threshold argument in dendrogram().\n\n\nCode\nplt.figure(figsize=(9, 4.5))\ndendrogram(Clust_Cereal, color_threshold = distance_threshold)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nComments\n\n\nRemember to standardize the predictors!\nIt’s not easy to choose the correct number of clusters using the dendrogram.\nThe results depend on the linkage measure used.\n\nComplete linkage results in narrower clusters.\nAverage linkage strikes a balance between narrow and thinner clusters.\n\nHierarchical clustering is useful for detecting outliers.\n\n\n\n\n\n\nWith these methods, there is no single correct answer; any solution that exposes some interesting aspect of the data should be considered.\n\nJames et al. (2017)"
  },
  {
    "objectID": "Module4/Clustering.html#return-to-main-page",
    "href": "Module4/Clustering.html#return-to-main-page",
    "title": "Clustering Methods",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Tools/Tools2.html",
    "href": "Tools/Tools2.html",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Basic Data Wrangling\nData Visualization"
  },
  {
    "objectID": "Tools/Tools2.html#basic-data-wrangling",
    "href": "Tools/Tools2.html#basic-data-wrangling",
    "title": "Data Wrangling and Visualization",
    "section": "Basic Data Wrangling",
    "text": "Basic Data Wrangling\n\nData wrangling\n\n\nData wrangling is the process of transforming raw data into a clean and structured format.\nIt involves merging, reshaping, filtering, and organizing data for analysis.\nHere, we illustrate some special functions of the pandas for cleaning common issues with a dataset.\n\n\n\nExample 1\nConsider an industrial engineer who receives a messy Excel file from a manufacturing client. The data file is called “industrial_dataset.xlsx”, which file includes data about machine maintenance logs, production output, and operator comments.\nThe goal is to clean and prepare this dataset using pandas so it can be analyzed.\nLet’s load pandas and read the data set into Python.\n\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\nclient_data = pd.read_excel(\"industrial_dataset.xlsx\")\n\n\n\n\n\n# Preview the dataset.\nclient_data.head()\n\n\n\n\n\n\n\n\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n\n\n3\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n\n\n4\n103\n950\n2023-01-13\nCharlie\nAll good\\n\n\n\n\n\n\n\n\n\n\nAdd an index column\n\nIn some cases, it is useful to have a unique identifier for each row in the dataset. We can create an identifier using the function .assign with some extra syntax.\n\nclient_data_single = (client_data\n                      .assign(ID = lambda x: x.index + 1)\n                      ) \n\n\n\n\nThe new column is appended to the end of the dataframe.\n\nclient_data_single.head()\n\n\n\n\n\n\n\n\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\nID\n\n\n\n\n0\n101\n1200\n2023-01-10\nAna\nok\n1\n\n\n1\n101\n1200\n2023-01-10\nAna\nok\n2\n\n\n2\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n3\n\n\n3\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n4\n\n\n4\n103\n950\n2023-01-13\nCharlie\nAll good\\n\n5\n\n\n\n\n\n\n\n\n\n\nTo bring it to the begining of the array, we can use the .filter() function.\n\nclient_data_single = (client_data_single\n.filter(['ID', 'Machine ID',    'Output (units)',   \n        'Maintenance Date', 'Operator', 'Comment'])\n)\nclient_data_single.head(3)  \n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n\n\n\n\n\n\n\n\n\nFill blank cells\nIn the dataset, there are columns with missing values. If we would like to fill them with specific values or text, we use the .fillna() function. In this function, we use the syntaxis 'Variable': 'Replace', where the Variable is the column in the dataset and Replace is the text or number to fill the entry in.\nLet’s fill in the missing entries of the columns Operator, Maintenance Date, and Comment.\n\ncomplete_data = (client_data_single\n                .fillna({'Operator': 'Unknown', \n                'Maintenance Date': '2023-01-01',\n                'Comment': 'None'})\n                ) \n\n\n\n\n\ncomplete_data.head()\n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n\n\n3\n4\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n\n\n4\n5\n103\n950\n2023-01-13\nCharlie\nAll good\\n\n\n\n\n\n\n\n\n\n\nReplace values\n\nThere are some cases in which columns have some undesired or unwatned values. Consider the Output (units) as an example.\n\ncomplete_data['Output (units)'].head()\n\n0     1200\n1     1200\n2     1050\n3    error\n4      950\nName: Output (units), dtype: object\n\n\nThe column has the numbers of units but also text such as “error”.\n\n\n\n\nWe can replace the “error” in this column by a user-specified value, say, 0. To this end, we use the function .replace(). The function has two inputs. The first one is the value to replace and the second one is the replacement value.\n\ncomplete_data['Output (units)'] = complete_data['Output (units)'].replace('error', 0)\n\n\n\n\n\nLet’s check the new column.\n\ncomplete_data['Output (units)']\n\n0     1200\n1     1200\n2     1050\n3        0\n4      950\n      ... \n95     800\n96    1100\n97     950\n98     950\n99    1100\nName: Output (units), Length: 100, dtype: int64\n\n\nNote that the new column is now numeric.\n\n\nSplit column into multiple ones\nThere are some cases in which we want to split a column according to a character. For example, consider the column Comment from the dataset.\n\ncomplete_data['Comment']\n\n0                       ok\n1                       ok\n2               Needs oil!\n3               All good\\n\n4               All good\\n\n              ...         \n95    Requires part: valve\n96                      ok\n97    Delay: maintenance\\n\n98              Needs oil!\n99              All good\\n\nName: Comment, Length: 100, dtype: object\n\n\n\n\n\n\nThe column has some values such as “Requires part: valve” and “Delay: maintenance” that we may want to split into columns.\n\n\n0                       ok\n1                       ok\n2               Needs oil!\n3               All good\\n\n4               All good\\n\n              ...         \n95    Requires part: valve\n96                      ok\n97    Delay: maintenance\\n\n98              Needs oil!\n99              All good\\n\nName: Comment, Length: 100, dtype: object\n\n\n\n\n\n\nWe can split the values in the column according to the colon “:”.\nThat is, everything before the colon will be in a column. Everything after the colon will be in another column. To achieve this, we use the function str.split().\nOne input of the function is the symbol or character for which we cant to make a split. The other input, expand = True tells Python that we want to create new columns.\n\ncomplete_data['Comment'].str.split(':', expand = True)\n\n\n\n\n\nThe result is two columns.\n\nsplit_column = complete_data['Comment'].str.split(':', expand = True)\nsplit_column.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nok\nNone\n\n\n1\nok\nNone\n\n\n2\nNeeds oil!\nNone\n\n\n3\nAll good\\n\nNone\n\n\n4\nAll good\\n\nNone\n\n\n\n\n\n\n\n\n\n\n\nWe can assign them to new columns in the dataset using the following code.\n\naugmented_data = (complete_data\n                  .assign(First_comment = split_column.filter([0]),\n                  Second_comment = split_column.filter([1]))\n                  )\n\n\n\n\n\naugmented_data.head()\n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\nFirst_comment\nSecond_comment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\nok\nNone\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\nok\nNone\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!\nNeeds oil!\nNone\n\n\n3\n4\n103\n0\n2023-01-13\nCharlie\nAll good\\n\nAll good\\n\nNone\n\n\n4\n5\n103\n950\n2023-01-13\nCharlie\nAll good\\n\nAll good\\n\nNone\n\n\n\n\n\n\n\n\n\nRemove characters\n\nSomething that we notice is that the column First_Comment has some extra characters like “” that may be useless when working with the data.\nWe can remove them using the function str.strip(). The input of the function is the character to remove.\n\naugmented_data['First_comment'] = augmented_data['First_comment'].str.strip(\"\\n\")\n\n\n\n\n\nLet’s see the cleaned column.\n\naugmented_data['First_comment']\n\n0                ok\n1                ok\n2        Needs oil!\n3          All good\n4          All good\n          ...      \n95    Requires part\n96               ok\n97            Delay\n98       Needs oil!\n99         All good\nName: First_comment, Length: 100, dtype: object\n\n\n\n\n\n\nWe can also remove other characters.\n\naugmented_data['First_comment'].str.strip(\"!\")\n\n0                ok\n1                ok\n2         Needs oil\n3          All good\n4          All good\n          ...      \n95    Requires part\n96               ok\n97            Delay\n98        Needs oil\n99         All good\nName: First_comment, Length: 100, dtype: object\n\n\n\n\nTransform text case\nWhen working with text columns such as those containing names, it might be possible to have different ways of writing. A common case is when having lower case or upper case names or a combination thereof.\nFor example, consider the column Operator containing the names of the operators.\n\ncomplete_data['Operator'].head()\n\n0       Ana \n1        Ana\n2        Bob\n3    Charlie\n4    Charlie\nName: Operator, dtype: object\n\n\n\n\nRemove extra spaces\n\nTo deal with names, we first use the .str.strip() to remove leading and trailing characters from strings.\n\ncomplete_data['Operator'] = complete_data['Operator'].str.strip()\ncomplete_data['Operator']\n\n0         Ana\n1         Ana\n2         Bob\n3     Charlie\n4     Charlie\n       ...   \n95    Charlie\n96        Ana\n97        Ana\n98    Charlie\n99        ana\nName: Operator, Length: 100, dtype: object\n\n\n\n\nChange to lowercase letters\n\nWe can turn all names to lowercase using the function str.lower().\n\ncomplete_data['Operator'].str.lower()\n\n0         ana\n1         ana\n2         bob\n3     charlie\n4     charlie\n       ...   \n95    charlie\n96        ana\n97        ana\n98    charlie\n99        ana\nName: Operator, Length: 100, dtype: object\n\n\n\n\nChange to uppercase letters\n\nWe can turn all names to lowercase using the function str.upper().\n\ncomplete_data['Operator'].str.upper()\n\n0         ANA\n1         ANA\n2         BOB\n3     CHARLIE\n4     CHARLIE\n       ...   \n95    CHARLIE\n96        ANA\n97        ANA\n98    CHARLIE\n99        ANA\nName: Operator, Length: 100, dtype: object\n\n\n\n\nCapitalize the first letter\n\nWe can convert all names to title case using the function str.title().\n\ncomplete_data['Operator'].str.title()\n\n0         Ana\n1         Ana\n2         Bob\n3     Charlie\n4     Charlie\n       ...   \n95    Charlie\n96        Ana\n97        Ana\n98    Charlie\n99        Ana\nName: Operator, Length: 100, dtype: object\n\n\n\n\nRemove duplicate rows\nDuplicate rows have the same entries in every column in the dataset. If only one row is needed for the analysis, we can remove the duplicates using .drop_duplicates(). For example, we can see the unique names of the operators.\n\noperators = complete_data['Operator'].str.title()\n\n(operators\n.drop_duplicates()\n)\n\n0         Ana\n2         Bob\n3     Charlie\n5     Unknown\n6        Dave\n14        Eve\nName: Operator, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.html#data-visualization",
    "href": "Tools/Tools2.html#data-visualization",
    "title": "Data Wrangling and Visualization",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nExample 2\n\nA criminologist is developing a rule-based system to classify the types of glasses encountered in criminal investigations.\nThe data consist of 214 glass samples labeled as one of seven class categories.\nThere are nine predictors, including refractive index and percentages of eight elements: Na, Mg, AL, Is, K, Ca, Ba, and Fe. The response is the type of glass.\n\n\n\n\nThe dataset is in the file “glass.xlsx”. Let’s load it using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nglass_data = pd.read_excel(\"glass.xlsx\")\n\n\nThe variable Type is categorical. So, let’s ensure Python knows this using the code below.\n\nglass_data['Type'] = pd.Categorical(glass_data['Type'])\n\n\n\nmatplotlib library\n\nmatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\nIt is widely used in the data science community for plotting data in various formats.\nIdeal for creating simple visualizations like line plots, bar charts, scatter plots, and more.\nhttps://matplotlib.org/\n\n\n\n\n\n\n\n\nseaborn library\n\nseaborn is a Python library built on top of Matplotlib.\nDesigned to make statistical data visualization easy and beautiful.\nIdeal for creating informative and attractive visualizations with minimal code.\nhttps://seaborn.pydata.org/index.html\n\n\n\n\n\n\n\n\nImporting the libraries\n\nThe matplotlib and seaborn libraries are pre-installed in Google Colab. However, we need to inform Google Colab that we want to use them and its functions using the following command:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSimilar to pandas, the command as sns allows us to have a short name for seaborn. Similarly, we rename matplotlib as plt.\n\n\nHistogram\n\nGraphical display that gives an idea of the “shape” of the sample, indicating regions where sample points are concentrated and regions where they are sparse.\n\nThe bars of the histogram touch each other. A space indicates that there are no observations in that interval.\n\n\nHistogram of Na\nTo create a histogram, we use the function histplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = glass_data, x = 'Na') # Create the histogram.\nplt.title(\"Histogram of Na\") # Plot title.\nplt.xlabel(\"Na\") # X label\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\n\n\n\nBox plot\n\nA box plot is a graphic that presents the median, the first and third quartiles, and any “outliers” present in the sample.\n\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile (\\(Q_3 - Q_1\\)). This is the distance needed to span the middle half of the data.\n\n\nAnatomy of a box plot\n\n\n\n\n\nSee also https://towardsdatascience.com/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097\n\n\nBox plot of Na\nTo create a boxplot, we use the function boxplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the figure.\nsns.boxplot(data = glass_data, y = 'Na') # Create boxplot.\nplt.title(\"Box plot of Na\") # Add title.\nplt.show() # Show the plot.\n\n\n\n\n\n\n\n\n\n\n\nOutliers\n\nOutliers are points that are much larger or smaller than the rest of the sample points.\nOutliers may be data entry errors or they may be points that really are different from the rest.\nOutliers should not be deleted without considerable thought—sometimes calculations and analyses will be done with and without outliers and then compared.\n\n\nScatter plot\n\nData for which items consists of a pair of numeric values is called bivariate. The graphical summary for bivariate data is a scatterplot.\nThe variables \\(X\\) and \\(Y\\) are placed on the horizontal and vertical axes, respectively. Each point on the graph marks the position of a pair of values of \\(X\\) and \\(Y\\).\nA scatterplot allows us to explore lineal and nonlinear relationships between two variables.\n\n\nScatter plot of Na versus RI\nTo create a scatter plot, we use the function scatter() from seabron. In this function, you must state the\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.scatterplot(data = glass_data, x = 'Na', y = 'RI') # Show the plot.\nplt.title(\"Scatter plot of Na vs RI\") # Set plot title.\nplt.xlabel(\"Na\") # Set label for X axis.\nplt.ylabel(\"RI\") # Set label for Y axis.\nplt.show() # Show plot.\n\n\n\n\n\n\n\n\n\n\n\nBar charts\nBar charts are commonly used to describe qualitative data classified into various categories based on sector, region, different time periods, or other such factors.\nDifferent sectors, different regions, or different time periods are then labeled as specific categories.\nA bar chart is constructed by creating categories that are represented by labeling each category and which are represented by intervals of equal length on a horizontal axis.\nThe count or frequency within the corresponding category is represented by a bar of height proportional to the frequency.\n\n\n\nWe create the bar chart using the function countplot() from seaborn.\n\n\nCode\n# Create plot.\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.countplot(data = glass_data, x = 'Type') # Show the plot.\nplt.title(\"Bar chart of Type of Glasses\") # Set plot title.\nplt.ylabel(\"Frequency\") # Set label for Y axis.\nplt.show() # Show plot.\n\n\n\n\n\n\n\n\n\n\n\nSaving plots\n\nWe save a figure using the save.fig function from matplotlib. The dpi argument of this function sets the resolution of the image. The higher the dpi, the better the resolution.\n\nplt.figure(figsize=(5, 7))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Frequency of Each Category')\nplt.ylabel('Frequency')\nplt.xlabel('Category')\nplt.savefig('bar_chart.png',dpi=300)\n\n\n\nImproving the figure\n\nWe can also use other functions to improve the aspect of the figure:\n\nplt.title(fontsize): Font size of the title.\nplt.ylabel(fontsize): Font size of y axis title.\nplt.xlabel(fontsize): Font size of x axis title.\nplt.yticks(fontsize): Font size of the y axis labels.\nplt.xticks(fontsize): Font size of the x axis labels.\n\n\n\n\n\nplt.figure(figsize=(5, 5))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Relative Frequency of Each Category', fontsize = 12)\nplt.ylabel('Relative Frequency', fontsize = 12)\nplt.xlabel('Category', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools2.html#return-to-main-page",
    "href": "Tools/Tools2.html#return-to-main-page",
    "title": "Data Wrangling and Visualization",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Tools/Tools1.html",
    "href": "Tools/Tools1.html",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "Introduction to Python\nReading data with Pandas"
  },
  {
    "objectID": "Tools/Tools1.html#introduction-to-python",
    "href": "Tools/Tools1.html#introduction-to-python",
    "title": "Introduction to Python and Pandas",
    "section": "Introduction to Python",
    "text": "Introduction to Python\n\nPython\n\n\n\n\nA versatile programming language.\nIt is free!\nIt is widely used for data cleaning, data visualization, and data modelling.\nIt can be extended with packages (libraries) developed by other users.\n\n\n\n\n\n\n\n\n\n\n\nGoogle Colab\nGoogle’s free cloud collaboration platform for creating Python documents.\n\nRun Python and collaborate on Jupyter notebooks for free.\nHarness the power of GPUs for free to accelerate your data science projects.\nEasily save and upload your notebooks to Google Drive.\n\n\n\n\n\n\n\n\nPython libraries\n\nLibraries are the fundamental units of reproducible Python code. They include reusable Python functions, documentation describing how to use them, and sample data.\nIn this course, we will be working mostly with the following libraries:\n\npandas for data manipulation\nmatplotlib and seaborn for data visualization\nstatsmodels and scikit-learn for data modelling"
  },
  {
    "objectID": "Tools/Tools1.html#reading-data-with-pandas",
    "href": "Tools/Tools1.html#reading-data-with-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Reading data with Pandas",
    "text": "Reading data with Pandas\n\nLoading data in Python\nWe assume that data is stored in an Excel file, where the rows are the observations and the columns contain the variables (e.g., predictors or responses). As an example, let’s use the file penguins.xlsx.\n\n\n\n\n\n\n\n\n\n\nThe dataset penguins.xlsx contains data from penguins living in three islands.\n\n\n\n\n\n\n\npandas library\n\n\n\n\n\n\n\n\n\n\npandas is an open-source Python library for data manipulation and analysis.\nIt is built on top of numpy for high-performance data operations.\nIt allows the user to import, clean, transform, and analyze data efficiently.\nhttps://pandas.pydata.org/\n\n\n\n\n\nImporting pandas\nFortunately, the pandas library is already pre-installed in Google Colab.\n\nHowever, we need to inform Google Colab that we want to use pandas and its functions using the following command:\n\nimport pandas as pd\n\n\nThe command as pd allows us to have a short name for pandas. To use a function of pandas, we use the command pd.function().\n\n\nLoading data using pandas\n\nThe following code shows how to read the data in the file “penguins.xlsx” into Python.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n\n\nThe function head()\nThe function head() allows you to print the first rows of a pandas data frame.\n\n# Print the first 4 rows of the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n\n\n\n\n\n\n\nIndexing variables a dataset\nWe can select a specific variables of a data frame using the syntaxis below.\n\npenguins_data['bill_length_mm']\n\n0      39.1\n1      39.5\n2      40.3\n3       NaN\n4      36.7\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 344, dtype: float64\n\n\nHere, we selected the variable bill_length_mm in the penguins_data dataset.\n\n\n\nTo index multiple variables of a data frame, we put the names of the variables in a list object. For example, we select bill_length_mm, species, and island as follows:\n\nsub_penguins_data = penguins_data[ ['bill_length_mm',  'species', 'island'] ]\nsub_penguins_data.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n0\n39.1\nAdelie\nTorgersen\n\n\n1\n39.5\nAdelie\nTorgersen\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen\n\n\n\n\n\n\n\n\n\nIndexing rows\nTo index rows in a dataset, we use the argument loc from pandas. For example, we select the rows 3 to 6 of the penguins_dataset dataset:\n\nrows_penguins_data = penguins_data.loc[2:5]\nrows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\n\n\n\n\n\nrows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\nNote that the index 2 and 5 refer to observations 3 and 7, respectively, in the dataset. This is because the first index in Python is 0.\n\n\nIndexing rows and columns\nUsing loc, we can also retrieve a subset from the dataset by selecting specific columns and rows.\n\nsub_rows_pdata = penguins_data.loc[2:5, ['bill_length_mm',  'species', 'island'] ]\nsub_rows_pdata\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen\n\n\n5\n39.3\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.html#data-manipulation-with-pandas",
    "href": "Tools/Tools1.html#data-manipulation-with-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Data manipulation with pandas",
    "text": "Data manipulation with pandas\n\nChaining operations with pandas\nOne of the most important techniques in pandas is chaining, which allows for cleaner and more readable data manipulation.\nThe general structure of chaining looks like this:\n\n\n\n\n\n\n\nKey pandas methods\npandas provides methods or functions to solve common data manipulation tasks:\n\n\n.filter() selects specific columns or rows.\n.query() filters observations based on conditions.\n.assign() adds new variables that are functions of existing variables.\n.sort_values() changes the order of rows.\n.agg() reduces multiple values to a single numerical summary.\n\n\n\n\n\n\n\n\n\n\nTo practice, we will use the dataset penguins_data.\n\n\nExample 1\nLet’s use the “penguins.xlsx” dataset.\n\n# Preview the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n\n\n\n\n\n\n\nSelecting columns with .filter()\nSelect the columns species, body_mass_g and sex.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\n3750.0\nmale\n\n\n1\nAdelie\n3800.0\nfemale\n\n\n2\nAdelie\n3250.0\nfemale\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\n3450.0\nfemale\n\n\n\n\n\n\n\n\n\n\n\nThe axis argument tells .filter() whether to select rows (0) or columns (1) from the dataframe.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\nThe .head() command allows us to print the first six rows of the newly produced dataframe. We must remove it to have the entire new dataframe.\n\n\n\n\n\nWe can also use .filter() to select rows too. To this end, we set axis = 1. We can select specific rows, such as 0 and 10.\n\n(penguins_data\n  .filter([0, 10], axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n10\nAdelie\nTorgersen\n37.8\n17.1\n186.0\n3300.0\nNaN\n2007\n\n\n\n\n\n\n\n\n\n\nOr, we can select a set of rows using the function range(). For example, let’s select the first 5 rows.\n\n(penguins_data\n  .filter(range(5), axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\nFiltering rows with .query()\n\nAn alternative way of selecting rows is .query(). Compared to .filter(), .query() allows us to filter the data using statements or queries involving the variables.\n\nFor example, let’s filter the data for the species “Gentoo.”\n\n(penguins_data\n  .query(\"species == 'Gentoo'\")\n)\n\n\n\n\n\n\n(penguins_data\n  .query(\"species == 'Gentoo'\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n152\nGentoo\nBiscoe\n46.1\n13.2\n211.0\n4500.0\nfemale\n2007\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n154\nGentoo\nBiscoe\n48.7\n14.1\n210.0\n4450.0\nfemale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007\n\n\n\n\n\n\n\n\n\n\nWe can also filter the data to get penguins with a body mass greater than 5000g.\n\n(penguins_data\n  .query(\"body_mass_g &gt; 5000\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007\n\n\n159\nGentoo\nBiscoe\n46.7\n15.3\n219.0\n5200.0\nmale\n2007\n\n\n161\nGentoo\nBiscoe\n46.8\n15.4\n215.0\n5150.0\nmale\n2007\n\n\n\n\n\n\n\n\n\n\nWe can even combine .filter() and .query(). For example, let’s select the columns species, body_mass_g and sex, then filter the data for the “Gentoo” species.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n  .query(\"species == 'Gentoo'\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n152\nGentoo\n4500.0\nfemale\n\n\n153\nGentoo\n5700.0\nmale\n\n\n154\nGentoo\n4450.0\nfemale\n\n\n155\nGentoo\n5700.0\nmale\n\n\n\n\n\n\n\n\n\nCreate new columns with .assign()\nWith .assign(), we can create new columns (variables) that are functions of existing ones. This function uses a special Python keyword called lambda. Technically, this keyword defines an anonymous function.\nFor example, we create a new variable LDRatio equaling the ratio of bill_length_mm and bill_depth_mm.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)\n\n\n\n\n\nIn this code, the df after lambda indicates that the dataframe (penguins_data) will be referred to as df inside the function. The colon : sets the start of the function.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)\n\nThe code appends the new variable to the end of the resulting dataframe.\n\n\n\nWe can see the new variable using .filter().\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"LDRatio\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nLDRatio\n\n\n\n\n0\n39.1\n18.7\n2.090909\n\n\n1\n39.5\n17.4\n2.270115\n\n\n2\n40.3\n18.0\n2.238889\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n1.901554\n\n\n\n\n\n\n\n\n\nSorting with .sort_values()\nWe can sort the data based on a column like bill_length_mm.\n\n(penguins_data\n  .sort_values(\"bill_length_mm\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n142\nAdelie\nDream\n32.1\n15.5\n188.0\n3050.0\nfemale\n2009\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n70\nAdelie\nTorgersen\n33.5\n19.0\n190.0\n3600.0\nfemale\n2008\n\n\n92\nAdelie\nDream\n34.0\n17.1\n185.0\n3400.0\nfemale\n2008\n\n\n\n\n\n\n\n\n\n\nTo sort in descending order, use ascending=False inside sort_values().\n\n(penguins_data\n  .sort_values(\"bill_length_mm\", ascending=False)\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n185\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nmale\n2007\n\n\n293\nChinstrap\nDream\n58.0\n17.8\n181.0\n3700.0\nfemale\n2007\n\n\n253\nGentoo\nBiscoe\n55.9\n17.0\n228.0\n5600.0\nmale\n2009\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n267\nGentoo\nBiscoe\n55.1\n16.0\n230.0\n5850.0\nmale\n2009\n\n\n\n\n\n\n\n\n\nSummarizing with .agg()\nWe can calculate summary statistics of the columns bill_length_mm, bill_depth_mm, and body_mass_g.\n\n(penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386\n\n\n\n\n\n\n\n\n\nBy default, agg() ignores missing values.\n\n\n\nSaving results in new objects\n\nAfter performing operations on our data, we can save the modified dataset as a new object.\n\nmean_penguins_data = (penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\nmean_penguins_data\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386\n\n\n\n\n\n\n\n\n\nMore on pandas\n\n\n\n\n\n\nhttps://wesmckinney.com/book/"
  },
  {
    "objectID": "Tools/Tools1.html#return-to-main-page",
    "href": "Tools/Tools1.html#return-to-main-page",
    "title": "Introduction to Python and Pandas",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module4/PCA.html",
    "href": "Module4/PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Introduction\nDispersion in one or more dimensions\nPrincipal component analysis"
  },
  {
    "objectID": "Module4/PCA.html#introduction",
    "href": "Module4/PCA.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nLoad the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, and sklearn libraries in Python.\n\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/PCA.html#dispersion-in-one-or-more-dimensions",
    "href": "Module4/PCA.html#dispersion-in-one-or-more-dimensions",
    "title": "Principal Component Analysis",
    "section": "Dispersion in one or more dimensions",
    "text": "Dispersion in one or more dimensions\n\nDispersion in one dimension\n\nThe concept of principal components requires an understanding of the dispersion or variability of the data.\nSuppose we have data for a single predictor.\n\n\n\n\n\n\n\n\nDispersion in two dimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\nA single predictor \\(X_2\\) captures much of the spread in the data.\n\n\n\n\n\n\n\n\n\n\nLet’s see another example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see another example\n\n\n\n\n\n\n\n\n\n\nA single predictor captures much of the dispersion in the data. In this case, the new predictor has the form \\(Z_1 = a X_1 + b X_2 + c.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, we can use two alternative dimensions to capture the dispersion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA new coordinate system\n\n\n\n\n\nThe new coordinate axis is given by two new predictors, \\(Z_1\\) and \\(Z_2\\). Both are given by linear equations of the new predictors.\nThe first axis, \\(Z_1\\), captures a large portion of the dispersion, while \\(Z_2\\) captures a small portion from another angle.\nThe new axes, \\(Z_1\\) and \\(Z_2\\), are called principal components."
  },
  {
    "objectID": "Module4/PCA.html#principal-component-analysis",
    "href": "Module4/PCA.html#principal-component-analysis",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nDimension Reduction\n\nPrincipal Components Analysis (PCA) helps us reduce the dimension of the data.\n\n\nIt creates a new coordinate axis in two (or more) dimensions.\nTechnically, it creates new predictors by combining highly correlated predictors. The new predictors are uncorrelated.\n\n\n\n\nSetup\n\nStep 1. We start with a database with \\(n\\) observations and \\(p\\) predictors.\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n15\n14\n5\n\n\n2\n1\n6\n\n\n10\n3\n17\n\n\n8\n18\n9\n\n\n12\n16\n11\n\n\n\n\n\n\nStep 2. We standardize each predictor individually.\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}\\]\n\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n\n1.15\n0.46\n-0.96\n\n\n\n-1.52\n-1.20\n-0.75\n\n\n\n0.12\n-0.95\n1.55\n\n\n\n-0.29\n0.97\n-0.13\n\n\n\n0.53\n0.72\n0.29\n\n\nSum\n0\n0\n0\n\n\nVariance\n1\n1\n1\n\n\n\n\n\n\n\nStep 3. We assume that the standardized database is an \\(n\\times p\\) matrix \\(\\mathbf{X}\\).\n\\[\\mathbf{X} = \\begin{pmatrix}\n1.15    &   0.46    &   -0.96   \\\\\n-1.52   &   -1.20   &   -0.75   \\\\\n0.12    &   -0.95   &   1.55    \\\\\n-0.29   &   0.97    &   -0.13   \\\\\n0.53    &   0.72    &   0.29    \\\\\n\\end{pmatrix}\\]\n\n\nAlgorithm\n\nThe PCA algorithm has its origins in linear algebra.\n\nIts basic idea is:\n\nCreate a matrix \\(\\mathbf{C}\\) with the correlations between the predictors of the matrix \\(\\mathbf{X}\\).\nSplit the matrix \\(\\mathbf{C}\\) into three parts, which give us the new coordinate axis and the importance of each axis.\n\n\n\nCorrelation matrix\n\nContinuing with our example, the correlation matrix contains the correlations between two columns of \\(\\mathbf{X}\\).\n\n\n\nPartitioning the correlation matrix\n\nThe \\(\\mathbf{C}\\) matrix is partitioned using the eigenvalue and eigenvector decomposition method.\n\n\n\n\n\n\n\nThe columns of \\(\\mathbf{B}\\) define the axes of the new coordinate system. These axes are called principal components (PCs).\nTechnically, the j-th PC is \\(\\mathbf{X} \\mathbf{b}_j\\) where \\(\\mathbf{b}_j\\)is the j-th column of \\(\\mathbf{B}\\).\n\n\n\n\n\n\n\nThe diagonal values in \\(\\mathbf{A}\\) define the individual importance of each principal component (axis).\n\n\n\nProportion of the dispersion explained by the component\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion of the dispersion in the data that is captured by the first component is \\(\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53\\) or 53%.\n\nRecall that \\(p\\) is the number of predictors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion captured by the second component is \\(\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36\\) or 36%.\nThe proportion captured by the third component is \\(\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11\\) or 11%.\n\n\n\n\n\n\nComments\n\nPrincipal components can be used to approximate a matrix.\nFor example, we can approximate the matrix \\(\\mathbf{C}\\) by setting the third component equal to zero.\n\n\\[\\begin{pmatrix}\n-0.68   &   0.35    &   0.00    \\\\\n-0.72   &   -0.13   &   0.00    \\\\\n0.16    &   0.93    &   0.00\\\\\n\\end{pmatrix} \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.00    \\\\\n\\end{pmatrix} \\begin{pmatrix}\n-0.68   &   -0.72   &   0.16    \\\\\n0.35    &   -0.13   &   0.93    \\\\\n0.00    &   0.00    &   0.00    \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.86    &   0.73    &   0.18    \\\\\n0.73    &   0.85    &   -0.30   \\\\\n0.18    &   -0.30   &   0.96    \\\\\n\\end{pmatrix}\\]\n\n\n\\[\\approx \\begin{pmatrix}\n1.00    &   0.58    &   0.11    \\\\\n0.58    &   1.00    &   -0.23   \\\\\n0.11    &   -0.23   &   1.00    \\\\\n\\end{pmatrix} = \\mathbf{C}\\]\n\n\n\n\n\n\n\n\n\nApproximations are useful for storing large matrices.\nThis is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\nThis is the idea behind image compression.\n\n\n\n\n\n\n\n\n\nExample\n\nConsider a database of the 100 most popular songs on TikTok in 2020. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements.\nEnergy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\nThe overall volume of a track in decibels (dB). Loudness values are averaged across the entire track.\n\n\n\n\n\nOther predictors are:\n\nSpeech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\nA confidence measure from 0 to 1 about whether the track is acoustic.\nDetects the presence of an audience in the recording.\nA measure from 0 to 1 that describes the musical positivity a track conveys.\n\n\n\nThe data\n\ntiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\ntiktok_data.head(3)\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nalbum\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\n0\nSay So\nDoja Cat\nHot Pink\n0.787\n0.673\n-4.583\n0.1590\n0.26400\n0.0904\n0.779\n110.962\n\n\n1\nBlinding Lights\nThe Weeknd\nAfter Hours\n0.514\n0.730\n-5.934\n0.0598\n0.00146\n0.0897\n0.334\n171.005\n\n\n2\nSupalonely (feat. Gus Dapperton)\nBENEE\nHey u x\n0.862\n0.631\n-4.746\n0.0515\n0.29100\n0.1230\n0.841\n128.978\n\n\n\n\n\n\n\n\n\nStandardize the data\n\nRemember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n\n# Select the predictors\nfeatures = ['danceability', 'energy', 'loudness', 'speechiness',\n            'acousticness', 'liveness', 'valence', 'tempo']\nX_tiktok = tiktok_data.filter(features)  \n\n# Standardize the data\nscaler = StandardScaler()\nXs_tiktok = scaler.fit_transform(X_tiktok)\n\n\n\nPCA in Python\n\nWe tell Python that we want to apply PCA using the function PCA() from sklearn. Next, we run the algorithm using .fit_transform().\n\npca = PCA()\nPCA_tiktok = pca.fit_transform(Xs_tiktok)\n\n\n\n\n\n\n\n\n\nThe Scree or Summary Plot tells you the variability captured by each component. From 1 to 8 components.\nThe first component covers most of the data dispersion.\nThis graph is used to define the total number of components to use.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to generate a scree plot is below.\n\nexplained_var = pca.explained_variance_ratio_\n\nplt.figure(figsize=(5, 5))\nplt.plot(range(1, len(explained_var) + 1), explained_var, \n         marker='o', linestyle='-')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.xticks(range(1, len(explained_var) + 1))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\nBiplot\n\n\n\n\nDisplays the observations on the new coordinate axis given by the first two components.\nHelps visualize data for 3 or more predictors using a two-dimensional scatter plot.\nA red line indicates the growth direction of the labeled variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to generate the biplot is lenghty but it can be broken into three steps.\nStep 1. Create a DataFrame with the PCA results\n\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\n0\n1.103065\n0.558086\n-0.800688\n0.446496\n0.605944\n-0.044089\n0.287325\n-0.413604\n\n\n1\n0.805080\n-0.766973\n1.580513\n-2.215856\n0.359655\n0.708123\n-0.882761\n0.113058\n\n\n2\n1.330433\n0.728161\n-0.288982\n0.376298\n0.786185\n-1.134308\n0.178388\n-0.242497\n\n\n3\n1.496277\n2.095014\n1.351398\n-0.621691\n0.390949\n0.494101\n0.024648\n-0.080720\n\n\n4\n-1.973362\n-0.966108\n-0.302071\n-1.266269\n0.414639\n-0.335677\n-0.076711\n-0.140126\n\n\n\n\n\n\n\n\n\n\nStep 2. Create biplot of first two principal components\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3. Add more information to the biplot.\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWith some extra lines of code, we label the points in the plot.\n\n\nCode\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df = (pca_df\n          .assign(songs = tiktok_data['track_name'])\n          )\n\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add labels for each song\nfor i in range(pca_df.shape[0]):\n    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n             pca_df['songs'][i], fontsize=8, alpha=0.7)\n\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.html#return-to-main-page",
    "href": "Module4/PCA.html#return-to-main-page",
    "title": "Principal Component Analysis",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/Nonparametric.html",
    "href": "Module3/Nonparametric.html",
    "title": "Ensemble Methods for Regression",
    "section": "",
    "text": "Introduction\nBagging\nRandom Forests\nBoosting\nEnsemble Methods for Time Series"
  },
  {
    "objectID": "Module3/Nonparametric.html#introduction",
    "href": "Module3/Nonparametric.html#introduction",
    "title": "Ensemble Methods for Regression",
    "section": "Introduction",
    "text": "Introduction\n\nLoad the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import BaggingRegressor, RandomForestRegressor\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.inspection import permutation_importance\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nDecision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision trees for regression\n\n\nThe procedure for constructing a decision tree for regression is the same as for a classification tree.\nThat is, we use the CART algorithm for building large trees and cost complexity pruning to reduce them, if desired.\nHowever, instead of using impurity to evaluate splits, we use the mean squared error.\n\n\n\n\n\nTo compute the mean squared error we use\n\nThe predicted response is the average response calculated in the node or region.\nThe actual responses are those of the observations in the region.\n\nWe refer to decision trees for regression as regression trees.\n\n\nExample 1\nThe “Hitters.xlsx” dataset contains data collected from Major League Baseball players. It includes data on various statistics such as number of hits, years in the league, and salary. The goal is to predict a player’s salary based on performance statistics.\n\nThe response is the player’s salary (in $), contained in the column Salary.\nFor this example, we will focus on two predictors:\n\nHits: Number of hits in the season\n\nYears: Number of years the player has been in the league\n\n\n\n\nRead the dataset\nWe read the dataset\n\nHitters_data = pd.read_excel('Hitters.xlsx')\nHitters_data.head()\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n0\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nNaN\nA\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n\n\n\n\n\n\n\n\nThe data set has some missing observations. So, we remove the observations with at least one missing value in a predictor.\n\nHitters_data_clean = Hitters_data.dropna()\nHitters_data_clean.head()\n\n\n\n\n\n\n\n\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n1\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n5\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n19\n501\n336\n194\nA\nW\n282\n421\n25\n750.0\nA\n\n\n\n\n\n\n\n\n\nGenerating training and validation data\n\nSelect the predictors and response\n\nX_full = Hitters_data_clean.filter([\"Years\", \"Hits\"])\nY_full = Hitters_data_clean.filter([\"Salary\"])\n\n\nWe partition the full dataset into 70% for training and the other 30% for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.3,\n                                                      random_state = 507134)\n\n\n\nRegression trees in Python\n\nWe train a regression tree using the DecisionTreeRegressor() function from regression.\n\n# We tell Python we want a regression tree\nreg_tree = DecisionTreeRegressor(min_samples_leaf = 5, max_depth=3, \n                              random_state=507134)\n\n# We train the regression tree using the training data.\nreg_tree.fit(X_train, Y_train)\n\nWe use max_depth = 3 to have a small tree as an example.\n\n\nPlotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib.\n\nplt.figure(figsize=(6, 6))\nplot_tree(reg_tree, feature_names = X_train.columns, filled=True, \n          rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the regions\n\n\n\n\n\n\n\n\n\n\n\nVisualize the predictions\nWe predict with the average response (\\(Y\\)) of the observations in each region.\n\n\n\n\n\n\n\n\n\n\n\nPredictions on the validation data\nTo predict the responses on the validation data, we use the function .predict() using the predictor values in the validation dataset contained in X_valid.\n\nY_pred = reg_tree.predict(X_valid)\nY_pred\n\narray([ 917.80951163,  601.96761111,  286.9375    ,  286.9375    ,\n        286.9375    ,  140.04411765,  917.80951163,  371.67336   ,\n        917.80951163,  917.80951163,  286.9375    ,  140.04411765,\n        286.9375    ,  140.04411765,  140.04411765,  601.96761111,\n        917.80951163,  371.67336   ,  917.80951163,  917.80951163,\n       1190.42853846,  233.33333333,  286.9375    ,  140.04411765,\n        917.80951163,  917.80951163,  233.33333333,  917.80951163,\n        140.04411765,  140.04411765,  140.04411765,  601.96761111,\n        601.96761111, 1190.42853846,  917.80951163,  233.33333333,\n        371.67336   ,  371.67336   ,  286.9375    ,  601.96761111,\n        917.80951163, 1190.42853846,  601.96761111,  371.67336   ,\n        917.80951163, 1190.42853846,  233.33333333,  917.80951163,\n        601.96761111, 1190.42853846,  917.80951163,  371.67336   ,\n       1190.42853846,  233.33333333,  917.80951163,  286.9375    ,\n        601.96761111,  371.67336   ,  233.33333333,  140.04411765,\n        233.33333333,  140.04411765,  917.80951163,  371.67336   ,\n       1190.42853846,  917.80951163,  917.80951163,  917.80951163,\n        917.80951163,  140.04411765,  286.9375    ,  917.80951163,\n        601.96761111,  140.04411765,  601.96761111,  601.96761111,\n        601.96761111,  601.96761111,  917.80951163])\n\n\n\n\nValidation RMSE\n\nRecall that the responses from the validation dataset are in Y_valid, and the model predictions are in Y_pred. We compute the root mean squared error on the validation data using the mse() function.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse**(1/2), 2)) # RMSE\n\n399.49\n\n\n\n\nLimitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, they can be combined to build effective algorithms for these problems.\n\n\n\nEnsamble methods\n\nEnsemble methods are frameworks to combine decision trees.\nHere, we will cover two popular ensamble methods:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests.\n\nBoosting. Ensemble small trees sequentially."
  },
  {
    "objectID": "Module3/Nonparametric.html#bagging",
    "href": "Module3/Nonparametric.html#bagging",
    "title": "Ensemble Methods for Regression",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\n\n\n\n\n\n\n\n\nBagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples.\n\n\n\n\n\n\n\nPredictions\nLet \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values.\nFor regression problems:\n\nEach regression tree outputs the average value depending on the region \\(\\boldsymbol{x}\\) falls in. For the b-th tree, denote such average as \\(\\hat{T}_{b}(\\boldsymbol{x})\\).\nPredict using the average of all regression trees\n\n\\[\\hat{f}_{bag}(\\boldsymbol{x}) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{T}_{b}(\\boldsymbol{x}).  \\]\n\n\nImplementation\n\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size.\n\n\n\n\nExample 2\n\nThe “BostonHousing.xlsx” contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\nThe goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\nThe response is the median value of owner-occupied homes in $1000s, contained in the column MEDV.\n\n\nThe predictors\n\n\nCRIM: per capita crime rate by town.\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS: proportion of non-retail business acres per town.\nCHAS: Charles River (‘Yes’ if tract bounds river; ‘No’ otherwise).\nNOX: nitrogen oxides concentration (parts per 10 million).\nRM: average number of rooms per dwelling.\nAGE: proportion of owner-occupied units built prior to 1940.\nDIS: weighted mean of distances to five Boston employment centers\nRAD: index of accessibility to radial highways (‘Low’, ‘Medium’, ‘High’).\nTAX: full-value property-tax rate per $10,000.\nPTRATIO: pupil-teacher ratio by town.\nLSTAT: lower status of the population (percent).\n\n\n\n\nRead the dataset\n\nWe read the dataset and set the variable CHAS and RAD as categorical.\n\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\nBoston_data['RAD'] = pd.Categorical(Boston_data['RAD'], \n                                      categories=[\"Low\", \"Medium\", \"High\"], \n                                      ordered=True)\n\n\n\n\n\n\nBoston_data.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\nNo\n0.538\n6.575\n65.2\n4.0900\nLow\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\nNo\n0.469\n6.421\n78.9\n4.9671\nLow\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\nNo\n0.469\n7.185\n61.1\n4.9671\nLow\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\nNo\n0.458\n6.998\n45.8\n6.0622\nLow\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\nNo\n0.458\n7.147\n54.2\n6.0622\nLow\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n\n\n\nGenerating predictors in Python\nWe use the function .drop() from pandas to drop the response column MEDV and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = Boston_data.drop(columns = ['MEDV']) \n\nUnfortunately, bagging does not work with categorical predictors such as CHAS and RAD. So, we must transform them into dummy variables using the code below.\n\n# Turn categorical predictors into dummy variables.\nX_dummies = pd.get_dummies(X_full[['CHAS', 'RAD']], drop_first = True, dtype = 'int')\n\n# Drop original predictors from the dataset.\nX_other = X_full.drop(['CHAS', 'RAD'], axis=1)\n\n# Update the predictor matrix.\nX_full = pd.concat([X_other, X_dummies], axis=1)\n\n\n\n\n\nNext, we use the function .filter() from pandas to extract the column MEDV from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = Boston_data.filter(['MEDV'])\n\n\nWe partition the full dataset into 80% for training and the other 20% for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.2,\n                                                      random_state = 507134)\n\n\n\nBagging in Python\nWe define a bagging algorithm for regression using the BaggingRegressor function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingRegressor(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm.\n\n\nPredictions on the validation data\nTo predict the responses on the validation data, we use the function .predict() using the predictor values in the validation dataset contained in X_valid.\n\nY_pred = Baggingalgorithm.predict(X_valid)\nY_pred\n\narray([19.6658, 13.7368, 21.1204, 44.2054, 25.0334, 34.5046, 30.0714,\n       14.9126, 23.185 , 20.2232, 17.0034, 20.0364, 19.7258,  8.9552,\n       24.2136, 29.2074, 21.12  , 20.8784, 25.4168, 18.6084, 22.7604,\n       26.452 , 24.025 , 35.9372, 23.5698, 20.2344,  8.3688, 15.9038,\n       47.4466, 24.2666, 34.4922, 23.2298, 24.4656, 13.3668, 22.6112,\n       12.7678, 27.8812, 19.8882, 22.7836, 18.9106,  9.0864, 24.6914,\n       21.9024, 12.3184, 27.1866, 33.0508, 19.436 , 25.7136, 19.4562,\n       25.3064, 26.8094, 16.9618, 20.8276, 28.7036, 27.884 , 12.5904,\n       23.2554, 38.432 , 20.3296, 22.0288, 17.894 , 15.8226, 13.5456,\n       21.0472, 23.069 , 20.6296, 20.5934, 15.8522, 32.7326, 33.4602,\n       15.3956, 17.5128, 21.2734, 20.6626, 29.7304, 15.4016, 18.9904,\n       19.0864, 32.4364, 21.6208, 22.1254, 20.5334, 23.8114, 21.0438,\n       34.0556, 21.5676, 19.8904, 19.025 , 21.4108, 15.335 , 46.5098,\n       23.533 ,  8.2252, 14.905 , 25.672 , 16.1982, 22.2198, 45.0716,\n       14.5424, 21.2396, 14.9436, 22.3806])\n\n\n\n\nValidation RMSE\n\nWe compute the root mean squared error on the validation data using the mse() function.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse**(1/2), 2)) # RMSE\n\n4.62\n\n\n\n\nA single deep tree\nTo compare the bagging, let’s use a single deep tree.\n\n# We tell Python that we want a classification tree\nreg_tree = DecisionTreeRegressor(ccp_alpha=0.0,\n                                 random_state=507134)\n\n# We train the classification tree using the training data.\nreg_tree.fit(X_train, Y_train)\n\nAnd compute the validation RMSE of that tree.\n\nreg_tree_Y_pred = reg_tree.predict(X_valid)\nmse = mean_squared_error(Y_valid, reg_tree_Y_pred) \nprint(round(mse**(1/2), 2)) \n\n5.87\n\n\nThe validation RMSE of the tree is higher than that of the bagging algorithm. So, ensembeling several trees improves the prediction performance.\n\n\nAdvantages\n\n\nBagging will have lower prediction errors than a single regression tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification. This estimate is called the out-of-bag error estimate which is an estimate of the RMSE on the test dataset.\n\n\n\nLimitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by \\(B\\).\nFundamental issue: bagging a good model can improve predictive performance, but bagging a bad one can seriously degrade it.\n\n\n\n\n\nBagging is unable to capture simple decision boundaries\n\n\n\n\n\n\n\n\nOther issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging."
  },
  {
    "objectID": "Module3/Nonparametric.html#random-forest",
    "href": "Module3/Nonparametric.html#random-forest",
    "title": "Ensemble Methods for Regression",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can improve the performance of bagging.\n\n\nWhy does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall.\n\n\n\nTuning parameter\n\nHow do we set \\(m\\)?\n\nFor regression, can use \\(m = \\sqrt{p}\\) and a minimum leaf node size of 5.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging.\n\n\n\nThe final product is a black box\n\n\n\n\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you.\n\n\n\nRandom Forest in Python\nIn Python, we define a RandomForest algorithm for classification using the RandomForestRegressor function from scikit-learn.\n\n# Set the random forest algorithm.\nRFalgorithm = RandomForestRegressor(n_estimators = 500,\n                                    min_samples_leaf = 5,\n                                    max_features = \"sqrt\",\n                                    random_state = 59227)\n\n# Train the random forest algorithm.\nRFalgorithm.fit(X_train, Y_train)\n\n\nn_estimators sets the number of decision trees to use, min_samples_leaf sets the minimum size for the terminal nodes, and max_features sets the maximum number of predictors to try in each split.\n\n\n\nValidation RMSE\n\nEvaluate the performance of random forest.\n\n# Predict responses in validation data.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute RMSE.\nmse = mean_squared_error(Y_valid, RF_predicted) \nprint(round(mse**(1/2), 2))\n\n4.63\n\n\n\n\nPredictor importance\n\n\nRandom Forests and bagged trees are a black box. But “Importance” is a statistic that attempts to give us some insight.\nThe importance statistic assumes that we have trained a random forest and we have validation data.\n\n\n\nCalculation of importance\n\n\n\nWe record the prediction error of the trained algorithm on the validation data. This serves as a baseline.\nNext, a variable is “taken out” by having all of its values permuted in the validation dataset. We then compute the prediction error of the algorithm on the perturbed validation data.\nThe importance statistic is the difference between the error due to the perturbation and the baseline. A larger difference means an important predictor for the algorithm.\n\n\n\n\n\n\nIn Python, we use compute the predictor importance using the permutation_importance() function.\n\n# Compute permutation importance\nresult = permutation_importance(RFalgorithm, X_valid, Y_valid,\n                                n_repeats=5, random_state=59227)\n\n\n\n\n\n\n\nCode\n# Create DataFrame\nimportance_df = pd.DataFrame({\n    'Feature': X_valid.columns,\n    'Importance': result.importances_mean\n}).sort_values(by='Importance', ascending=False)\n\n# Plot bar chart\nplt.figure(figsize=(8, 5))\nplt.bar(importance_df['Feature'], importance_df['Importance'])\nplt.xticks(rotation=90)\nplt.title('Permutation Feature Importance')\nplt.show()"
  },
  {
    "objectID": "Module3/Nonparametric.html#boosting",
    "href": "Module3/Nonparametric.html#boosting",
    "title": "Ensemble Methods for Regression",
    "section": "Boosting",
    "text": "Boosting\n\nBoosting\n\n\nIn boosting, we also grow multiple decision trees. But instead of growing trees randomly, each new tree depends on the previous one.\nBoosting is easier to understand in the context of regression, rather than classification.\nIdea: Explore cooperation between desicion trees, rather than diversity as in Bagging and Random Forest.\n\n\n\nBoosting for regression\n\nBoosting creates a sequence of trees, each one building upon the previous.\nEarlier trees are small, and the next tree is created with the residuals of the previous tree.\nIn other words, at each step, we try to explain the information that we didn’t explain in previous steps.\nGradually, the sequence “learns” to predict.\nSomething a little odd: earlier trees are deliberately “held back” to keep them from explaining too much. This creates “slow learning”.\n\n\n\n\n\n\n\n\n\nhttps://pythongeeks.org/gradient-boosting-algorithm-in-machine-learning/\n\n\n\n\nInitially, the boosted tree is \\(\\hat{f}(\\boldsymbol{x}) = 0\\) and the residuals of this tree are \\(r_i = y_i - f(\\boldsymbol{x}_i)\\) for all \\(i\\) in the training data.\nAt each step \\(b\\) in the process (\\(b = 1, \\ldots, B\\)), we\n\nBuild a regression tree \\(\\hat{T}_b\\) with \\(d\\) splits to the training data \\((\\boldsymbol{X}, \\boldsymbol{r})\\). This tree has \\(d+1\\) terminal nodes.\nUpdate the boosted tree \\(\\hat{f}\\) by adding in a shrunken version of the new tree: \\(\\hat{f}(\\boldsymbol{x}) \\leftarrow \\hat{f}(\\boldsymbol{x}) + \\lambda \\hat{T}_b(\\boldsymbol{x})\\).\nUpdate the residuals using shrunken tree, \\(r_i \\leftarrow r_i - \\lambda \\hat{T}_b(x_i)\\).\n\nThe final boosted tree is: \\(\\hat{f}(\\boldsymbol{x}) = \\sum_{b=1}^{B} \\lambda \\hat{T}_b (\\boldsymbol{x}).\\)\n\n\nD is 4 or 5, sometimes 1. Lambda is 0.001 or 0.01 This algorithm is a gradient boosting method for regression.\n\n\n\nWhy does this work?\n\n\nBy using a small tree, we are deliberately leaving information out of the first round of the model. So what gets fit is the “easy” stuff.\nThe residuals have all of the information that we haven’t yet explained. We continue iterating on the residuals, fitting them with small trees, so that we slowly explain the bits of the variation that are harder to explain.\nThis process is called “learning”: at each iteration, we get a better fit.\nBy multiplying by \\(\\lambda &lt;1\\), we “slow down” the learning (by making it harder to fit all of the variation), and there is research that says that slower learning is better.\n\n\n\n\nTuning parameters\n\nThe number of trees \\(B\\). Unlike bagging and random forest, boosting can overfit if \\(B\\) is too large. We use so-called K-fold cross-validation to select \\(B\\).\nThe shrinkage parameter \\(\\lambda\\), a small positive number. Typical values are 0.01 or 0.001. Very small \\(\\lambda\\) can require using a very large value of B to achieve good performance.\nThe number of splits \\(d\\) in each tree. Common choices are 1, 4 or 5. Often \\(d = 1\\), in which case each tree is a stump.\n\n\nBoosting in Python\nIn Python, we define a Boosting algorithm for classification using the GradientBoostingRegressor function from scikit-learn.\n\n# Set the boosting algorithm.\nGBalgorithm = GradientBoostingRegressor(learning_rate = 0.1,\n                                    n_estimators = 1000,\n                                    max_depth = 3,\n                                    random_state = 59227)\n\n# Train the boosting algorithm.\nGBalgorithm.fit(X_train, Y_train)\n\nn_estimators sets the number of decision trees to use, learning_rate sets the value of \\(\\lambda\\), and max_depth sets the depth of the tree.\n\n\nValidation RMSE for boosting\n\nEvaluate the performance of the boosting algorithm.\n\n# Predict responses in validation data.\nGB_predicted = GBalgorithm.predict(X_valid)\n\n# Compute RMSE.\nmse = mean_squared_error(Y_valid, GB_predicted) \nprint(round(mse**(1/2), 2))\n\n3.85\n\n\n\n\nPredictor importance\nWe can also apply predictor importance to the gradient boosting algorithm.\n\n\nCode\n# Compute permutation importance\nresult = permutation_importance(GBalgorithm, X_valid, Y_valid, n_repeats=5, random_state=59227)\n# Create DataFrame\nimportance_df = pd.DataFrame({\n    'Feature': X_valid.columns,\n    'Importance': result.importances_mean\n}).sort_values(by='Importance', ascending=False)\n\n# Plot bar chart\nplt.figure(figsize=(5, 3))\nplt.bar(importance_df['Feature'], importance_df['Importance'])\nplt.xticks(rotation=90)\nplt.title('Permutation Feature Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIssues with boosting\n\n\nLoss of interpretability: the final boosted model is a weighted sum of trees, which we cannot interpret easily.\nComputational complexity: since it uses slow learners, it can be time consuming. However, we are growing small trees, each step can be done relatively quickly in some cases (e.g. AdaBoost)."
  },
  {
    "objectID": "Module3/Nonparametric.html#ensemble-methods-for-time-series",
    "href": "Module3/Nonparametric.html#ensemble-methods-for-time-series",
    "title": "Ensemble Methods for Regression",
    "section": "Ensemble methods for Time Series",
    "text": "Ensemble methods for Time Series\n\nExample 3\nLet’s consider the dataset consisting of weekly cases of chickenpox (a childhood disease) in Hungary. The dataset has the number of reported cases in Budapest from January 2005 to December 2013.\nThe goal is to predict the number of reported cases weekly starting in 2014."
  },
  {
    "objectID": "Module3/Nonparametric.html#return-to-main-page",
    "href": "Module3/Nonparametric.html#return-to-main-page",
    "title": "Ensemble Methods for Regression",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/Autocorrelation.html",
    "href": "Module3/Autocorrelation.html",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Autocorrelation\nThe ARIMA model\nThe SARIMA model"
  },
  {
    "objectID": "Module3/Autocorrelation.html#the-arima-model",
    "href": "Module3/Autocorrelation.html#the-arima-model",
    "title": "Autocorrelation Models",
    "section": "The ARIMA Model",
    "text": "The ARIMA Model\n\nAutoregressive models\nAutoregressive models are a type of linear regression model that directly incorporate the autocorrelation of the time series to predict the current response.\nTheir main characteristic is that the predictors of the current value of the series are its past values.\n\n\nAn autoregressive model of order 2 has the mathematical form: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}.\\)\nA model of order 3 looks like this: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2} + \\hat{\\beta}_3 Y_{t-3}.\\)\n\n\n\n\nARIMA models\n\nA special class of autoregressive models are ARIMA (Autoregressive Integrated Moving Average).\n. . .\nAn ARIMA model consists of three elements:\n\n\nIntegrated operators (integrated).\nAutoregressive terms (autoregressive).\nStochastic terms (moving average).\n\n\n\n\n1. Integrated or differentiated operators (I)\n\nThey create a new variable \\(Z_t\\), which equals the difference between the current response and the delayed response by a number of periods or lags.\nThere are three common levels of differentiation:\n\nLevel 0: \\(Z_t = Y_t\\).\nLevel 1: \\(Z_t = Y_t - Y_{t-1}\\).\nLevel 2: \\(Z_t = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\).\n\n\n\nExample 2\nWe consider the time series “CanadianWorkHours.xlsx” that contains the average hours worked by a certain group of workers over a certain range of years.\n\nCanadianWorkHours = pd.read_excel('CanadianWorkHours.xlsx')\nCanadianWorkHours.head(4)\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n0\n1966\n37.2\n\n\n1\n1967\n37.0\n\n\n2\n1968\n37.4\n\n\n3\n1969\n37.5\n\n\n\n\n\n\n\n\n\nCreating a train and a validation data\n\nRecall that we would like to train the model on earlier time periods and test it on later ones. To this end, we make the split using the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(CanadianWorkHours) * split_ratio)\n\n# Split the data\nCanadian_train = CanadianWorkHours[:split_point]\nCanadian_validation = CanadianWorkHours[split_point:]\n\nWe use 80% of the time series for training and the rest for validation.\n\n\nTraining data\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='Year', y='Hours per Week', data = Canadian_train)\nplt.xlabel('Year')\nplt.ylabel('Hours per Week')\nplt.title('Hours per Week Over Time')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn statsmodels, we apply the integration operator using the pre-loaded diff() function. The function’s k_diff argument specifies the order or level of the operator.\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Canadian_train['Hours per Week'], k_diff = 1)\n\n\n\n\nThe time series with a level-1 operator looks like this.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_one)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 1')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nA level-2 operator would work like this.\n\nZ_series_two = diff(Canadian_train['Hours per Week'], k_diff = 2)\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_two)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the level 2 operator is more successful in removing the trend from the original time series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\n\nThe differentiation operator de-trends the time series.\n\nThe level 0 differentiation operator leaves the time series intact.\nThe level 1 differentiation operator removes its linear trend.\nThe level 2 differentiation operator removes its quadratic trend.\n\n\n\nHow do we determine the operator level?\n\nVisualizing the time series and determining whether there is a linear or quadratic trend.\nIf level 1 and level 2 operators yield similar results, we choose level 1 because it is simpler.\nOnce this is done, we set our transformed variable \\(Z_t\\) as the new response variable!\n\n\n2. Autoregressive (AR) terms\n\nHere we use autoregressive models, but with the new response variable \\(Z_t\\).\nWe can have different levels of order (or number of terms) in the autoregression model. For example:\n\nOrder 1 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1}\\).\nOrder 2 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2}\\).\n\nIf necessary, we can exclude the constant coefficient \\(\\hat{\\beta}_0\\) from the model.\n\n\nHow do we determine the number of terms?\nUsing the partial autocorrelation function (PACF) of the differenced series.\n\nA first-order autoregressive model has a PACF with a single peak at the first period difference (lag).\nA second-order autoregressive model has a PACF with two peaks at the first lags.\nIn general, the lag at which the PACF cuts off from the confidence limits in the software is the indicated number of AR terms.\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the PACF, we conclude that an autoregressive term of order 2 will be sufficient to capture the relationships between the elements of the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n3. Stochastic Terms (Moving Averages, MA)\nInstead of using past values of the response variable, a moving average model uses stochastic terms to predict the current response. The model has different versions depending on the number of errors used to predict the response. For example:\n\nMA of order 1: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1}\\);\nMA of order 2: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1} + \\theta_2 a_{t-2}\\),\n\nwhere \\(\\theta_0\\) is a constant and \\(a_t\\) are terms from a white noise series (i.e., random terms).\n\n\nHow do I choose the order of the MAs?\n\nUsing the autocorrelation function (ACF) of the differenced series.\n\nIn general, the lag at which the ACF cuts off from the confidence limits in the software is the indicated number of MA terms.\n\n\n\n\nSince there is no significant correlation for any lag above 0, we do not need any MA elements to model the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nARIMA\n\n\n\nDefine the response differentiation level and create \\(Z_t\\).\nDefine the order of the AR model (e.g., order 2).\nDefine the order of the MA model (e.g., order 1).\n\n\n. . .\n\n\\[\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2} + \\theta_1 a_{t-1}\\]\n\n. . .\nThe ARIMA model coefficients are estimated using an advanced method that takes into account the dependencies between the time series responses.\n\n\nARIMA in Pyhon\n\nTo fit an ARIMA model, we use the ARIMA() function from statsmodels.\nThe function has an important argument called order, which equals (p,d,q), where\n\np is the order of the autoregressive model.\nd is the level of the integration or differencing operator.\nq is the number of elements in the moving average.\n\n\n\n\n\nFrom our previous analysis of the training data for the Canadian workhours example, we conclude that:\n\nWe must use a level-2 differencing operator to remove the quadratic trend from the series. Therefore, d = 2.\nOne autoregressive term should be sufficient to capture the patterns in the time series. Therefore, p = 2.\nIt is not necessary to have moving average terms. Therefore, q = 0.\n\n\n\n\n\nOnce this is defined, we can train an ARIMA model using the training data with the following code:\n\nArima_Canadian = ARIMA(Canadian_train['Hours per Week'], \n                       order=(2, 2, 0))\nresults_ARIMA_Canadian = Arima_Canadian.fit()\n\nTechnically, ARIMA() defines the model and .fit() fits the model to the data using maximum likelihood estimation.\n\n\n\nAfter fitting, we can get a summary of the model fit using the following code.\n\nprint(results_ARIMA_Canadian.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:         Hours per Week   No. Observations:                   28\nModel:                 ARIMA(2, 2, 0)   Log Likelihood                  -2.537\nDate:                Mon, 01 Sep 2025   AIC                             11.074\nTime:                        10:39:30   BIC                             14.849\nSample:                             0   HQIC                            12.161\n                                 - 28                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.3236      0.403     -0.803      0.422      -1.113       0.466\nar.L2         -0.4401      0.250     -1.760      0.078      -0.930       0.050\nsigma2         0.0699      0.021      3.356      0.001       0.029       0.111\n===================================================================================\nLjung-Box (L1) (Q):                   0.26   Jarque-Bera (JB):                 3.02\nProb(Q):                              0.61   Prob(JB):                         0.22\nHeteroskedasticity (H):               1.48   Skew:                             0.74\nProb(H) (two-sided):                  0.57   Kurtosis:                         3.75\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\n\nThe next step in evaluating an ARIMA model is to study the model’s residuals to ensure there is nothing else to explain in the model.\n\nHowever, since the ARIMA model has two AR terms, we must inspect the residuals starting from the third observation. We achieve this using the code below.\n\n# Extract the residuals.\nARIMA_residuals = results_ARIMA_Canadian.resid\n\n# Select residuals starting from observation 3 and onwards.\nARIMA_residuals = ARIMA_residuals[2:]\n\n\n\nTime series of residuals\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=ARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCorrelation plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe three graphs show no obvious patterns or significant correlations between the residuals. Therefore, we say the model is good.\n\n\nForecast\nOnce the model is validated, we make predictions for elements in the time series.\n\nTo predict the average number of hours worked in the next, say, 3 years, we use the .forecast() function. The steps argument indicates the number of steps in the future to make the predictions.\n\nresults_ARIMA_Canadian.forecast(steps = 3)\n\n28    36.102917\n29    36.205233\n30    36.382827\nName: predicted_mean, dtype: float64\n\n\n\n\nModel evaluation using RMSE\n\n\nInstead of evaluating the ARIMA model using graphical analyses of the residuals, we can take a more data-driven approach and evaluate the model using the root mean squared error (RMSE).\nTo this end, we use the mean_squared_error() function with the validation responses and our predictions. We take the square root of the result to obtain the RMSE.\n\n\n\nValidation data\n\nCanadian_validation\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n28\n1994\n36.0\n\n\n29\n1995\n35.7\n\n\n30\n1996\n35.7\n\n\n31\n1997\n35.5\n\n\n32\n1998\n35.6\n\n\n33\n1999\n36.3\n\n\n34\n2000\n36.5\n\n\n\n\n\n\n\n\n\n\n\nThe validation data has 7 time periods that can be determined using the command below.\n\nlen(Canadian_validation)\n\n7\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Canadian = results_ARIMA_Canadian.forecast(steps = 7)\nforecast_Canadian\n\n28    36.102917\n29    36.205233\n30    36.382827\n31    36.580331\n32    36.738265\n33    36.900243\n34    37.078325\nName: predicted_mean, dtype: float64\n\n\n\n\n\n\nUsing the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Canadian_validation[\"Hours per Week\"], \n                         forecast_Canadian)  \nprint(round(mse**(1/2), 2))\n\n0.75\n\n\n\n\nNaive forecast\nTo determine if the RMSE is high or small, we must compare it with a naive forecast that predicts the response \\(Y_t\\) with the previous one \\(Y_{t-1}\\). To build this forecast, we follow three steps.\nFirst, we shift the time series in the validation data by one position using the .shit(1) function.\n\nnaive_forecast = Canadian_validation[\"Hours per Week\"].shift(1)\nnaive_forecast.head()\n\n28     NaN\n29    36.0\n30    35.7\n31    35.7\n32    35.5\nName: Hours per Week, dtype: float64\n\n\n\n\n\nSecond, we notice that the first element of the shifted series is missing or not available (NaN). We thus fill it in with the last observation in our training dataset, which we can obtain using the .iloc() function.\n\nCanadian_train.iloc[-1]\n\nYear              1993.0\nHours per Week      35.9\nName: 27, dtype: float64\n\n\n\nThird, we input 35.9 as the first record in naive_forecast. The first record of the object is indexed by a 28.\n\nnaive_forecast[28] = 35.9\n\n\n\n\nOur naive forecast of the time series in the validation data would be these one:\n\n\n\n\n\n\n\n\n\nNaive Forecast\nHours per week\n\n\n\n\n28\n35.9\n36.0\n\n\n29\n36.0\n35.7\n\n\n30\n35.7\n35.7\n\n\n31\n35.7\n35.5\n\n\n32\n35.5\n35.6\n\n\n33\n35.6\n36.3\n\n\n34\n36.3\n36.5\n\n\n\n\n\n\n\n\n\n\n\nThe RMSE of the naive forecast is calculated below.\n\nnaive_rmse = mean_squared_error(Canadian_validation[\"Hours per Week\"], \n                                naive_forecast)\nprint(round(naive_rmse**(1/2), 2))\n\n0.31\n\n\n\nWe conclude that predicting with the previous response is a better approach than predicting with an ARIMA model. This is because the RMSE for the naive forecast is 0.31, and that of the ARIMA model is 0.75."
  },
  {
    "objectID": "Module3/Autocorrelation.html#the-sarima-model",
    "href": "Module3/Autocorrelation.html#the-sarima-model",
    "title": "Autocorrelation Models",
    "section": "The SARIMA Model",
    "text": "The SARIMA Model\n\nSeasonality\n\n\nSeasonality consists of repetitive or cyclical behavior that occurs with a constant frequency.\nIt can be identified from the series graph or using the ACF and PACF.\nTo do this, we must have removed the trend.\n\n\n\nExample 3\nWe use the Airline data containing the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121\n\n\n\n\n\n\n\n\n\nCreate training and validation data\n\nWe use 80% of the time series for training and the rest for validation.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Airline_data) * split_ratio)\n\n# Split the data\nAirline_train = Airline_data[:split_point]\nAirline_validation = Airline_data[split_point:]\n\n\n\nTraining data\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_train)\nplt.xlabel('T')\nplt.ylabel('Number of passengers')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Airline_train['Number of passengers'], k_diff = 1)\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x=Airline_train['Number of passengers'], y=Z_series_one)\nplt.xlabel('T')\nplt.ylabel('Difference Level 1')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation plots\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSARIMA model\nThe SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an extension of the ARIMA model for modeling seasonality patterns.\nThe SARIMA model has three additional elements for modeling seasonality in time series.\n\nDifferenced or integrated operators (integrated) for seasonality.\nAutoregressive terms (autoregressive) for seasonality.\nStochastic terms or moving averages (moving average) for seasonality.\n\n\n\nNotation\n\nSeasonality in a time series is a regular pattern of change that repeats over \\(S\\) time periods, where \\(S\\) defines the number of time periods until the pattern repeats again.\nFor example, there is seasonality in monthly data, where high values always tend to occur in some particular months and low values always tend to occur in other particular months.\nIn this case, \\(S=12\\) (months per year) is the length of periodic seasonal behavior. For quarterly data, \\(S=4\\) time periods per year.\n\n\nSeasonal differentiation\n\nThis is the difference between a response and a response with a lag that is a multiple of \\(S\\).\nFor example, with monthly data \\(S=12\\),\n\nA level 1 seasonal difference is \\(Y_{t} - Y_{t-12}\\).\nA level 2 seasonal difference is \\((Y_{t-12}) - (Y_{t-12} - Y_{t-24})\\).\n\nSeasonal differencing eliminates the seasonal trend and can also eliminate a type of nonstationarity caused by a seasonal random walk.\n\n\nSeasonal AR and MA Terms\nIn SARIMA, the seasonal AR and MA component terms predict the current response (\\(Y_t\\)) using responses and errors at times with lags that are multiples of \\(S\\).\nFor example, with monthly data \\(S = 12\\),\n\n\nThe first-order seasonal AR model would use \\(Y_{t-12}\\) to predict \\(Y_{t}\\).\nThe second-order seasonal AR model would use \\(Y_{t-12}\\) and \\(Y_{t-24}\\) to predict \\(Y_{t}\\).\nThe first-order seasonal MA model would use the stochastic term \\(a_{t-12}\\) as a predictor.\nThe second-order seasonal MA model would use the stochastic terms \\(a_{t-12}\\) and \\(a_{t-24}\\) as predictors.\n\n\n\n\n\n\nTo fit the SARIMA model, we use the ARIMA() function from statsmodels, but with an additional argument, seasonal_order=(0, 0, 0, 0).\n\nThis is the order (P, D, Q, s) of the seasonal component of the model for the autoregressive parameters, differencing operator levels, moving average parameters, and periodicity.\n\nRecall that the function has the argument order = (p, d, q) where p is the order of the autoregressive model, d is the differencing operator level, and q is the number of elements in the moving average.\nThese arguments capture the detailed information of the time series, while seasonal_order captures the patterns given by seasonality.\n\n\n\n\n\nLet’s fit a SARIMA model.\n\nSARIMA_model = ARIMA(Airline_train['Number of passengers'], order=(1, 2, 1), \n                      seasonal_order=(1, 1, 0, 12))\nSARIMA_Airline = SARIMA_model.fit()\n\n\n\nSummary of fit\n\nprint(SARIMA_Airline.summary())\n\n                                     SARIMAX Results                                     \n=========================================================================================\nDep. Variable:              Number of passengers   No. Observations:                  115\nModel:             ARIMA(1, 2, 1)x(1, 1, [], 12)   Log Likelihood                -374.241\nDate:                           Mon, 01 Sep 2025   AIC                            756.482\nTime:                                   10:39:32   BIC                            766.943\nSample:                                        0   HQIC                           760.717\n                                           - 115                                         \nCovariance Type:                             opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.1729      0.094     -1.833      0.067      -0.358       0.012\nma.L1         -0.9999     14.439     -0.069      0.945     -29.300      27.300\nar.S.L12      -0.1303      0.084     -1.543      0.123      -0.296       0.035\nsigma2        91.7900   1326.009      0.069      0.945   -2507.139    2690.719\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 3.18\nProb(Q):                              0.98   Prob(JB):                         0.20\nHeteroskedasticity (H):               1.13   Skew:                             0.39\nProb(H) (two-sided):                  0.73   Kurtosis:                         2.64\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\nResidual analysis\nWe can have a graphical evaluation of the model’s performance using a residual analysis.\n\nSARIMA_residuals = SARIMA_Airline.resid\nSARIMA_residuals = SARIMA_residuals[2:]\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\nsns.lineplot(x=Airline_train['T'], y=SARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation\nThe validation data has 29 time periods that can be determined using the command below.\n\nlen(Airline_validation)\n\n29\n\n\n\nSo, we must forecast 7 periods ahead using our SARIMA model.\n\nforecast_Airline = SARIMA_Airline.forecast(steps = 29)\nforecast_Airline.head(3)\n\n115    489.635426\n116    428.961078\n117    373.169684\nName: predicted_mean, dtype: float64\n\n\n\n\n\n\nUsing the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Airline_validation[\"Number of passengers\"], \n                        forecast_Airline)  \nprint(round(mse**(1/2), 2))\n\n26.39\n\n\n\n\nComparison with a naive forecast\n\nNow, we build a naive forecast to study the performance of our SARIMA model. To this end, we shift the validation data by one period using the code below.\n\nnaive_forecast = Airline_validation[\"Number of passengers\"].shift(1)\nnaive_forecast.head()\n\n115      NaN\n116    505.0\n117    404.0\n118    359.0\n119    310.0\nName: Number of passengers, dtype: float64\n\n\n\n\n\nNext, we determine the last element of the training data.\n\nAirline_train.iloc[-1]\n\nT                       115\nNumber of passengers    491\nName: 114, dtype: int64\n\n\nFinally, we assemble our naive forecast by replacing the first entry of naive_forecast with the last entry of the response in the training dataset.\n\nnaive_forecast[115] = 491\nnaive_forecast.head()\n\n115    491.0\n116    505.0\n117    404.0\n118    359.0\n119    310.0\nName: Number of passengers, dtype: float64\n\n\n\n\n\n\nLet’s calculate the RMSE of the naive forecast.\n\nnaive_rmse = mean_squared_error(Airline_validation[\"Number of passengers\"], naive_forecast)\nprint(round(naive_rmse**(1/2), 2))\n\n52.49\n\n\nThe RMSE of the SARIMA model is 26.38, while that of the naive forecast is 54.15. Therefore, the SARIMA model is better than the naive forecast."
  },
  {
    "objectID": "Module3/Autocorrelation.html#return-to-main-page",
    "href": "Module3/Autocorrelation.html#return-to-main-page",
    "title": "Autocorrelation Models",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/Classification.html",
    "href": "Module2/Classification.html",
    "title": "Classification Trees",
    "section": "",
    "text": "Introduction\nTraining, Validation, and Test Data\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Module2/Classification.html#introduction",
    "href": "Module2/Classification.html#introduction",
    "title": "Classification Trees",
    "section": "Introduction",
    "text": "Introduction\n\nLoad the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nscikit-learn library\n\nscikit-learn is a robust and popular library for machine learning in Python\nIt provides simple, efficient tools for data mining and data analysis\nIt is built on top of libraries such as NumPy, SciPy, and Matplotlib\nhttps://scikit-learn.org/stable/\n\n\n\n\n\n\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors can be numerical or categorical.\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors can be numerical or categorical.\n\n\nTerminology\n\nRecall that\n\n\\(X\\) represents a predictor or explanatory variable.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors.\n\n\n\n\n\nResponse:\n\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\).\n\n\n\n\n\nClassification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (either target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n\n\nThe goal of classification algorithms\n\nGoal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n. . .\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n. . .\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification.\n\n\nExample 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n. . .\n\n\n\n\n\n. . .\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email.\n\n\n\n\nTechnically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\n\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n. . .\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]\n\n\nBayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\).\n\n\n\n\nHOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n. . .\nTo overcome this issue, we several methods:\n\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits).\n\n\n\n\n\n\nOnce we estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) using one of these methods, we plug it into the Bayes classifier:\n\\[\\hat{C}(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nwhere\n\n\\(\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) is an estimate of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\).\n\\(\\hat{C}(\\boldsymbol{X})\\) is an estimate of the true Bayes Classifier \\(C(\\boldsymbol{X})\\)"
  },
  {
    "objectID": "Module2/Classification.html#training-validation-and-test-data",
    "href": "Module2/Classification.html#training-validation-and-test-data",
    "title": "Classification Trees",
    "section": "Training, Validation, and Test Data",
    "text": "Training, Validation, and Test Data\n\nTwo datasets\n\nThe application of data science algorithms needs two data sets:\n\n\nTraining data is data that we use to train or construct the approximation \\(\\hat{C}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the classification performance of \\(\\hat{C}(\\boldsymbol{X})\\) only.\n\n\n\n\n\n\n\n\n\n\nA random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{C}(\\boldsymbol{X})\\).\n\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{C}(\\boldsymbol{X})\\).\n\n\n\n\nValidation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\n\n\n\n. . .\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset.\n\n\nExample 2: Identifying Counterfeit Banknotes\n\n\n\n\nDataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7\n\n\n\n\n\n\n\n\n\nHow do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set.\n\n\n\nCreate the predictor matrix\nWe use the function .filter() from pandas to select two predictors: Top and Bottom.\n\n# Set full matrix of predictors.\nX_full = bank_data.filter(['Top', 'Bottom'])\nX_full.head(4)\n\n\n\n\n\n\n\n\nTop\nBottom\n\n\n\n\n0\n9.7\n9.0\n\n\n1\n9.5\n8.1\n\n\n2\n9.6\n8.7\n\n\n3\n10.4\n7.5\n\n\n\n\n\n\n\n\n\nCreate the response column\nWe do the same to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data['Status']\nY_full.head(4)\n\n0    genuine\n1    genuine\n2    genuine\n3    genuine\nName: Status, dtype: category\nCategories (2, object): ['counterfeit', 'genuine']\n\n\n\n\nSet the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: counterfeit, dtype: int64\n\n\n\n\nLet’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3, \n                                                      stratify = Y_target_full,\n                                                      random_state=507134)\n\n\nThanks to the stratify parameter, the function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%.\n\n\n\n\nThe predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nTop\nBottom\ncounterfeit\n\n\n\n\n187\n10.6\n11.4\n1\n\n\n54\n10.0\n7.9\n0\n\n\n33\n10.3\n8.4\n0\n\n\n180\n10.7\n11.4\n1\n\n\n\n\n\n\n\n\n\n\nEquivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nTop\nBottom\ncounterfeit\n\n\n\n\n56\n10.4\n9.2\n0\n\n\n17\n9.0\n9.0\n0\n\n\n24\n10.8\n7.4\n0\n\n\n20\n10.0\n8.4\n0\n\n\n139\n10.2\n11.8\n1\n\n\n\n\n\n\n\n\n\nWork on your training dataset\n\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nAlgorithm development.\n\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your algorithm!"
  },
  {
    "objectID": "Module2/Classification.html#classification-trees",
    "href": "Module2/Classification.html#classification-trees",
    "title": "Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nDecision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique.\n\n\n\nBasic idea of a decision tree\nStratify or segment the predictor space into several simpler regions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it.\n\n\nHow do we grow a tree?\n\n. . .\nUsing the CART algorithm!\n\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe repeat the partitioning process until:\n\nimpurity does not improve in any of the terminal nodes, or\neach terminal node has no less than, say, 5 observations.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes.\n\n\nHow do we measure impurity?\n\n\n\n\nThere are three different metrics for impurity:\n\nMisclassification risk.\nCross entropy.\nGini impurity index.\n\n\n \n\np: Proportion of elements in the target class\n\n\n\n\n\n\nMathematically\n\nLet \\(p_1\\) and \\(p_2\\) be the proportion of observations in the target and reference class, respectively, in a node.\n\nMisclassification risk: \\(1 - \\max\\{p_1, p_2\\}\\)\nCross entropy: \\(- (p_1 \\log(p_1) + p_2 \\log(p_2))\\)\nGini impurity index: \\(p_1(1 - p_1) + p_2(1 - p_2)\\)\n\n\n\nIn Python\nIn Python, we use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a classification tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(min_samples_leaf= 5, ccp_alpha=0, \n                              random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameter min_samples_leaf controls the minimum number of observations in a terminal node, and the cc_alpha controls the tree complexity (to be described later). The parameter random_state allows you to reproduce the same tree in different runs of the Python code.\n\n\nPlotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, feature_names = X_train.columns,\n    class_names=[\"genuine\", \"counterfeit\"], filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tree in the predictor space\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated conditional probabilities\n\nAfter training a classification tree, we can calculate the estimated conditional probability \\(\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\).\nTo this end, let\n\n\\(\\hat{p}_1(\\boldsymbol{x}) = \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)\n\\(\\hat{p}_0(\\boldsymbol{x}) = 1 - \\hat{p}_1(\\boldsymbol{x})\\)\n\nbe the estimated probabilities that \\(\\boldsymbol{x}\\) belongs to class 1 or 0.\n\n\n\n\nEssentially, we calculate \\(\\hat{p}_1(\\boldsymbol{x})\\) and \\(\\hat{p}_0(\\boldsymbol{x})\\) as follows:\n\nSelect the region or terminal node where \\(\\boldsymbol{x}\\) belongs.\n\\(\\hat{p}_1(\\boldsymbol{x})\\) is the proportion of observations in that terminal node that belong to class 1. \\(\\hat{p}_0(\\boldsymbol{x})\\) is the proportion of observations that belong to class 0.\n\n\n\nVisually\nLet \\((\\hat{p}_0(\\boldsymbol{x}), \\hat{p}_1(\\boldsymbol{x}))\\) be the estimated probabilities.\n\n\n\n\n\n\n\n\n\n\n\nEstimated Bayes Classifier\n\nOnce we have estimated the conditional probability using the classification tree, we plug it into the Bayes classifier to have our approximated function:\n\\[\\hat{C}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases},\\]\nwhere \\(\\hat{p}_1(\\boldsymbol{x})\\) depends on the region or terminal node \\(\\boldsymbol{x}\\) falls in.\n\n\nSimplified decision boundary\n\n\n\n\n\n\n\n\n\n\n\n\nDecision boundary logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability."
  },
  {
    "objectID": "Module2/Classification.html#classification-metrics",
    "href": "Module2/Classification.html#classification-metrics",
    "title": "Classification Trees",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nEvaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)\n\n\n\n\n\nThe predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.71428571, 0.28571429],\n       [1.        , 0.        ],\n       [0.11111111, 0.88888889],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ]])\n\n\n\n\nConfusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems.\n\n\n\n\n\n\n\n\nIn Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[29  1]\n [ 1 29]]\n\n\n\n\n\n\nWe can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nCMplot = ConfusionMatrixDisplay(cm, display_labels = [\"genuine\", \"counterfeit\"])\nCMplot.plot()\n\n\n\n\n\n\n\n\n\n\nAccuracy\nIt is a simple metric for summarizing the information in the confusion matrix. It is the proportion of correct classifications for both classes, out of the total classifications performed.\n\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.97\n\n\nThe higher the accuracy, the better the performance of the classification algorithm.\n\n\nPruning the tree\nIn some cases, we can optimize the performance of the tree by pruning it. That is, we collapse two internal (non-terminal) nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThis algorithm minimizes the following function:\n\n\\[\\text{Missclassification rate of tree} + \\alpha (\\text{\\# terminal nodes}),\\]\n\nwhere \\(\\alpha\\) is a tuning parameter that places greater weight on the number of tree nodes (or size).\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes.\n\n\n\nApply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of \\(\\alpha\\), which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas = path.ccp_alphas\n\n\n\n\n\nThe ccp_alphas object contains the different values of \\(\\alpha\\) used. To train a decision tree using different \\(\\alpha\\) values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf_alpha = DecisionTreeClassifier(min_samples_leaf= 5, \n                                       ccp_alpha=ccp_alpha, random_state=507134)\n    clf_alpha.fit(X_train, Y_train)\n    clfs.append(clf_alpha)\n\n\n\n\n\nNow, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]\n\nThe .score() function computes the accuracy of a classification tree.\n\n\n\nWe visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nChoosing the best tree\n\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0, min_samples_leaf= 5)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\n\n\nOnce this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, feature_names = X_train.columns,\n    class_names=[\"genuine\", \"counterfeit\"], filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy of the pruned tree is:\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.97\n\n\n\n\nComments on accuracy\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading.\n\n\n\nAn example\n\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay.\n\n\n\n\n\n\nIn other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway.\n\n\n\nAnother example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease.\n\n\n\nClassification-specific metrics\n\nTo overcome this limitation of accuracy, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix.\n\n\n\n\n\n\n\n\nSensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”\n\n\n\n\n\n\n\n\nPrecision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?\n\n\n\n\nIn Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n0.97\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.97\n\n\n\n\n\n\n\n\n\n\nType I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”\n\n\n\n\nUnfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n0.03\n\n\n\n\nDiscussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error.\n\n\n\nDisadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "Module2/Classification.html#return-to-main-page",
    "href": "Module2/Classification.html#return-to-main-page",
    "title": "Classification Trees",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/KNearestNeighbours.html",
    "href": "Module2/KNearestNeighbours.html",
    "title": "K nearest neighbors",
    "section": "",
    "text": "Before we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/KNearestNeighbours.html#return-to-main-page",
    "href": "Module2/KNearestNeighbours.html#return-to-main-page",
    "title": "K nearest neighbors",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module1/Indicators.html",
    "href": "Module1/Indicators.html",
    "title": "Indicators",
    "section": "",
    "text": "Basic Indicator Concepts\nLagging and Leading Indicators"
  },
  {
    "objectID": "Module1/Indicators.html#basic-indicator-concepts",
    "href": "Module1/Indicators.html#basic-indicator-concepts",
    "title": "Indicators",
    "section": "Basic Indicator Concepts",
    "text": "Basic Indicator Concepts\n\nManagement\n\n\n“Management is the process by which an environment is designed and maintained in which individuals working in groups effectively accomplish specific goals.”\n\nKoontz, Harold, “Management,” 14th Edition, McGraw Hill, 2012\n\n\nFunctions of management\nThe management process is broken down into five management functions:\n\n\n\nPlan: Choosing missions and objectives, and the actions to achieve them.\nOrganizing: Establishing an intentional structure of roles for people to perform in an organization.\nStaffing: Filling and maintaining positions in the organizational structure.\nDirecting: Influencing people to contribute to organizational and group goals.\nControlling: Measuring and correcting individual and organizational performance to ensure that events conform to plans.\n\n\n\n\n\nIntegration of planning and control\n\n\n\n\n\n\n\n\nPlanning\nAs a result of the planning process, some of the following types of plans can be generated:\n\n\nObjectives or goals. These are specific and measurable results.\nStrategies. Approaches or plans to achieve objectives.\nProcedures. These detail the execution of an activity.\nPrograms. Organized sets of activities to achieve objectives.\nBudgets. Detailed financial plans to facilitate economic decision-making.\n\n\n\nHigh need to measure progress\n\n\n\nPlanning\nAs a result of the planning process, some of the following types of plans can be generated:\n\n\nObjectives or goals. These are specific and measurable results.\nStrategies. Approaches or plans to achieve objectives.\nProcedures. These detail the execution of an activity.\nPrograms. Organized sets of activities to achieve objectives.\nBudgets. Detailed financial plans to facilitate economic decision-making.\n\n\n\nHigh need to measure progress\n\n\n\nDefinition of SMART Objectives\n\n\n\nSpecific: An objective should be clear and specific, avoiding ambiguity. It should answer the questions what, who, when, where, and why.\nMeasurable: An objective should be quantifiable or at least evaluable to determine progress and success. It should be possible to measure it with tangible indicators or criteria.\nAchievable: A goal should be realistic and achievable, considering available resources, time, and necessary skills.\nRelevant: A goal should be relevant and aligned with the broader goals of the organization or individual.\nTime-bounded: A goal should have a clearly defined timeframe or deadline.\n\n\n\n\n\nExamples\nExample 1:\n\n\n\nObjective: Complete staff training.\nSMART: Complete at least 90% of the 2014 training program for all company operating personnel by November 30 of this year.\n\n\n\n. . .\nExample 2\n\n\n\nObjective: Increase sales by 20%.\nSMART: Achieve a sales increase of product X of at least 17% by the end of the first half of 2015, while maintaining a profitability for the company of at least 5%.\n\n\n\n\n\nThe need to measure\n\nTo carry out the Planning, Implementation, and Control processes, we need an information system to evaluate whether the planned objectives are being achieved and whether the implemented actions are being carried out in accordance with the plans.\n\n\n\n\n\n\n\n\nWhat is an indicator?\n\n\n“It is the result of a quantitative or qualitative measurement, or some other criterion, by which the performance, efficiency, achievement, etc., of a person or organization can be evaluated, often compared to a standard or goal.”\n\nCollins English Dictionary.\n\n\nWhat is an indicator?\n\n\n“The qualitative and/or quantitative information on an examined phenomenon (or a process, or a result), which makes it possible to analyze its evolution and to check whether quality targets are met, driving actions and decisions.”\n\nFranceschini, Fiorenzo & Galetto, Maurizio & Maisano, Domenico. (2007). Management by Measurement: Designing Key Indicators and Performance Measurement Systems. 10.1007/978-3-540-73212-9.\n\n\nExamples of indicators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristics of an indicator\n\nFundamental:\n\nValidity: The indicator must accurately reflect the actual behavior of the phenomenon, variable, result, etc., to be measured.\nStability: The indicator must be defined, calculated, and interpreted in the same way over time (allowing for comparisons and observing trends).\n\n\n\n\n\nIdeal:\n\nSimple and easy to interpret.\nAble to indicate trends over time.\nSensitive to changes within and outside the organization.\nEasy to collect and process data.\nQuick and easy to update.\n\n\n\nUsefulness of indicators\n\n\n\n\n\n\n\nDimensions of analysis\n\n\nIndicators and the data that drive them are often stratified with respect to other variables.\nVariables used as stratification criteria are called “Analysis Dimensions” (they are dimensions from the perspective of data).\n\n. . .\nExample: In a sales process, monthly sales can be stratified by distribution channel, region of the country, product family, etc., for analysis and visualization purposes.\n\n\nFor example"
  },
  {
    "objectID": "Module1/Indicators.html#lagging-and-leading-indicators",
    "href": "Module1/Indicators.html#lagging-and-leading-indicators",
    "title": "Indicators",
    "section": "Lagging and Leading Indicators",
    "text": "Lagging and Leading Indicators\n\nSelecting indicators\n\nSelecting indicators is a critical factor for an organization to move closer to fulfilling its mission and turning its strategies into reality. Indicators and strategies are inextricably linked.\n. . .\n\nA strategy without indicators is useless; indicators without a strategy are irrelevant!\n\n\n\nTwo main types of indicators\n\nA lagging indicator measures the outcome of performance at the end of a period. It is backward-looking because it shows us the consequences of what has already been done. They are also known as result indicators.\n. . .\n\nA leading indicator measures the performance of factors that are critical now to achieving a desired outcome in the future.\n\n\nLagging Indicator\n\nGoal: Measures the outcome of performance at the end of a period.\n\nExample: Annual sales, market share, ROI.\n\nAdvantage: They are objective.\nDisadvantage: They reflect the effect of past actions.\n\n\n\nLeading indicator\n\nGoal: Measures processes, activities, behaviors that tend to change before the system starts to follow a particular pattern or trend.\n\nExample: # of customers visited, # of courses offered\n\nAdvantage: They are predictive, allowing for strategy corrections\nDisadvantage: Based on cause-and-effect hypotheses.\n\n\n\nOther examples\nIn the context of a YouTuber:\n\nLeading indicator: Number of views of a YouTube video in the first 24 hours.\nLagging indicator: Monthly revenue generated from monetization.\n\n. . .\nIn the context of Netflix’s sales department:\n\nLeading indicator: Number of users who start a free trial in a given month.\nLagging indicator: Monthly revenue from subscriptions.\n\n\n\n\nIn the context of a GenAI company:\n\nLeading indicator: Number of programmers hired with expertise in AI.\nLagging indicator: Number of licenses sold per year.\n\n. . .\nIn the context of a mini-split maintenance company:\n\nLeading indicator: Average response time from technical support to support requests.\nLagging indicator: Number of positive reviews on Google Maps in a month.\n\n\n\nHow do I define an indicator?\n\nSelection criteria:\n\nDirect relationship with the objective to be measured\nEase of communication focused on the strategy\nRepeatability and reliability\nUpdate frequency\nUsefulness in goal setting\nUsefulness in assigning responsibilities\nUsefulness for downward deployment\n\n\n\nBasic and Derived Indicators\nA basic indicator is obtained from the direct measurement of a phenomenon or fact. For example: Number of orders delivered completely and on time during the week.\nA derived indicator combines the information from two or more basic or derived indicators. For example: Percentage of orders delivered completely and on time during the week.\n. . .\n\nExample: in Failure Modes and Effects Analysis\n\nBasic indicators: Severity, Occurrence, Detection\nDerived indicator: Risk Priority Number = S \\(\\times\\) O \\(\\times\\) D\n\n\n\nSeverity (S): Measures the impact or severity of the failure if it occurs. It is measured on a scale of 1 to 10, where 1 indicates a negligible effect and 10 represents a catastrophic effect for the user or the system.\nOccurrence (O): Evaluates the probability of the failure occurring. It is evaluated on a scale of 1 to 10, where 1 indicates that the occurrence is very rare and 10 indicates that it is highly probable or frequent.\nDetection (D): Represents the system’s ability to detect the failure before it reaches the customer or end user. It is rated on a scale of 1 to 10, where 1 means that detection is almost certain and 10 means that it is very difficult or impossible to detect before a problem occurs.\n\nDerived Indicator: Risk Priority Number = S\\(\\times\\)O\\(\\times\\)D\n\n\n\nThe format of an indicator\nAn indicator should be measured numerically using:\nAbsolute Numbers: Results of a measurement or counting process (volume produced, share price, number of employees, fixed costs, etc.)\nRates: Relationship between two variables with different units (number of units / number of workers, energy consumption / liters produced, etc.)\nIndexes: Dimensionless quantity resulting from dividing the current value of a variable by a reference base value for that variable (consumer price index)\n\n\n\n\nProportions: Relationships between two variables measured in the same units (men vs. women, admitted vs. rejected)\nGrowth or Decrease Percentages: (Current Value – Previous Value)*100/Previous Value.\nEvaluations: Evaluations of a qualitative variable on a Likert-type ordinal scale (low, medium, high, terrible, bad, average, good, excellent).\n\n\nActivity 1.1 (solo mode)\nFor the following concepts, propose one leading and one lagging (basic and derived) quantitative indicators:\n\n\nMonthly productivity of a furniture production line\nAnnual staff turnover in a manufacturing company\nCustomer service level of a company that manufactures plastic packaging and delivers regionally\nBusiness profitability for a medium-sized wholesale grocery company\nPerformance of the fundraising process of an association supporting homeless children\n\n\n\n\nRemember that…\n\n\n\nThe purposes of an indicator are:\n\nEstablish quantitative goals.\nOrganizational motivation, induction of desirable behaviors.\nStrategy evaluation and strategic learning."
  },
  {
    "objectID": "Module1/Indicators.html#return-to-main-page",
    "href": "Module1/Indicators.html#return-to-main-page",
    "title": "Indicators",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "index.slides.html#course-topics",
    "href": "index.slides.html#course-topics",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "Course topics",
    "text": "Course topics\nModule 1\n\nIndicators (slides)\nSelection of Indicators (slides)\n\nModule 2\n\nIntroduction to Data Science (slides)\nIntroduction to Python and pandas (slides) (colab)\nData wrangling and visualization (slides) (colab) (pandas cheat sheet)\nDecision trees for classification (slides) (colab)\nEnsemble methods for classification (slides) (colab)\nK-nearest neighbours (slides) (colab)\n\nModule 3\n\nIntroduction to predictive models (slides) (colab)\nIntroduction to time series (slides) (colab)\nAutocorrelation models (slides) (colab)\nEnsemble methods for regression (slides) (colab)\n\nModule 4\n\nClustering methods (slides) (colab)\nPrincipal component analysis (slides) (colab)"
  },
  {
    "objectID": "index.slides.html#about-the-author",
    "href": "index.slides.html#about-the-author",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "About the author",
    "text": "About the author\nAlan R. Vazquez is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\nLicense\nIN2004B Generation of Value with Data Analytics © 2025 by Alan R Vazquez is licensed under CC BY-NC-SA 4.0"
  }
]
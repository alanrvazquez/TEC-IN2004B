[
  {
    "objectID": "Module1/BSC.slides.html#agenda",
    "href": "Module1/BSC.slides.html#agenda",
    "title": "Selection of Indicators",
    "section": "Agenda",
    "text": "Agenda\n\n\nModels for Defining Indicators\nIndicator Documentation\nDashboards for Visualizing Indicators"
  },
  {
    "objectID": "Module1/BSC.slides.html#introduction",
    "href": "Module1/BSC.slides.html#introduction",
    "title": "Selection of Indicators",
    "section": "Introduction",
    "text": "Introduction\n\nOnce we know what indicators are, their role in the management process, and the types of indicators that exist, we ask ourselves:\n\n\nWhich indicators should we use?\nHow many indicators should we have?\nWhat is an appropriate set of indicators?\nHow do we document and share them?"
  },
  {
    "objectID": "Module1/BSC.slides.html#section",
    "href": "Module1/BSC.slides.html#section",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Although there are some indicators that could be generally applied to any company, each company has its own strategy, its own priorities, and its particular competitive environment\nTherefore, the most appropriate set of indicators depends on each organization.\n\n\nHere we describe models for finding the appropriate number and set of indicators."
  },
  {
    "objectID": "Module1/BSC.slides.html#models-for-defining-indicators-1",
    "href": "Module1/BSC.slides.html#models-for-defining-indicators-1",
    "title": "Selection of Indicators",
    "section": "Models for defining indicators",
    "text": "Models for defining indicators\n\nThe models for defining indicators depend on the type of strategic planning.\nThree strategic planning frameworks are:\n\nManagement by Objectives (MBO; Peter Drucker, 1954).\nHoshin Kanri.\nBalanced Score Card (BSC; Kaplan & Norton, 1992, 1996)."
  },
  {
    "objectID": "Module1/BSC.slides.html#models-for-defining-indicators-2",
    "href": "Module1/BSC.slides.html#models-for-defining-indicators-2",
    "title": "Selection of Indicators",
    "section": "Models for defining indicators",
    "text": "Models for defining indicators\n\nThe models for defining indicators depend on the type of strategic planning.\nThree strategic planning frameworks are:\n\nManagement by Objectives (MBO; Peter Drucker, 1954).\nHoshin Kanri.\nBalanced Scorecard (BSC; Kaplan & Norton, 1992, 1996)."
  },
  {
    "objectID": "Module1/BSC.slides.html#balanced-scorecard-bsc",
    "href": "Module1/BSC.slides.html#balanced-scorecard-bsc",
    "title": "Selection of Indicators",
    "section": "Balanced Scorecard (BSC)",
    "text": "Balanced Scorecard (BSC)\n\n\n\nIt’s a model that helps organizations translate strategy into operational (measurable) objectives, resulting in actions, behaviors, and performance.\n\nThe BSC includes all critical success factors in a measurement system, giving organizations a better chance of achieving their goals."
  },
  {
    "objectID": "Module1/BSC.slides.html#bscs-objectives",
    "href": "Module1/BSC.slides.html#bscs-objectives",
    "title": "Selection of Indicators",
    "section": "BSC’s objectives",
    "text": "BSC’s objectives\n\n\n\nTranslate the strategy into measurable objectives.\nAlign the strategy components: objectives, indicators, and initiatives.\nCommunicate the strategy to the organization.\nCreate the basis for strategic management."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-1",
    "href": "Module1/BSC.slides.html#section-1",
    "title": "Selection of Indicators",
    "section": "",
    "text": "BSC transforms strategy into an integrated system defined through four perspectives:\n\n\n\n\n\nFinancial\nClients\nInternal Processes\nLearning and Growth"
  },
  {
    "objectID": "Module1/BSC.slides.html#the-four-perspectives",
    "href": "Module1/BSC.slides.html#the-four-perspectives",
    "title": "Selection of Indicators",
    "section": "The four perspectives",
    "text": "The four perspectives\n\n\n\n\nFinancial. Includes objectives related to profitability, productivity, profits, stock price, etc.; these are the objectives that the organization must achieve from the shareholders’ perspective."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-2",
    "href": "Module1/BSC.slides.html#section-2",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Customers. Includes objectives related to the company’s value proposition, are market-oriented, and are established from the customer’s perspective. Includes customer perception objectives regarding service, delivery time, quality, and value/price."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-3",
    "href": "Module1/BSC.slides.html#section-3",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Internal Processes. Includes objectives related to the performance of processes that are critical to meeting customer perspective objectives. Performance objectives for the business’s primary value chain."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-4",
    "href": "Module1/BSC.slides.html#section-4",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Learning and growth. These are the objectives related to the enablers for achieving the objectives of the other perspectives. These are objectives related to competency development, the work environment, the physical environment, technological infrastructure, etc."
  },
  {
    "objectID": "Module1/BSC.slides.html#planning-your-birthday-party",
    "href": "Module1/BSC.slides.html#planning-your-birthday-party",
    "title": "Selection of Indicators",
    "section": "Planning your birthday party",
    "text": "Planning your birthday party"
  },
  {
    "objectID": "Module1/BSC.slides.html#model-components",
    "href": "Module1/BSC.slides.html#model-components",
    "title": "Selection of Indicators",
    "section": "Model components",
    "text": "Model components\n\nFor each perspective, the following should be defined:\n\nA small set of strategic objectives\nFor each objective, one (or more if necessary) metric as a performance indicator\nFor each indicator, establish long- and short-term goals\nInitiatives (programs, projects, actions) to close the gaps between current and desired performance according to the goals."
  },
  {
    "objectID": "Module1/BSC.slides.html#strategy-map",
    "href": "Module1/BSC.slides.html#strategy-map",
    "title": "Selection of Indicators",
    "section": "Strategy map",
    "text": "Strategy map\n\n\nThe strategy map displays the strategic objectives within each perspective using a matrix.\nPossible causal relationships between objectives are also shown using arrows.\nIf the organization’s strategic charter (Vision and Mission) is reviewed, Strategic Themes can be included to show whether the organization is explicitly addressing these themes in its planning."
  },
  {
    "objectID": "Module1/BSC.slides.html#strategy-map-structure",
    "href": "Module1/BSC.slides.html#strategy-map-structure",
    "title": "Selection of Indicators",
    "section": "Strategy map structure",
    "text": "Strategy map structure\n\n\n\nSeguimos la filosofia de BSC y lo definimos de arriba abajo. Pero se ejecuta de abajo hacia arriba. Los niveles superiores son consecuencia de los niveles inferiores. No necesariamente todos están ligados."
  },
  {
    "objectID": "Module1/BSC.slides.html#examples",
    "href": "Module1/BSC.slides.html#examples",
    "title": "Selection of Indicators",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "Module1/BSC.slides.html#more-on-strategy-maps",
    "href": "Module1/BSC.slides.html#more-on-strategy-maps",
    "title": "Selection of Indicators",
    "section": "More on strategy maps",
    "text": "More on strategy maps"
  },
  {
    "objectID": "Module1/BSC.slides.html#activity-1.2-cooperative-mode",
    "href": "Module1/BSC.slides.html#activity-1.2-cooperative-mode",
    "title": "Selection of Indicators",
    "section": "Activity 1.2 (cooperative mode)",
    "text": "Activity 1.2 (cooperative mode)\n\n\nForm teams of three.\nBuild a strategy map as part of the Balanced Scorecard model application.\nRead the introduction to the Muebles Finos MF mini-case study in CANVAS.\nThe activity consists of mapping the company’s objectives to the perspectives of the BSC model and to the business’s strategic lines."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-8",
    "href": "Module1/BSC.slides.html#section-8",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Once you have placed the objectives on the map, link them to each other, establishing cause-and-effect relationships between them.\nYou are also asked to select three of the objectives and propose at least one suggested metric for each.\nFinally, you will write the justification for the established causal relationships."
  },
  {
    "objectID": "Module1/BSC.slides.html#final-comments",
    "href": "Module1/BSC.slides.html#final-comments",
    "title": "Selection of Indicators",
    "section": "Final comments",
    "text": "Final comments\n\nOnce the BSC model has been designed for the company, specific models are developed by functional area.\nIn the functional areas, work is based on the strategic objectives, identifying objectives specific to the functional area.\nIt is more common to use performance indicators than impact indicators at the departmental level.\nNot all strategic themes are necessarily maintained across functional areas, nor do all areas necessarily have objectives from all BSC perspectives."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-9",
    "href": "Module1/BSC.slides.html#section-9",
    "title": "Selection of Indicators",
    "section": "",
    "text": "It is essential that functional managers fully understand the company’s BSC and their area’s contribution to the overall objectives.\nThe person responsible for BSC deployment must ensure the consistency and alignment of the functional BSCs.\nSome organizational-level indicators are simply aggregations of departmental indicators."
  },
  {
    "objectID": "Module1/BSC.slides.html#indicator-documentation-1",
    "href": "Module1/BSC.slides.html#indicator-documentation-1",
    "title": "Selection of Indicators",
    "section": "Indicator documentation",
    "text": "Indicator documentation\n\nWhen selecting and/or designing indicators, it is necessary to formally document the definition of each indicator.\nDocumentation is very useful because:\n\nIt helps clarify the meaning of the indicators.\nIt facilitates communication between users and indicator creators.\nIt serves as a future reference when revising the system."
  },
  {
    "objectID": "Module1/BSC.slides.html#basic-documenting-format",
    "href": "Module1/BSC.slides.html#basic-documenting-format",
    "title": "Selection of Indicators",
    "section": "Basic documenting format",
    "text": "Basic documenting format"
  },
  {
    "objectID": "Module1/BSC.slides.html#indicator-visualization",
    "href": "Module1/BSC.slides.html#indicator-visualization",
    "title": "Selection of Indicators",
    "section": "Indicator visualization",
    "text": "Indicator visualization\nOnce the indicators are defined using the Balanced Scorecard (BSC) and the documentation format, we design dashboards with:\n\n\nScreens.\nChart and table styles.\nAggregation levels.\nDefault reports.\nDrill-down requirements.\n\n\nThis applies to both the overall organizational level and the functional BSCs."
  },
  {
    "objectID": "Module1/BSC.slides.html#dashboard-deployment",
    "href": "Module1/BSC.slides.html#dashboard-deployment",
    "title": "Selection of Indicators",
    "section": "Dashboard deployment",
    "text": "Dashboard deployment\nVarious strategies can be followed to implement the dashboards, for example:\n\nDevelopment on a specialized platform for indicator systems (https://www.predictiveanalyticstoday.com/open-source-balanced-scorecard-software/)\nDevelopment with generic OLAP Online Analytical Processing tools (https://www.softwaretestinghelp.com/best-olap-tools/)\nShort-term, isolated implementations in spreadsheets."
  },
  {
    "objectID": "Module1/BSC.slides.html#tableu-dashboard-example",
    "href": "Module1/BSC.slides.html#tableu-dashboard-example",
    "title": "Selection of Indicators",
    "section": "Tableu dashboard example",
    "text": "Tableu dashboard example\n\nhttps://www.tableau.com/es-mx"
  },
  {
    "objectID": "Module1/BSC.slides.html#another-example",
    "href": "Module1/BSC.slides.html#another-example",
    "title": "Selection of Indicators",
    "section": "Another example",
    "text": "Another example"
  },
  {
    "objectID": "Module1/BSC.slides.html#google-looker-studio",
    "href": "Module1/BSC.slides.html#google-looker-studio",
    "title": "Selection of Indicators",
    "section": "Google Looker Studio",
    "text": "Google Looker Studio\nFree online tool by Google for building interactive dashboards and reports. Works directly in the web browser and it connects to many data sources:\n\nGoogle Sheets\nCSV files\nBigQuery\nOther cloud and database services"
  },
  {
    "objectID": "Module1/BSC.slides.html#getting-started",
    "href": "Module1/BSC.slides.html#getting-started",
    "title": "Selection of Indicators",
    "section": "Getting started",
    "text": "Getting started\n\nGo to https://lookerstudio.google.com/\nClick Blank Report"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-10",
    "href": "Module1/BSC.slides.html#section-10",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Select Microsoft Excel as your data source"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-11",
    "href": "Module1/BSC.slides.html#section-11",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Add the dataset."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-12",
    "href": "Module1/BSC.slides.html#section-12",
    "title": "Selection of Indicators",
    "section": "",
    "text": "The variables in the dataset."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-13",
    "href": "Module1/BSC.slides.html#section-13",
    "title": "Selection of Indicators",
    "section": "",
    "text": "The graphics available."
  },
  {
    "objectID": "Module1/BSC.slides.html#example",
    "href": "Module1/BSC.slides.html#example",
    "title": "Selection of Indicators",
    "section": "Example",
    "text": "Example\n\nThe dataset “McDonalds Financial Statements.xlsx” includes McDonald’s annual financial data over two decades, offering insights into its economic performance. The dataset contains records of the following indicators:\n\n\n\nMarket cap ($B)\nRevenue ($B)\nEarnings ($B)\nP/E ratio\nP/S ratio\nP/B ratio\nOperating Margin\nEPS ($)\nShares Outstanding"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-14",
    "href": "Module1/BSC.slides.html#section-14",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Let’s load the data into looker studio\n\n\n\n\n\nWe rename the report."
  },
  {
    "objectID": "Module1/BSC.slides.html#section-15",
    "href": "Module1/BSC.slides.html#section-15",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Let’s insert our first table\n\n\n\n\n\nWe can place the table anywhere in the dashboard canvas."
  },
  {
    "objectID": "Module1/BSC.slides.html#the-default-table",
    "href": "Module1/BSC.slides.html#the-default-table",
    "title": "Selection of Indicators",
    "section": "The default table",
    "text": "The default table"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-16",
    "href": "Module1/BSC.slides.html#section-16",
    "title": "Selection of Indicators",
    "section": "",
    "text": "We change the graphic type by going to its properties."
  },
  {
    "objectID": "Module1/BSC.slides.html#select-another-variable",
    "href": "Module1/BSC.slides.html#select-another-variable",
    "title": "Selection of Indicators",
    "section": "Select another variable",
    "text": "Select another variable"
  },
  {
    "objectID": "Module1/BSC.slides.html#we-can-sort-the-table-according-to-date",
    "href": "Module1/BSC.slides.html#we-can-sort-the-table-according-to-date",
    "title": "Selection of Indicators",
    "section": "We can sort the table according to date",
    "text": "We can sort the table according to date"
  },
  {
    "objectID": "Module1/BSC.slides.html#improving-the-table",
    "href": "Module1/BSC.slides.html#improving-the-table",
    "title": "Selection of Indicators",
    "section": "Improving the table",
    "text": "Improving the table\nWe can change the layout of the table by changing the metrics in the style of the table"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-17",
    "href": "Module1/BSC.slides.html#section-17",
    "title": "Selection of Indicators",
    "section": "",
    "text": "Change it to bars.\n\n\n\n\n\n\nShow the number too."
  },
  {
    "objectID": "Module1/BSC.slides.html#the-result",
    "href": "Module1/BSC.slides.html#the-result",
    "title": "Selection of Indicators",
    "section": "The result",
    "text": "The result"
  },
  {
    "objectID": "Module1/BSC.slides.html#lets-create-a-bar-chart",
    "href": "Module1/BSC.slides.html#lets-create-a-bar-chart",
    "title": "Selection of Indicators",
    "section": "Let’s create a bar chart",
    "text": "Let’s create a bar chart"
  },
  {
    "objectID": "Module1/BSC.slides.html#lets-change-the-variable-or-metric",
    "href": "Module1/BSC.slides.html#lets-change-the-variable-or-metric",
    "title": "Selection of Indicators",
    "section": "Let’s change the variable or metric",
    "text": "Let’s change the variable or metric"
  },
  {
    "objectID": "Module1/BSC.slides.html#we-can-also-add-a-box",
    "href": "Module1/BSC.slides.html#we-can-also-add-a-box",
    "title": "Selection of Indicators",
    "section": "We can also add a box",
    "text": "We can also add a box"
  },
  {
    "objectID": "Module1/BSC.slides.html#section-20",
    "href": "Module1/BSC.slides.html#section-20",
    "title": "Selection of Indicators",
    "section": "",
    "text": "We can edit the properties or layout of the number."
  },
  {
    "objectID": "Module1/BSC.slides.html#select-the-mean",
    "href": "Module1/BSC.slides.html#select-the-mean",
    "title": "Selection of Indicators",
    "section": "Select the mean",
    "text": "Select the mean"
  },
  {
    "objectID": "Module1/BSC.slides.html#we-can-increase-the-font-size",
    "href": "Module1/BSC.slides.html#we-can-increase-the-font-size",
    "title": "Selection of Indicators",
    "section": "We can increase the font size",
    "text": "We can increase the font size"
  },
  {
    "objectID": "Module1/BSC.slides.html#add-a-text-or-a-header-to-the-report",
    "href": "Module1/BSC.slides.html#add-a-text-or-a-header-to-the-report",
    "title": "Selection of Indicators",
    "section": "Add a text or a header to the report",
    "text": "Add a text or a header to the report"
  },
  {
    "objectID": "Module1/BSC.slides.html#change-the-style",
    "href": "Module1/BSC.slides.html#change-the-style",
    "title": "Selection of Indicators",
    "section": "Change the style",
    "text": "Change the style"
  },
  {
    "objectID": "Module1/BSC.slides.html#the-report",
    "href": "Module1/BSC.slides.html#the-report",
    "title": "Selection of Indicators",
    "section": "The report",
    "text": "The report"
  },
  {
    "objectID": "Module1/BSC.slides.html#tutorials-on-looker-studio",
    "href": "Module1/BSC.slides.html#tutorials-on-looker-studio",
    "title": "Selection of Indicators",
    "section": "Tutorials on looker studio",
    "text": "Tutorials on looker studio"
  },
  {
    "objectID": "Module1/BSC.slides.html#tips-for-effective-dashboards",
    "href": "Module1/BSC.slides.html#tips-for-effective-dashboards",
    "title": "Selection of Indicators",
    "section": "Tips for effective dashboards",
    "text": "Tips for effective dashboards\n\n\nKeep it simple — focus on the key indicators.\nUse color to draw attention, not decorate.\nProvide context: compare current value vs. target.\nTest your dashboard on different screen sizes."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#load-the-libraries",
    "href": "Module2/KNearestNeighbours.slides.html#load-the-libraries",
    "title": "K nearest neighbors",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "href": "Module2/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "title": "K nearest neighbors",
    "section": "K-nearest neighbors (KNN)",
    "text": "K-nearest neighbors (KNN)\n\nKNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n\nBasic idea: Predict a new observation using the K closest observations in the training dataset.\n\nTo predict the response for a new observation, KNN uses the K nearest neighbors (observations) in terms of the predictors!\nThe predicted response for the new observation is the most common response among the K nearest neighbors."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "href": "Module2/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "title": "K nearest neighbors",
    "section": "The algorithm has 3 steps:",
    "text": "The algorithm has 3 steps:\n\n\n\nChoose the number of nearest neighbors (K).\nFor a new observation, find the K closest observations in the training data (ignoring the response).\nFor the new observation, the algorithm predicts the value of the most common response among the K nearest observations."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-1",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-1",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-2",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-2",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-3",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-3",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”\nCloseness is based on Euclidean distance."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#implementation-details",
    "href": "Module2/KNearestNeighbours.slides.html#implementation-details",
    "title": "K nearest neighbors",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nTies\n\nIf there are more than K nearest neighbors, include them all.\nIf there is a tie in the vote, set a rule to break the tie. For example, randomly select the class."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section",
    "href": "Module2/KNearestNeighbours.slides.html#section",
    "title": "K nearest neighbors",
    "section": "",
    "text": "KNN uses the Euclidean distance between points. So it ignores units.\n\nExample: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\nThese people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\nTherefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n\n\nTherefore, as a first step, we must transform the predictors so that they have the same units!"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#standardization",
    "href": "Module2/KNearestNeighbours.slides.html#standardization",
    "title": "K nearest neighbors",
    "section": "Standardization",
    "text": "Standardization\n\nStandardization refers to centering and scaling each numerical predictor individually. This places all predictors on the same scale.\nIn mathematical terms, we standardize a predictor \\(\\mathbf{X}\\) as:\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}},\\]\nwith \\(\\bar{X} = \\sum_{i=1}^n \\frac{x_i}{n}\\)."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#example",
    "href": "Module2/KNearestNeighbours.slides.html#example",
    "title": "K nearest neighbors",
    "section": "Example",
    "text": "Example\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "href": "Module2/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "title": "K nearest neighbors",
    "section": "Create the predictor matrix and response column",
    "text": "Create the predictor matrix and response column\nLet’s create the predictor matrix or response column\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \n\n# Vector with responses\nY_full = bank_data.filter(['Status'])\n\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "href": "Module2/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "title": "K nearest neighbors",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\nWe use 70% for training and the rest for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#standardization-in-python",
    "href": "Module2/KNearestNeighbours.slides.html#standardization-in-python",
    "title": "K nearest neighbors",
    "section": "Standardization in Python",
    "text": "Standardization in Python\n\nTo standardize numeric predictors, we use the StandardScaler() function. We also apply the function to variables using the fit_transform() function.\n\n\nscaler = StandardScaler()\nXs_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#knn-in-python",
    "href": "Module2/KNearestNeighbours.slides.html#knn-in-python",
    "title": "K nearest neighbors",
    "section": "KNN in Python",
    "text": "KNN in Python\n\nIn Python, we can use the KNeighborsClassifier() and fit() from scikit-learn to train a KNN.\nIn the KNeighborsClassifier function, we can define the number of nearest neighbors using the n_neighbors parameter.\n\n# For example, let's use KNN with three neighbours\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Now, we train the algorithm.\nknn.fit(Xs_train, Y_train)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#evaluation",
    "href": "Module2/KNearestNeighbours.slides.html#evaluation",
    "title": "K nearest neighbors",
    "section": "Evaluation",
    "text": "Evaluation\n\nTo evaluate KNN, we make predictions on the validation data (not used to train the KNN). To do this, we must first perform standardization operations on the predictors in the validation dataset.\n\nXs_valid = scaler.fit_transform(X_valid)\n\n\nNext, we make predictions.\n\nY_pred_knn = knn.predict(Xs_valid)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#confusion-matrix",
    "href": "Module2/KNearestNeighbours.slides.html#confusion-matrix",
    "title": "K nearest neighbors",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Calcular matriz de confusión.\ncm = confusion_matrix(Y_valid, Y_pred_knn)\n\n# Mostrar matriz de confusión.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "href": "Module2/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "title": "K nearest neighbors",
    "section": "Finding the best value of K",
    "text": "Finding the best value of K\nWe can determine the best value of K for the KNN algorithm. To this end, we evaluate the performance of the KNN for different values of \\(K\\) in terms of accuracy on the validation dataset.\n\nbest_k = 1\nbest_accuracy = 0\nk_values = range(1, 50)  # Test k values from 1 to 50\nvalidation_accuracies = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(Xs_train, Y_train)\n    val_accuracy = accuracy_score(Y_valid, model.predict(Xs_valid))\n    validation_accuracies.append(val_accuracy)\n\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_k = k"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#visualize",
    "href": "Module2/KNearestNeighbours.slides.html#visualize",
    "title": "K nearest neighbors",
    "section": "Visualize",
    "text": "Visualize\nWe can then visualize the accuracy for different values of \\(K\\) using the following graph and code.\n\n\nCode\nplt.figure(figsize=(6.3, 4.3))\nplt.plot(k_values, validation_accuracies, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Validation Accuracy\")\nplt.title(\"Choosing the Best k for KNN\")\nplt.show()"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section-1",
    "href": "Module2/KNearestNeighbours.slides.html#section-1",
    "title": "K nearest neighbors",
    "section": "",
    "text": "Finally, we select the best number of nearest neighbors contained in the best_k object.\n\nKNN_final = KNeighborsClassifier(n_neighbors = best_k)\nKNN_final.fit(Xs_train, Y_train)\n\n\nThe accuracy of the best KNN is\n\nY_pred_KNNfinal = KNN_final.predict(Xs_valid)\nvalid_accuracy = accuracy_score(Y_valid, Y_pred_KNNfinal)\nprint(valid_accuracy)\n\n0.9833333333333333"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#discussion",
    "href": "Module2/KNearestNeighbours.slides.html#discussion",
    "title": "K nearest neighbors",
    "section": "Discussion",
    "text": "Discussion\n\nKNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:\n\nWhen the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (“Lazy learner”).\nIn this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section-2",
    "href": "Module2/KNearestNeighbours.slides.html#section-2",
    "title": "K nearest neighbors",
    "section": "",
    "text": "The predictive performance of KNN deteriorates as the number of predictors increases.\nThis is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\nThis is known as the curse of dimensionality.\n\n\n\n\nhttps://aiaspirant.com/curse-of-dimensionality/"
  },
  {
    "objectID": "Module2/Classification.slides.html#agenda",
    "href": "Module2/Classification.slides.html#agenda",
    "title": "Classification Trees",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nTraining, Validation, and Test Data\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Module2/Classification.slides.html#load-the-libraries",
    "href": "Module2/Classification.slides.html#load-the-libraries",
    "title": "Classification Trees",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nBefore we start, let’s import the data science libraries into Python.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems",
    "href": "Module2/Classification.slides.html#main-data-science-problems",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems-1",
    "href": "Module2/Classification.slides.html#main-data-science-problems-1",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#terminology",
    "href": "Module2/Classification.slides.html#terminology",
    "title": "Classification Trees",
    "section": "Terminology",
    "text": "Terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors."
  },
  {
    "objectID": "Module2/Classification.slides.html#section",
    "href": "Module2/Classification.slides.html#section",
    "title": "Classification Trees",
    "section": "",
    "text": "Response:\n\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-algorithms",
    "href": "Module2/Classification.slides.html#classification-algorithms",
    "title": "Classification Trees",
    "section": "Classification algorithms",
    "text": "Classification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n\nTechnically, they predict the probability that the record belongs to the target class."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-1",
    "href": "Module2/Classification.slides.html#section-1",
    "title": "Classification Trees",
    "section": "",
    "text": "Goal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n\n\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-1",
    "href": "Module2/Classification.slides.html#example-1",
    "title": "Classification Trees",
    "section": "Example 1",
    "text": "Example 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n\n\n\n\n\n\n\n\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-2",
    "href": "Module2/Classification.slides.html#section-2",
    "title": "Classification Trees",
    "section": "",
    "text": "Technically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]"
  },
  {
    "objectID": "Module2/Classification.slides.html#bayes-classifier",
    "href": "Module2/Classification.slides.html#bayes-classifier",
    "title": "Classification Trees",
    "section": "Bayes classifier",
    "text": "Bayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-3",
    "href": "Module2/Classification.slides.html#section-3",
    "title": "Classification Trees",
    "section": "",
    "text": "HOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n\nTo overcome this issue, we several standard solutions:\n\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits)."
  },
  {
    "objectID": "Module2/Classification.slides.html#two-datasets",
    "href": "Module2/Classification.slides.html#two-datasets",
    "title": "Classification Trees",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science algorithms needs two data sets:\n\n\nTraining data is data that we use to train or construct an approximation \\(\\hat{C}(\\boldsymbol{X})\\) to the true Bayes Classifier \\(C(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the classification performance of \\(\\hat{C}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-4",
    "href": "Module2/Classification.slides.html#section-4",
    "title": "Classification Trees",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{C}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{C}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#validation-dataset",
    "href": "Module2/Classification.slides.html#validation-dataset",
    "title": "Classification Trees",
    "section": "Validation dataset",
    "text": "Validation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "href": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "title": "Classification Trees",
    "section": "Example 2: Identifying Counterfeit Banknotes",
    "text": "Example 2: Identifying Counterfeit Banknotes"
  },
  {
    "objectID": "Module2/Classification.slides.html#dataset",
    "href": "Module2/Classification.slides.html#dataset",
    "title": "Classification Trees",
    "section": "Dataset",
    "text": "Dataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "href": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "title": "Classification Trees",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "href": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "title": "Classification Trees",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .filter() from pandas to select two predictors: Bottom and Top.\n\n# Set full matrix of predictors.\nX_full = bank_data.filter(['Bottom', 'Top'])\nX_full.head(4)\n\n\n\n\n\n\n\n\nBottom\nTop\n\n\n\n\n0\n9.0\n9.7\n\n\n1\n8.1\n9.5\n\n\n2\n8.7\n9.6\n\n\n3\n7.5\n10.4"
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-response-column",
    "href": "Module2/Classification.slides.html#create-the-response-column",
    "title": "Classification Trees",
    "section": "Create the response column",
    "text": "Create the response column\nWe use the function .filter() from pandas to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data.filter(['Status'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\n0\ngenuine\n\n\n1\ngenuine\n\n\n2\ngenuine\n\n\n3\ngenuine"
  },
  {
    "objectID": "Module2/Classification.slides.html#set-the-target-category",
    "href": "Module2/Classification.slides.html#set-the-target-category",
    "title": "Classification Trees",
    "section": "Set the target category",
    "text": "Set the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: Status_counterfeit, dtype: int64"
  },
  {
    "objectID": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "href": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "title": "Classification Trees",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-5",
    "href": "Module2/Classification.slides.html#section-5",
    "title": "Classification Trees",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n185\n11.5\n10.7\n1\n\n\n69\n8.0\n11.2\n0\n\n\n109\n10.4\n11.2\n1\n\n\n75\n7.6\n10.7\n0"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-6",
    "href": "Module2/Classification.slides.html#section-6",
    "title": "Classification Trees",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n141\n10.2\n11.0\n1\n\n\n120\n10.1\n12.1\n1\n\n\n169\n11.4\n10.9\n1\n\n\n48\n7.9\n10.3\n0\n\n\n96\n9.1\n10.2\n0"
  },
  {
    "objectID": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "href": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "title": "Classification Trees",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\n\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nAlgorithm development.\n\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your algorithm!"
  },
  {
    "objectID": "Module2/Classification.slides.html#decision-tree",
    "href": "Module2/Classification.slides.html#decision-tree",
    "title": "Classification Trees",
    "section": "Decision tree",
    "text": "Decision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique."
  },
  {
    "objectID": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "href": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "title": "Classification Trees",
    "section": "Basic idea of a decision tree",
    "text": "Basic idea of a decision tree\nStratify or segment the prediction space into several simpler regions."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "href": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "title": "Classification Trees",
    "section": "How do you build a decision tree?",
    "text": "How do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "href": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "title": "Classification Trees",
    "section": "How do we grow a tree?",
    "text": "How do we grow a tree?\n\n\nUsing the CART algorithm!\n\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-25",
    "href": "Module2/Classification.slides.html#section-25",
    "title": "Classification Trees",
    "section": "",
    "text": "We repeat the partitioning process until the terminal nodes have no less than, say, 5 observations."
  },
  {
    "objectID": "Module2/Classification.slides.html#what-is-impurity",
    "href": "Module2/Classification.slides.html#what-is-impurity",
    "title": "Classification Trees",
    "section": "What is impurity?",
    "text": "What is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "href": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "title": "Classification Trees",
    "section": "How do we measure impurity?",
    "text": "How do we measure impurity?\n\n\n\nThere are three different metrics for impurity:\n\nRisk of misclassification.\nCross entropy.\nGini impurity index.\n\n\n \n\nProportion of elements in a class"
  },
  {
    "objectID": "Module2/Classification.slides.html#pruning-the-tree",
    "href": "Module2/Classification.slides.html#pruning-the-tree",
    "title": "Classification Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\nTo avoid overfitting, we pruned some of the tree’s branches. More specifically, we collapsed two internal (non-terminal) nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-28",
    "href": "Module2/Classification.slides.html#section-28",
    "title": "Classification Trees",
    "section": "",
    "text": "To prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThe algorithm has a tuning parameter called \\(\\alpha\\), which places greater weight on the number of tree nodes (or size):\n\\[\\text{Overall impurity of terminal nodes} + \\alpha (\\text{\\# terminal nodes})\\]\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python",
    "href": "Module2/Classification.slides.html#in-python",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\nIn Python, we can use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a decision tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(min_samples_leaf= 5, ccp_alpha=0, \n                              random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameter min_samples_leaf controls the minimum number of observations in a terminal node, and the cc_alpha controls the tree complexity (to be described later). The parameter random_state allows you to reproduce the same tree in different runs of the Python code."
  },
  {
    "objectID": "Module2/Classification.slides.html#plotting-the-tree",
    "href": "Module2/Classification.slides.html#plotting-the-tree",
    "title": "Classification Trees",
    "section": "Plotting the tree",
    "text": "Plotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#implementation-details",
    "href": "Module2/Classification.slides.html#implementation-details",
    "title": "Classification Trees",
    "section": "Implementation details",
    "text": "Implementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability."
  },
  {
    "objectID": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "href": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "title": "Classification Trees",
    "section": "Apply penalty for large trees",
    "text": "Apply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of the \\(\\alpha\\) parameter, which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-30",
    "href": "Module2/Classification.slides.html#section-30",
    "title": "Classification Trees",
    "section": "",
    "text": "The ccp_alphas and impurities objects contain the different values of the \\(\\alpha\\) parameter used, as well as the impurity performance of the generated trees.\nTo train a decision tree using different alpha values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf_alpha = DecisionTreeClassifier(random_state=507134, min_samples_leaf= 5, \n                                       ccp_alpha=ccp_alpha)\n    clf_alpha.fit(X_train, Y_train)\n    clfs.append(clf)\n\nIn the next section, we will evaluate the performance of these decision trees."
  },
  {
    "objectID": "Module2/Classification.slides.html#evaluation",
    "href": "Module2/Classification.slides.html#evaluation",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-31",
    "href": "Module2/Classification.slides.html#section-31",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-32",
    "href": "Module2/Classification.slides.html#section-32",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [0.6, 0.4],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0.2, 0.8],\n       [0.6, 0.4],\n       [1. , 0. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0.4, 0.6],\n       [0. , 1. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0.6, 0.4],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0.6, 0.4],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ],\n       [0.4, 0.6],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [0. , 1. ],\n       [1. , 0. ]])"
  },
  {
    "objectID": "Module2/Classification.slides.html#confusion-matrix",
    "href": "Module2/Classification.slides.html#confusion-matrix",
    "title": "Classification Trees",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems."
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python-1",
    "href": "Module2/Classification.slides.html#in-python-1",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[26  1]\n [ 1 32]]"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-33",
    "href": "Module2/Classification.slides.html#section-33",
    "title": "Classification Trees",
    "section": "",
    "text": "We can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/Classification.slides.html#accuracy",
    "href": "Module2/Classification.slides.html#accuracy",
    "title": "Classification Trees",
    "section": "Accuracy",
    "text": "Accuracy\nA simple metric for summarizing the information in the confusion matrix is accuracy. It is the proportion of correct classifications for both classes, out of the total classifications performed.\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.97\n\n\nThe higher the accuracy, the better the performance of the classifier."
  },
  {
    "objectID": "Module2/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "href": "Module2/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "title": "Classification Trees",
    "section": "Let’s get back to the penalized trees",
    "text": "Let’s get back to the penalized trees\n\nNow, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-34",
    "href": "Module2/Classification.slides.html#section-34",
    "title": "Classification Trees",
    "section": "",
    "text": "We visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#choosing-the-best-tree",
    "href": "Module2/Classification.slides.html#choosing-the-best-tree",
    "title": "Classification Trees",
    "section": "Choosing the best tree",
    "text": "Choosing the best tree\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.01.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0, min_samples_leaf= 5)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-35",
    "href": "Module2/Classification.slides.html#section-35",
    "title": "Classification Trees",
    "section": "",
    "text": "Once this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#accuracy-in-python",
    "href": "Module2/Classification.slides.html#accuracy-in-python",
    "title": "Classification Trees",
    "section": "Accuracy in Python",
    "text": "Accuracy in Python\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.97"
  },
  {
    "objectID": "Module2/Classification.slides.html#comments",
    "href": "Module2/Classification.slides.html#comments",
    "title": "Classification Trees",
    "section": "Comments",
    "text": "Comments\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading."
  },
  {
    "objectID": "Module2/Classification.slides.html#an-example",
    "href": "Module2/Classification.slides.html#an-example",
    "title": "Classification Trees",
    "section": "An example",
    "text": "An example\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-36",
    "href": "Module2/Classification.slides.html#section-36",
    "title": "Classification Trees",
    "section": "",
    "text": "In other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway."
  },
  {
    "objectID": "Module2/Classification.slides.html#another-example",
    "href": "Module2/Classification.slides.html#another-example",
    "title": "Classification Trees",
    "section": "Another example",
    "text": "Another example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-specific-metrics",
    "href": "Module2/Classification.slides.html#classification-specific-metrics",
    "title": "Classification Trees",
    "section": "Classification-specific metrics",
    "text": "Classification-specific metrics\n\nTo overcome this limitation of accuracy, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-37",
    "href": "Module2/Classification.slides.html#section-37",
    "title": "Classification Trees",
    "section": "",
    "text": "Sensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-38",
    "href": "Module2/Classification.slides.html#section-38",
    "title": "Classification Trees",
    "section": "",
    "text": "Precision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-39",
    "href": "Module2/Classification.slides.html#section-39",
    "title": "Classification Trees",
    "section": "",
    "text": "In Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n0.97\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.97"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-40",
    "href": "Module2/Classification.slides.html#section-40",
    "title": "Classification Trees",
    "section": "",
    "text": "Type I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-41",
    "href": "Module2/Classification.slides.html#section-41",
    "title": "Classification Trees",
    "section": "",
    "text": "Unfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n0.04"
  },
  {
    "objectID": "Module2/Classification.slides.html#discussion",
    "href": "Module2/Classification.slides.html#discussion",
    "title": "Classification Trees",
    "section": "Discussion",
    "text": "Discussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error."
  },
  {
    "objectID": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "href": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "title": "Classification Trees",
    "section": "Disadvantages of decision trees",
    "text": "Disadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#agenda",
    "href": "Module3/Autocorrelation.slides.html#agenda",
    "title": "Autocorrelation Models",
    "section": "Agenda",
    "text": "Agenda\n\n\nAutocorrelation\nThe ARIMA model\nThe SARIMA model"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#load-the-libraries",
    "href": "Module3/Autocorrelation.slides.html#load-the-libraries",
    "title": "Autocorrelation Models",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.tools import diff\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn and statsmodels libraries in Python."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#problem-with-linear-regression-models",
    "href": "Module3/Autocorrelation.slides.html#problem-with-linear-regression-models",
    "title": "Autocorrelation Models",
    "section": "Problem with linear regression models",
    "text": "Problem with linear regression models\n\nLinear regression models do not incorporate the dependence between consecutive values in a time series.\nThis is unfortunate because responses recorded over close time periods tend to be correlated. This correlation is called the autocorrelation of the time series.\nAutocorrelation helps us develop a model that can make better predictions of future responses."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#what-is-correlation",
    "href": "Module3/Autocorrelation.slides.html#what-is-correlation",
    "title": "Autocorrelation Models",
    "section": "What is correlation?",
    "text": "What is correlation?\n\n\n\n\n\nIt is a measure of the strength and direction of the linear relationship between two numerical variables.\nSpecifically, it is used to assess the relationship between two sets of observations.\nCorrelation is between \\(-1\\) and 1."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-measure-autocorrelation",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-measure-autocorrelation",
    "title": "Autocorrelation Models",
    "section": "How do we measure autocorrelation?",
    "text": "How do we measure autocorrelation?\n\nThere are two formal tools for measuring the correlation between observations in a time series:\n\n\nThe autocorrelation function.\nThe partial autocorrelation function."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-1",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-1",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-2",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-2",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before. This would be the correlation between these two columns"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-1",
    "href": "Module3/Autocorrelation.slides.html#example-1",
    "title": "Autocorrelation Models",
    "section": "Example 1",
    "text": "Example 1\nLet’s consider again the dataset in the file “Amtrak.xlsx.” The file contains records of Amtrak passenger numbers from January 1991 to March 2004.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\n\nThe autocorrelation function measures the correlation between responses that are separated by a specific number of periods.\nThe autocorrelation function is commonly visualized using a bar chart.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the correlations between observations at different lags."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-plot",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-plot",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation plot",
    "text": "Autocorrelation plot\n\nIn Python, we use the plot_acf function from the statsmodels library.\n\nplt.figure(figsize=(10, 6))\nplot_acf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()\n\nThe lags parameter controls the number of periods for which to compute the autocorrelation function."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-resulting-plot",
    "href": "Module3/Autocorrelation.slides.html#the-resulting-plot",
    "title": "Autocorrelation Models",
    "section": "The resulting plot",
    "text": "The resulting plot\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-1",
    "href": "Module3/Autocorrelation.slides.html#section-1",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The autocorrelation plot shows that the responses and those from zero periods ago have a correlation of 1.\nThe autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45.\nThe autocorrelation plot shows that the responses and those from 24 periods ago have a correlation of around 0.5.\n\n\n\n\n\n\n&lt;Figure size 384x384 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-patterns",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-patterns",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation patterns",
    "text": "Autocorrelation patterns\n\n\nA strong autocorrelation (positive or negative) with a lag \\(j\\) greater than 1 and its multiples (\\(2k, 3k, \\ldots\\)) typically reflects a cyclical pattern or seasonality.\nPositive lag-1 autocorrelation describes a series in which consecutive values generally move in the same direction.\nNegative lag-1 autocorrelation reflects oscillations in the series, where high values (generally) are immediately followed by low values and vice versa."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#more-about-the-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#more-about-the-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "More about the autocorrelation function",
    "text": "More about the autocorrelation function\nConsider the problem of predicting the average price of a kilo of avocado this month.\nFor this, we have the average price from last month and the month before that."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-2",
    "href": "Module3/Autocorrelation.slides.html#section-2",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The autocorrelation function for \\(Y_t\\) and \\(Y_{t-2}\\) includes the direct and indirect effect of \\(Y_{t-2}\\) on \\(Y_t\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#partial-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#partial-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "Partial autocorrelation function",
    "text": "Partial autocorrelation function\n\nMeasures the correlation between responses that are separated by \\(j\\) periods, excluding correlation due to responses separated by intervening periods."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-3",
    "href": "Module3/Autocorrelation.slides.html#section-3",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "In technical terms, the partial autocorrelation function fits the following linear regression model\n\\[\\hat{Y}_t = \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}\\] Where:\n\n\\(\\hat{Y}_{t}\\) is the predicted response at the current time (\\(t\\)).\n\\(\\hat{\\beta}_1\\) is the direct effect of \\(Y_{t-1}\\) on predicting \\(Y_{t}\\).\n\\(\\hat{\\beta}_2\\) is the direct effect of \\(Y_{t-2}\\) on predicting \\(Y_{t}\\).\n\nThe partial autocorrelation between \\(Y_t\\) and \\(Y_{t-2}\\) is equal to \\(\\hat{\\beta}_2\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-4",
    "href": "Module3/Autocorrelation.slides.html#section-4",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The partial autocorrelation function is visualized using a graph similar to that for autocorrelation.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the partial correlations between observations at different lags.\n\nIn Python, we use the plot_pacf function from statsmodels.\n\nplt.figure(figsize=(10, 6))\nplot_pacf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-5",
    "href": "Module3/Autocorrelation.slides.html#section-5",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The partial autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45. This is the same for the autocorrelation plot.\nThe partial autocorrelation plot shows that the responses and those from two periods ago have a correlation near 0.\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autoregressive-models",
    "href": "Module3/Autocorrelation.slides.html#autoregressive-models",
    "title": "Autocorrelation Models",
    "section": "Autoregressive models",
    "text": "Autoregressive models\nAutoregressive models are a type of linear regression model that directly incorporate the autocorrelation of the time series to predict the current response.\nTheir main characteristic is that the predictors of the current value of the series are its past values.\n\nAn autoregressive model of order 2 has the mathematical form: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}.\\)\nAn order 3 model looks like this: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2} + \\hat{\\beta}_3 Y_{t-3}.\\)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima-models",
    "href": "Module3/Autocorrelation.slides.html#arima-models",
    "title": "Autocorrelation Models",
    "section": "ARIMA models",
    "text": "ARIMA models\n\nA special class of autoregressive models are ARIMA (Autoregressive Integrated Moving Average).\n\nAn ARIMA model consists of three elements:\n\n\nIntegrated operators (integrated).\nAutoregressive terms (autoregressive).\nStochastic terms (moving average)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#integrated-or-differentiatedoperators-i",
    "href": "Module3/Autocorrelation.slides.html#integrated-or-differentiatedoperators-i",
    "title": "Autocorrelation Models",
    "section": "1. Integrated or differentiatedoperators (I)",
    "text": "1. Integrated or differentiatedoperators (I)\n\nThey create a new variable \\(Z_t\\), which equals the difference between the current response and the delayed response by a number of periods or lags.\nThere are three common levels of differentiation:\n\nLevel 0: \\(Z_t = Y_t\\).\nLevel 1: \\(Z_t = Y_t - Y_{t-1}\\).\nLevel 2: \\(Z_t = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-2",
    "href": "Module3/Autocorrelation.slides.html#example-2",
    "title": "Autocorrelation Models",
    "section": "Example 2",
    "text": "Example 2\nWe consider the time series “CanadianWorkHours.xlsx” that contains the average hours worked by a certain group of workers over a certain range of years.\n\nCanadianWorkHours = pd.read_excel('CanadianWorkHours.xlsx')\nCanadianWorkHours.head(4)\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n0\n1966\n37.2\n\n\n1\n1967\n37.0\n\n\n2\n1968\n37.4\n\n\n3\n1969\n37.5"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#creating-a-train-and-a-validation-data",
    "href": "Module3/Autocorrelation.slides.html#creating-a-train-and-a-validation-data",
    "title": "Autocorrelation Models",
    "section": "Creating a train and a validation data",
    "text": "Creating a train and a validation data\nRecall that we would like to train the model on earlier time periods and test it on later ones. To this end, we make the split using the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(CanadianWorkHours) * split_ratio)\n\n# Split the data\nCanadian_train = CanadianWorkHours[:split_point]\nCanadian_validation = CanadianWorkHours[split_point:]\n\nWe use 80% of the time series for training and the rest for validation."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#training-data",
    "href": "Module3/Autocorrelation.slides.html#training-data",
    "title": "Autocorrelation Models",
    "section": "Training data",
    "text": "Training data\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='Year', y='Hours per Week', data = Canadian_train)\nplt.xlabel('Year')\nplt.ylabel('Hours per Week')\nplt.title('Hours per Week Over Time')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-6",
    "href": "Module3/Autocorrelation.slides.html#section-6",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "In statsmodels, we apply the integration operator using the pre-loaded diff() function. The function’s k_diff argument specifies the order or level of the operator.\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Canadian_train['Hours per Week'], k_diff = 1)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-7",
    "href": "Module3/Autocorrelation.slides.html#section-7",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The time series with a level-1 operator looks like this.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_one)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 1')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-8",
    "href": "Module3/Autocorrelation.slides.html#section-8",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "A level-2 operator would work like this.\n\nZ_series_two = diff(Canadian_train['Hours per Week'], k_diff = 2)\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_two)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 2')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-9",
    "href": "Module3/Autocorrelation.slides.html#section-9",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "We see that the level 2 operator is more successful in removing the trend from the original time series."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#comments",
    "href": "Module3/Autocorrelation.slides.html#comments",
    "title": "Autocorrelation Models",
    "section": "Comments",
    "text": "Comments\n\nThe differentiation operator removes or de-trends the time series.\n\nThe level 0 differentiation operator leaves the time series intact.\nThe level 1 differentiation operator removes its linear trend.\nThe level 2 differentiation operator removes its quadratic trend."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-operator-level",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-operator-level",
    "title": "Autocorrelation Models",
    "section": "How do we determine the operator level?",
    "text": "How do we determine the operator level?\n\nVisualizing the time series and determining whether there is a linear or quadratic trend.\nIf level 1 and level 2 operators yield similar results, we choose level 1 because it is simpler.\nOnce this is done, we set our transformed variable \\(Z_t\\) as the new response variable."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autoregressive-ar-terms",
    "href": "Module3/Autocorrelation.slides.html#autoregressive-ar-terms",
    "title": "Autocorrelation Models",
    "section": "2. Autoregressive (AR) terms",
    "text": "2. Autoregressive (AR) terms\n\nHere we use autoregressive models, but with the new response variable \\(Z_t\\).\nWe can have different levels of order (or number of terms) in the autoregression model. For example:\n\nOrder 1 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1}\\).\nOrder 2 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2}\\).\n\nIf necessary, we can exclude the constant coefficient \\(\\hat{\\beta}_0\\) from the model."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-number-of-terms",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-number-of-terms",
    "title": "Autocorrelation Models",
    "section": "How do we determine the number of terms?",
    "text": "How do we determine the number of terms?\n\nUsing the correlation functions (ACF) and partial correlation functions (PACF) of the differenced series."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-10",
    "href": "Module3/Autocorrelation.slides.html#section-10",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nTo achieve this, we have some rules."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-1",
    "href": "Module3/Autocorrelation.slides.html#rule-1",
    "title": "Autocorrelation Models",
    "section": "Rule 1",
    "text": "Rule 1\n\nA first-order autoregressive model has:\n\nAn ACF with a single peak at the first period difference (lag).\nAn ACF with exponentially decreasing correlations."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-2",
    "href": "Module3/Autocorrelation.slides.html#rule-2",
    "title": "Autocorrelation Models",
    "section": "Rule 2",
    "text": "Rule 2\n\nA second-order autoregressive model has:\n\nA PACF with two peaks at the first period differences (lags).\nAn ACF with correlations that decrease positively and negatively but approach zero."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-3",
    "href": "Module3/Autocorrelation.slides.html#rule-3",
    "title": "Autocorrelation Models",
    "section": "Rule 3",
    "text": "Rule 3\n\nIf the PACF of the differenced series \\(Z_t\\) shows a higher partial correlation than the others and/or the lag-1 autocorrelation is positive, then consider adding an AR term to the model.\n\nThe lag at which the PACF cuts off from the confidence limits in the software is the indicated number of AR terms."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-11",
    "href": "Module3/Autocorrelation.slides.html#section-11",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Following part 1 of Rule 1, we conclude that an autoregressive term of order 2 will be sufficient to capture the relationships between the elements of the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#stochastic-terms-moving-averages-ma",
    "href": "Module3/Autocorrelation.slides.html#stochastic-terms-moving-averages-ma",
    "title": "Autocorrelation Models",
    "section": "3. Stochastic Terms (Moving Averages, MA)",
    "text": "3. Stochastic Terms (Moving Averages, MA)\nInstead of using past values of the response variable, a moving average model uses stochastic terms to predict the current response. The model has different versions depending on the number of errors used to predict the response. For example:\n\nMA of order 1: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1}\\);\nMA of order 2: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1} + \\theta_2 a_{t-2}\\),\n\nwhere \\(\\theta_0\\) is a constant and \\(a_t\\) are terms from a white noise series (i.e., random terms)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-i-choose-the-order-of-the-mas",
    "href": "Module3/Autocorrelation.slides.html#how-do-i-choose-the-order-of-the-mas",
    "title": "Autocorrelation Models",
    "section": "How do I choose the order of the MAs?",
    "text": "How do I choose the order of the MAs?\n\nRule 4: MA models have:\n\nCorrelations other than 0 in the ACF. The lags at which this occurs indicate the terms to include in the MA model.\nCorrelations in the PACF that gradually decrease to zero in some way."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-12",
    "href": "Module3/Autocorrelation.slides.html#section-12",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "This means that to determine the order, we focus primarily on the autocorrelation function. Remember, it’s the autocorrelation function of the series after it’s been differentiated.\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-13",
    "href": "Module3/Autocorrelation.slides.html#section-13",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Since there is no significant correlation for any lag above 0, we do not need any MA elements to model the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima",
    "href": "Module3/Autocorrelation.slides.html#arima",
    "title": "Autocorrelation Models",
    "section": "ARIMA",
    "text": "ARIMA\n\n\n\nDefine the response differentiation level and create \\(Z_t\\).\nDefine the order of the AR model (e.g., order 2).\nDefine the order of the MA model (e.g., order 1).\n\n\n\n\n\\[\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2} + \\theta_1 a_{t-1}\\]\n\n\n\nThe ARIMA model coefficients are estimated using an advanced method that takes into account the dependencies between the time series responses."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima-in-pyhon",
    "href": "Module3/Autocorrelation.slides.html#arima-in-pyhon",
    "title": "Autocorrelation Models",
    "section": "ARIMA in Pyhon",
    "text": "ARIMA in Pyhon\n\nTo fit an ARIMA model, we use the ARIMA() function from statsmodels.\nThe function has an important argument called order, which equals (p,d,q), where\n\np is the order of the autoregressive model.\nd is the level of the integration or differencing operator.\nq is the number of elements in the moving average."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-14",
    "href": "Module3/Autocorrelation.slides.html#section-14",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "From our previous analysis of the training data for the Canadian workhours example, we conclude that:\n\nWe must use a level-2 differencing operator to remove the quadratic trend from the series. Therefore, d = 2.\nOne autoregressive term should be sufficient to capture the patterns in the time series. Therefore, p = 2.\nIt is not necessary to have moving average terms. Therefore, q = 0."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-15",
    "href": "Module3/Autocorrelation.slides.html#section-15",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Once this is defined, we can train an ARIMA model using the training data with the following code:\n\nArima_Canadian = ARIMA(Canadian_train['Hours per Week'], \n                       order=(2, 2, 0))\nresults_ARIMA_Canadian = Arima_Canadian.fit()\n\nTechnically, ARIMA() defines the model and .fit() fits the model to the data using maximum likelihood estimation."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-16",
    "href": "Module3/Autocorrelation.slides.html#section-16",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "After fitting, we can get a summary of the model fit using the following code.\n\nprint(results_ARIMA_Canadian.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:         Hours per Week   No. Observations:                   28\nModel:                 ARIMA(2, 2, 0)   Log Likelihood                  -2.537\nDate:                Thu, 14 Aug 2025   AIC                             11.074\nTime:                        11:03:42   BIC                             14.849\nSample:                             0   HQIC                            12.161\n                                 - 28                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.3236      0.403     -0.803      0.422      -1.113       0.466\nar.L2         -0.4401      0.250     -1.760      0.078      -0.930       0.050\nsigma2         0.0699      0.021      3.356      0.001       0.029       0.111\n===================================================================================\nLjung-Box (L1) (Q):                   0.26   Jarque-Bera (JB):                 3.02\nProb(Q):                              0.61   Prob(JB):                         0.22\nHeteroskedasticity (H):               1.48   Skew:                             0.74\nProb(H) (two-sided):                  0.57   Kurtosis:                         3.75\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-17",
    "href": "Module3/Autocorrelation.slides.html#section-17",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The next step in evaluating an ARIMA model is to study the model’s residuals to ensure there is nothing else to explain in the model.\n\nWe can obtain the residuals using the following code.\n\nARIMA_residuals = results_ARIMA_Canadian.resid\nARIMA_residuals = ARIMA_residuals.drop(0)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#time-series-of-residuals",
    "href": "Module3/Autocorrelation.slides.html#time-series-of-residuals",
    "title": "Autocorrelation Models",
    "section": "Time series of residuals",
    "text": "Time series of residuals\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=ARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#correlation-plots",
    "href": "Module3/Autocorrelation.slides.html#correlation-plots",
    "title": "Autocorrelation Models",
    "section": "Correlation plots",
    "text": "Correlation plots"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-18",
    "href": "Module3/Autocorrelation.slides.html#section-18",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The three graphs show no obvious patterns or significant correlations between the residuals. Therefore, we say the model is correct."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#forecast",
    "href": "Module3/Autocorrelation.slides.html#forecast",
    "title": "Autocorrelation Models",
    "section": "Forecast",
    "text": "Forecast\nOnce the model is validated, we make predictions for elements in the time series.\n\nTo predict the average number of hours worked in the next, say, 3 years, we use the .forecast() function. The steps argument indicates the number of steps in the future to make the predictions.\n\nresults_ARIMA_Canadian.forecast(steps = 3)\n\n28    36.102917\n29    36.205233\n30    36.382827\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#model-evaluation-using-mse",
    "href": "Module3/Autocorrelation.slides.html#model-evaluation-using-mse",
    "title": "Autocorrelation Models",
    "section": "Model evaluation using MSE",
    "text": "Model evaluation using MSE\n\nInstead of evaluating the ARIMA model using graphical analyses of the residuals, we can take a more data-driven approach and evaluate the model using the mean squared error (MSE) or root MSE.\nTo this end, we simply use the mean_squared_error() function with the validation responses and our predictions."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#validation-data",
    "href": "Module3/Autocorrelation.slides.html#validation-data",
    "title": "Autocorrelation Models",
    "section": "Validation data",
    "text": "Validation data\n\nCanadian_validation\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n28\n1994\n36.0\n\n\n29\n1995\n35.7\n\n\n30\n1996\n35.7\n\n\n31\n1997\n35.5\n\n\n32\n1998\n35.6\n\n\n33\n1999\n36.3\n\n\n34\n2000\n36.5"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-19",
    "href": "Module3/Autocorrelation.slides.html#section-19",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The validation data has 7 time periods that can be determined using the command below.\n\nlen(Canadian_validation)\n\n7\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Canadian = results_ARIMA_Canadian.forecast(steps = 7)\nforecast_Canadian\n\n28    36.102917\n29    36.205233\n30    36.382827\n31    36.580331\n32    36.738265\n33    36.900243\n34    37.078325\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-20",
    "href": "Module3/Autocorrelation.slides.html#section-20",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Using the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(mse**(1/2), 2))\n\n0.75\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(rtwo, 2))\n\n-3.52\n\n\n\nA negative signifies that the model’s predictions are worse than simply predicting the average of the response"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonality",
    "href": "Module3/Autocorrelation.slides.html#seasonality",
    "title": "Autocorrelation Models",
    "section": "Seasonality",
    "text": "Seasonality\n\n\nSeasonality consists of repetitive or cyclical behavior that occurs with a constant frequency.\nIt can be identified from the series graph or using the ACF and PACF.\nTo do this, we must have removed the trend."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-3",
    "href": "Module3/Autocorrelation.slides.html#example-3",
    "title": "Autocorrelation Models",
    "section": "Example 3",
    "text": "Example 3\nWe use the Airline data containing the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#create-training-and-validation-data",
    "href": "Module3/Autocorrelation.slides.html#create-training-and-validation-data",
    "title": "Autocorrelation Models",
    "section": "Create training and validation data",
    "text": "Create training and validation data\n\nWe use 80% of the time series for training and the rest for validation.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Airline_data) * split_ratio)\n\n# Split the data\nAirline_train = Airline_data[:split_point]\nAirline_validation = Airline_data[split_point:]"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#training-data-1",
    "href": "Module3/Autocorrelation.slides.html#training-data-1",
    "title": "Autocorrelation Models",
    "section": "Training data",
    "text": "Training data\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_train)\nplt.xlabel('T')\nplt.ylabel('Number of passengers')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-22",
    "href": "Module3/Autocorrelation.slides.html#section-22",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "First, let’s use a level-1 operator.\n\nZ_series_one = diff(Airline_train['Number of passengers'], k_diff = 1)\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x=Airline_train['Number of passengers'], y=Z_series_one)\nplt.xlabel('T')\nplt.ylabel('Difference Level 1')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-plots",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-plots",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#sarima-model",
    "href": "Module3/Autocorrelation.slides.html#sarima-model",
    "title": "Autocorrelation Models",
    "section": "SARIMA model",
    "text": "SARIMA model\nThe SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an extension of the ARIMA model for modeling seasonality patterns.\nThe SARIMA model has three additional elements for modeling seasonality in time series.\n\nDifferenced or integrated operators (integrated) for seasonality.\nAutoregressive terms (autoregressive) for seasonality.\nStochastic terms or moving averages (moving average) for seasonality."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#notation",
    "href": "Module3/Autocorrelation.slides.html#notation",
    "title": "Autocorrelation Models",
    "section": "Notation",
    "text": "Notation\n\nSeasonality in a time series is a regular pattern of change that repeats over \\(S\\) time periods, where \\(S\\) defines the number of time periods until the pattern repeats again.\nFor example, there is seasonality in monthly data, where high values always tend to occur in some particular months and low values always tend to occur in other particular months.\nIn this case, \\(S=12\\) (months per year) is the length of periodic seasonal behavior. For quarterly data, \\(S=4\\) time periods per year."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonal-differentiation",
    "href": "Module3/Autocorrelation.slides.html#seasonal-differentiation",
    "title": "Autocorrelation Models",
    "section": "Seasonal differentiation",
    "text": "Seasonal differentiation\n\nThis is the difference between a response and a response with a lag that is a multiple of \\(S\\).\nFor example, with monthly data \\(S=12\\),\n\nA level 1 seasonal difference is \\(Y_{t} - Y_{t-12}\\).\nA level 2 seasonal difference is \\((Y_{t-12}) - (Y_{t-12} - Y_{t-24})\\).\n\nSeasonal differencing eliminates the seasonal trend and can also eliminate a type of nonstationarity caused by a seasonal random walk."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonal-ar-and-ma-terms",
    "href": "Module3/Autocorrelation.slides.html#seasonal-ar-and-ma-terms",
    "title": "Autocorrelation Models",
    "section": "Seasonal AR and MA Terms",
    "text": "Seasonal AR and MA Terms\nIn SARIMA, the seasonal AR and MA component terms predict the current response (\\(Y_t\\)) using responses and errors at times with lags that are multiples of \\(S\\).\nFor example, with monthly data \\(S = 12\\),\n\n\nThe first-order seasonal AR model would use \\(Y_{t-12}\\) to predict \\(Y_{t}\\).\nThe second-order seasonal AR model would use \\(Y_{t-12}\\) and \\(Y_{t-24}\\) to predict \\(Y_{t}\\).\nThe first-order seasonal MA model would use the stochastic term \\(a_{t-12}\\) as a predictor.\nThe second-order seasonal MA model would use the stochastic terms \\(a_{t-12}\\) and \\(a_{t-24}\\) as predictors."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-23",
    "href": "Module3/Autocorrelation.slides.html#section-23",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "To fit the SARIMA model, we use the ARIMA() function from statsmodels, but with an additional argument, seasonal_order=(0, 0, 0, 0).\n\nThis is the order (P, D, Q, s) of the seasonal component of the model for the autoregressive parameters, differencing operator levels, moving average parameters, and periodicity.\n\nRecall that the function has the argument order = (p, d, q) where p is the order of the autoregressive model, d is the differencing operator level, and q is the number of elements in the moving average.\nThese arguments capture the detailed information of the time series, while seasonal_order captures the patterns given by seasonality."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-24",
    "href": "Module3/Autocorrelation.slides.html#section-24",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Let’s fit a SARIMA model.\n\nSARIMA_model = ARIMA(Airline_train['Number of passengers'], order=(1, 2, 1), \n                      seasonal_order=(1, 1, 0, 12))\nSARIMA_Airline = SARIMA_model.fit()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#summary-of-fit",
    "href": "Module3/Autocorrelation.slides.html#summary-of-fit",
    "title": "Autocorrelation Models",
    "section": "Summary of fit",
    "text": "Summary of fit\n\nprint(SARIMA_Airline.summary())\n\n                                     SARIMAX Results                                     \n=========================================================================================\nDep. Variable:              Number of passengers   No. Observations:                  115\nModel:             ARIMA(1, 2, 1)x(1, 1, [], 12)   Log Likelihood                -374.241\nDate:                           Thu, 14 Aug 2025   AIC                            756.482\nTime:                                   11:03:44   BIC                            766.943\nSample:                                        0   HQIC                           760.717\n                                           - 115                                         \nCovariance Type:                             opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.1729      0.094     -1.833      0.067      -0.358       0.012\nma.L1         -0.9999     14.439     -0.069      0.945     -29.300      27.300\nar.S.L12      -0.1303      0.084     -1.543      0.123      -0.296       0.035\nsigma2        91.7900   1326.009      0.069      0.945   -2507.139    2690.719\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 3.18\nProb(Q):                              0.98   Prob(JB):                         0.20\nHeteroskedasticity (H):               1.13   Skew:                             0.39\nProb(H) (two-sided):                  0.73   Kurtosis:                         2.64\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#residual-analysis",
    "href": "Module3/Autocorrelation.slides.html#residual-analysis",
    "title": "Autocorrelation Models",
    "section": "Residual analysis",
    "text": "Residual analysis\nWe can have a graphical evaluation of the model’s performance using a residual analysis.\n\nSARIMA_residuals = SARIMA_Airline.resid\nSARIMA_residuals = SARIMA_residuals.drop(0)\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\nsns.lineplot(x=Airline_train['T'], y=SARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-25",
    "href": "Module3/Autocorrelation.slides.html#section-25",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#validation",
    "href": "Module3/Autocorrelation.slides.html#validation",
    "title": "Autocorrelation Models",
    "section": "Validation",
    "text": "Validation\nThe validation data has 29 time periods that can be determined using the command below.\n\nlen(Airline_validation)\n\n29\n\n\n\nSo, we must forecast 7 periods ahead using our SARIMA model.\n\nforecast_Airline = SARIMA_Airline.forecast(steps = 29)\nforecast_Airline.head(3)\n\n115    489.635426\n116    428.961078\n117    373.169684\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-26",
    "href": "Module3/Autocorrelation.slides.html#section-26",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Using the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Airline_validation[\"Number of passengers\"], \n                        forecast_Airline)  \nprint(round(mse**(1/2), 2))\n\n26.39\n\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Airline_validation[\"Number of passengers\"], forecast_Airline)  \nprint(round(rtwo, 2))\n\n0.89"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#agenda",
    "href": "Module3/PredictiveModels.slides.html#agenda",
    "title": "Predictive Models and Time Series",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nTime Series\nLinear Regression Model for Time Series"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#load-the-libraries",
    "href": "Module3/PredictiveModels.slides.html#load-the-libraries",
    "title": "Predictive Models and Time Series",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#main-data-science-problems",
    "href": "Module3/PredictiveModels.slides.html#main-data-science-problems",
    "title": "Predictive Models and Time Series",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#main-data-science-problems-1",
    "href": "Module3/PredictiveModels.slides.html#main-data-science-problems-1",
    "title": "Predictive Models and Time Series",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#regression-problem",
    "href": "Module3/PredictiveModels.slides.html#regression-problem",
    "title": "Predictive Models and Time Series",
    "section": "Regression problem",
    "text": "Regression problem\n\nGoal: Find the best function \\(f(\\mathbf{X})\\) of the predictors \\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) that describes the response \\(Y\\).\nIn mathematical terms, we want to establish the following relationship:\n\\[Y = f(\\mathbf{X}) + \\epsilon\\]\n\nWhere \\(\\epsilon\\) is a natural (random) error."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx",
    "href": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx",
    "title": "Predictive Models and Time Series",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx-1",
    "href": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx-1",
    "title": "Predictive Models and Time Series",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "href": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "title": "Predictive Models and Time Series",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "href": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "title": "Predictive Models and Time Series",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#moreover",
    "href": "Module3/PredictiveModels.slides.html#moreover",
    "title": "Predictive Models and Time Series",
    "section": "Moreover…",
    "text": "Moreover…\n\n\n\n\nWe can use test data for a final evaluation of the model.\nTest data is data obtained from the process that generated the training data.\nTest data is independent of the training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model",
    "title": "Predictive Models and Time Series",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nA common candidate function for predicting a response is the linear regression model. It has the mathematical form:\n\\[\\hat{Y}_i = \\hat{f}(X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i.\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) associated with a predictor value equal to \\(X_i\\).\nThe values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are called the coefficients of the model."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section",
    "href": "Module3/PredictiveModels.slides.html#section",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "The values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the test data set and the least squares method.\nThis method finds the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the error made by the model \\(\\hat{f}(X_i)\\) when trying to predict the responses of the training set.\n\nTechnically, the method minimizes the following expression\n\n\\[(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_t} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_t} ))^2 \\]\n\nFor the \\(n_t\\) the training data!"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#the-idea-in-two-dimensions",
    "href": "Module3/PredictiveModels.slides.html#the-idea-in-two-dimensions",
    "title": "Predictive Models and Time Series",
    "section": "The idea in two dimensions",
    "text": "The idea in two dimensions"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-1",
    "href": "Module3/PredictiveModels.slides.html#example-1",
    "title": "Predictive Models and Time Series",
    "section": "Example 1",
    "text": "Example 1\n\nWe used the dataset called “Advertising.xlsx” in Canvas.\n\nTV: Money spent on TV ads for a product ($).\nSales: Sales generated from the product ($).\n200 markets\n\n\n# Load the data into Python\nAds_data = pd.read_excel('Advertising.xlsx')"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-1",
    "href": "Module3/PredictiveModels.slides.html#section-1",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Ads_data.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-2",
    "href": "Module3/PredictiveModels.slides.html#section-2",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Now, let’s choose our predictor and response. In the definition of X_full, the double bracket in [] is important because it allows us to have a pandas DataFrame as output. This makes it easier to fit the linear regression model with scikit-learn.\n\n# Chose the predictor.\nX_full = Ads_data.filter(['TV'])\n\n# Set the response.\nY_full = Ads_data.filter(['Sales'])"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#create-training-and-validation-data",
    "href": "Module3/PredictiveModels.slides.html#create-training-and-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Create training and validation data",
    "text": "Create training and validation data\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into a training dataset and a validation dataset. To do this, we use the scikit-learn train_test_split() function.\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.25,\n                                                      random_state = 301655)\n\nWe use 75% of the data for training and the rest for validation."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#fit-a-linear-regression-model-in-python",
    "href": "Module3/PredictiveModels.slides.html#fit-a-linear-regression-model-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Fit a linear regression model in Python",
    "text": "Fit a linear regression model in Python\n\nIn Python, we use the LinearRegression() and fit() functions from the scikit-learn to fit a linear regression model.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-3",
    "href": "Module3/PredictiveModels.slides.html#section-3",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "The following commands allow you to show the estimated coefficients of the model.\n\nprint(\"Coefficients:\", LRmodel.coef_)\n\nCoefficients: [[0.05185463]]\n\n\nWe can also show the estimated intercept.\n\nprint(\"Intercept:\", LRmodel.intercept_)\n\nIntercept: [6.69303889]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 6.69 + 0.051 X_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model-assumptions",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model-assumptions",
    "title": "Predictive Models and Time Series",
    "section": "Linear regression model assumptions",
    "text": "Linear regression model assumptions\n\nTo use the regression model, the model errors \\(e_i = Y_i - \\hat{Y}_i\\) obtained on the training data must meet three conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nThese assumptions are evaluated using a graphical analysis of residuals (model errors)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python",
    "href": "Module3/PredictiveModels.slides.html#in-python",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nTo conduct a residual analysis, we need to obtain the predicted values and residuals of the model first (on the training data).\n\n# Fitted values.\nfitted = LRmodel.predict(X_train)\n\n# Residuals\nresiduals = Y_train - fitted\n\nFor plotting, we put everything together in a pandas dataframe.\n\n# Create a DataFrame for plotting\n#results_df = pd.DataFrame()\n#results_df['predicted'] = list(fitted)\n#results_df['actual'] = list(Y_train)\n#results_df['residual'] = results_df['predicted'] - results_df['actual']\n#results_df = results_df.sort_values(by='residual').reset_index(drop=True)\n#results_df.describe()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-4",
    "href": "Module3/PredictiveModels.slides.html#section-4",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Code\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = fitted, y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Fitted\")\n#plt.xlabel(\"Fitted values\")\n#plt.ylabel(\"Residuals\")\n#plt.show()\n\n\n\n\nCode\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = range(len(residuals)), y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Row Order\")\n#plt.xlabel(\"Observation Order\")\n#plt.ylabel(\"Residuals\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#prediction-error",
    "href": "Module3/PredictiveModels.slides.html#prediction-error",
    "title": "Predictive Models and Time Series",
    "section": "Prediction error",
    "text": "Prediction error\nAfter estimating and validating the linear regression model, we can check the quality of its predictions on unobserved data. That is, on the data in the validation set.\nOne metric for this is the mean prediction error (MSE\\(_v\\)):\n\n\\[\\text{MSE}_v = \\frac{(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_v} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_v} ))^2}{n_v}\\]\n\n\nFor \\(n_v\\), the validation data!\n\nThe smaller \\(\\text{MSE}_v\\), the better the predictions."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-5",
    "href": "Module3/PredictiveModels.slides.html#section-5",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "In practice, the square root of the mean prediction error is used:\n\\[\\text{RMSE}_v = \\sqrt{\\text{MSE}_v}.\\]\nThe advantage of \\(\\text{RMSE}_v\\) is that it can be interpreted as:\n\nThe average variability of a model prediction.\n\nFor example, if \\(\\text{RMSE}_v = 1\\), then a prediction of \\(\\hat{Y} = 5\\) will have an (average) error rate of \\(\\pm 1\\)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-1",
    "href": "Module3/PredictiveModels.slides.html#in-python-1",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\n\nTo evaluate the model’s performance, we use the validation dataset. Specifically, we use the predictor matrix stored in X_valid.\n\nIn Python, we make the prediction using the pre-trained LRmodel.\n\nY_pred = LRmodel.predict(X_valid)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-6",
    "href": "Module3/PredictiveModels.slides.html#section-6",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "To evaluate the model, we use the mean squared error in the Python mse() function. Recall that the responses from the validation dataset are in Y_valid, and the model predictions are in Y_pred.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse, 2))\n\n15.49\n\n\n\nTo obtain the root mean squared error (RMSE), we simply take the square root of the MSE.\n\nprint(round(mse**(1/2), 2))\n\n3.94"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#another-metric-r2",
    "href": "Module3/PredictiveModels.slides.html#another-metric-r2",
    "title": "Predictive Models and Time Series",
    "section": "Another Metric: \\(R^2\\)",
    "text": "Another Metric: \\(R^2\\)\n\nIn the context of Data Science, \\(R^2\\) can be interpreted as the squared correlation between the actual responses and those predicted by the model.\nThe higher the correlation, the better the agreement between the predicted and actual responses.\n\n\nWe compute \\(R^2\\) in Python as follows:\n\nrtwo_sc = r2_score(Y_valid, Y_pred)  # Rsquared\nprint(round(rtwo_sc, 2))\n\n0.33"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode",
    "href": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode",
    "title": "Predictive Models and Time Series",
    "section": "Mini-Activity (cooperative mode)",
    "text": "Mini-Activity (cooperative mode)\n\n\nConsider the Advertising.xlsx dataset in Canvas.\nUse a model to predict Sales that includes the Radio predictor (money spent on radio ads for a product ($)). What is the \\(\\text{RMSE}_v\\)?\nNow, use a model to predict Sales that includes two predictors: TV and Radio. What is the \\(\\text{RMSE}_v\\)?\nWhich model do you prefer?"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#other-candidate-functions",
    "href": "Module3/PredictiveModels.slides.html#other-candidate-functions",
    "title": "Predictive Models and Time Series",
    "section": "Other candidate functions",
    "text": "Other candidate functions\nThe linear regression model is one of the most common models for predicting a response. It is simple and easy to calculate and interpret.\nHowever, it can be limited for very complex problems.\nFor this purpose, there are other, more advanced candidate functions \\(\\hat{f}(X)\\), such as:\n\nK nearest neighbors.\nDecision or regression trees.\nEnsamble methods (bagging and random forest)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#what-is-a-time-series",
    "href": "Module3/PredictiveModels.slides.html#what-is-a-time-series",
    "title": "Predictive Models and Time Series",
    "section": "What is a time series?",
    "text": "What is a time series?\n\n\nIt is a sequence of observations collected at successive time intervals.\nTime series data is commonly used in fields such as finance, economics, weather forecasting, signal processing, and many others.\nAnalyzing time series data helps us understand patterns, trends, and behaviors over time, enabling prediction, anomaly detection, and decision-making."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-2",
    "href": "Module3/PredictiveModels.slides.html#example-2",
    "title": "Predictive Models and Time Series",
    "section": "Example 2",
    "text": "Example 2"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-7",
    "href": "Module3/PredictiveModels.slides.html#section-7",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Technically, a time series is a set of observations about a (discrete) predictor \\(T\\) and a response \\(Y\\).\nObservations of \\(Y\\) are recorded at the moments or times given by the predictor \\(T\\).\nThe special feature of the time series is that the observations of \\(Y\\) are not independent!\n\n\n\n\n\nDay\nT\nTemperature (Y)\n\n\n\n\nMonday\n1\n10\n\n\nTuesday\n2\n12\n\n\nWednesday\n3\n15\n\n\nThursday\n4\n14\n\n\nFriday\n5\n18"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-3-amtrak-data",
    "href": "Module3/PredictiveModels.slides.html#example-3-amtrak-data",
    "title": "Predictive Models and Time Series",
    "section": "Example 3: Amtrak data",
    "text": "Example 3: Amtrak data\n\n\nThe Amtrak train company in the USA collects data on the number of passengers traveling on its trains.\nRecords are available from January 1991 to March 2004.\nThe data is available in “Amtrak.xlsx” on Canvas.\n\n\nAmtrak_data = pd.read_excel('Amtrak.xlsx')"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-8",
    "href": "Module3/PredictiveModels.slides.html#section-8",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Amtrak_data.head()\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#time-series-plot-in-python",
    "href": "Module3/PredictiveModels.slides.html#time-series-plot-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Time series plot in Python",
    "text": "Time series plot in Python\nWe can create a line graph to visualize the evolution of Amtrak train ridership over time using lineplot from seaborn.\n\n\nCode\nplt.figure(figsize=(6, 4))\nsns.lineplot(x='Month', y='Ridership (in 000s)', data = Amtrak_data)\nplt.xlabel('Month')\nplt.ylabel('Ridership')\nplt.title('Amtrak Ridership Over Time')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#informative-series",
    "href": "Module3/PredictiveModels.slides.html#informative-series",
    "title": "Predictive Models and Time Series",
    "section": "Informative Series",
    "text": "Informative Series\n\nAn informative time series is a series that contains patterns that we can use to predict future values of the series.\nThe three possible patterns are:\n\n\nTrend: the series has an increasing/decreasing behavior.\nSeasonality: the series has a repeating cyclical pattern in its values.\nAutocorrelation: the series follows a pattern that can be described by previous values of the series."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-4-airline-data",
    "href": "Module3/PredictiveModels.slides.html#example-4-airline-data",
    "title": "Predictive Models and Time Series",
    "section": "Example 4: Airline data",
    "text": "Example 4: Airline data\n\n\n\n\n\nThis series has an upper trend.\nThis series has cyclical patterns in its values.\nAlthough not immediately visible, we can use the previous values of the series to describe the future ones."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#non-informative-series-white-noise",
    "href": "Module3/PredictiveModels.slides.html#non-informative-series-white-noise",
    "title": "Predictive Models and Time Series",
    "section": "Non-informative series: White noise",
    "text": "Non-informative series: White noise\n\n\n\n\n\nWhite noise is a series whose values, on average, are 0 and have a constant variation.\nIts values are also independent of each other.\nIt is used to describe random or natural error."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model-1",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model-1",
    "title": "Predictive Models and Time Series",
    "section": "Linear regression model",
    "text": "Linear regression model\n\nThe linear regression model is useful for capturing patterns in a time series. In this context, the model takes the form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) at time \\(T_i\\)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#trend",
    "href": "Module3/PredictiveModels.slides.html#trend",
    "title": "Predictive Models and Time Series",
    "section": "Trend",
    "text": "Trend\n\nThe trend of the time series is captured by the value of \\(\\hat{\\beta}_1\\) at\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nIf \\(\\hat{\\beta}_1\\) is positive, the series has an upward trend.\nIf \\(\\hat{\\beta}_1\\) is negative, the series has a downward trend.\n\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the least squares method."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#model-evaluation",
    "href": "Module3/PredictiveModels.slides.html#model-evaluation",
    "title": "Predictive Models and Time Series",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nRemember that the errors of the linear regression model (\\(e_i = Y_i - \\hat{Y}_i\\)) must meet two conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nIn the context of time series, this means that the model errors \\(e_i\\) must behave like white noise that contains no patterns."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-3-amtrak-data-cont.",
    "href": "Module3/PredictiveModels.slides.html#example-3-amtrak-data-cont.",
    "title": "Predictive Models and Time Series",
    "section": "Example 3: Amtrak data (cont.)",
    "text": "Example 3: Amtrak data (cont.)\n\nLet’s fit a linear regression model to the ridership data from Amtrak."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#creating-a-train-and-a-validation-data",
    "href": "Module3/PredictiveModels.slides.html#creating-a-train-and-a-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Creating a train and a validation data",
    "text": "Creating a train and a validation data\nIn time series, the order of the data matters because each observation is tied to a specific point in time.\nUnlike typical datasets where observations are independent of one another, time series data follows a sequence where past values influence future ones.\nBecause of this, we cannot randomly split the data using a function like train_test_split(). Doing so might result in a situation where the model learns from future values to predict past ones—which doesn’t make sense and would lead to overly optimistic performance."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-9",
    "href": "Module3/PredictiveModels.slides.html#section-9",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Instead, we want to mimic real-world forecasting: train the model on earlier time periods and test it on later ones.\nTo help us do this properly, we use the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Amtrak_data) * split_ratio)\n\n# Split the data\nAmtrak_train = Amtrak_data[:split_point]\nAmtrak_validation = Amtrak_data[split_point:]\n\nThis code ensures that the training data always comes before the validation data in time, preserving the temporal order. The proportion of data that goes to training is set using split_ratio."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#fit-linear-regression-model",
    "href": "Module3/PredictiveModels.slides.html#fit-linear-regression-model",
    "title": "Predictive Models and Time Series",
    "section": "Fit linear regression model",
    "text": "Fit linear regression model\nWe first set the predictor and response.\n\n# Set predictor.\nX_train = Amtrak_train.filter(['t'])\n\n# Set response.\nY_train = Amtrak_train.filter(['Ridership (in 000s)'])\n\nNext, we fit the model using LinearRegression() and fit() from scikit-learn.\n\n# 1. Create linear regression model.\nLRmodelAmtrak = LinearRegression()\n\n# 2. Fit the model to the training data.\nLRmodelAmtrak.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-11",
    "href": "Module3/PredictiveModels.slides.html#section-11",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s inspect the estimated coefficient for the predictor (time).\n\nprint(LRmodelAmtrak.coef_)\n\n[[-1.2818326]]\n\n\nAnd the intercept.\n\nprint(LRmodelAmtrak.intercept_)\n\n[1810.77686661]\n\n\nThe estimated model then is:\n\\[\\hat{Y}_i = 1810.777 - 1.281 T_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\nWe can validate the model using a residual analysis on the training data. To this end, we first compute the predicted values and residuals of the model.\n\nY_pred = LRmodelAmtrak.predict(X_train)\nresiduals = Y_train - Y_pred"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-12",
    "href": "Module3/PredictiveModels.slides.html#section-12",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Code\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=X_train, y=residuals)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#the-model-is-more-flexible-than-that",
    "href": "Module3/PredictiveModels.slides.html#the-model-is-more-flexible-than-that",
    "title": "Predictive Models and Time Series",
    "section": "The model is more flexible than that",
    "text": "The model is more flexible than that\n\nIf necessary, the linear regression model can be extended to capture quadratic relationships. For this, the model takes the following form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i + \\hat{\\beta}_2 T^{2}_i \\]\n\nWhere \\(T^{2}_i\\) is the squared value of the time index.\n\\(\\hat{\\beta}_2\\) is a term that captures possible curvature in the time series."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-2",
    "href": "Module3/PredictiveModels.slides.html#in-python-2",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nTo include a quadratic term, we must augment our predictor matrix with an additional column. The following code shows how to augment X_full by the square of the Amtrak_data['t'] column. This is done using the pandas .concat() function. The resulting matrix is stored in X_quad.\n\nX_quad = pd.concat([X_train, Amtrak_train['t']**2], axis = 1)\n\nNext, we follow the same steps to fit this model.\n\n# 1. Create linear regression model\nQuadmodelAmtrak = LinearRegression()\n\n# 2. Fit linear regression model\nQuadmodelAmtrak.fit(X_quad, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-13",
    "href": "Module3/PredictiveModels.slides.html#section-13",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "We show the estimated coefficients in Python.\n\nprint(\"Intercept = \", QuadmodelAmtrak.intercept_)\nprint(\"Coefficients = \", QuadmodelAmtrak.coef_)\n\nIntercept =  [1866.84019635]\nCoefficients =  [[-4.64563238  0.03397778]]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 1866.84 - 4.65 T_i + 0.03 T^2_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-1",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-1",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\n\n\n\nCode\n# Remember to use the same `X_quad`\nY_pred_quad = QuadmodelAmtrak.predict(X_quad)\nresiduals_quad = Y_train - Y_pred_quad\n\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred_quad, y=residuals_quad, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=Amtrak_train['t'], y=residuals_quad)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#model-evaluation-using-validation-data",
    "href": "Module3/PredictiveModels.slides.html#model-evaluation-using-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Model evaluation using validation data",
    "text": "Model evaluation using validation data\n\nRemember that another way to evaluate the performance of a model is using the \\(\\text{MSE}_v\\) or \\(\\text{RMSE}_v\\) on the validation data. To this end, we need some Python objects.\n\n# Set predictor.\nX_valid = Amtrak_validation.filter(['t'])\n\n# Set response.\nY_valid = Amtrak_validation.filter(['Ridership (in 000s)'])"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-14",
    "href": "Module3/PredictiveModels.slides.html#section-14",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s compute the \\(\\text{MSE}_v\\) for the linear regression model.\n\nY_val_pred_lin = LRmodelAmtrak.predict(X_valid)\nmse = mean_squared_error(Y_valid, Y_val_pred_lin)  \nprint(round(mse, 2))\n\n62517.47\n\n\nLet’s do the same for the the linear regression model with a quadratic term.\n\nX_quad_valid = pd.concat([X_valid, Amtrak_validation['t']**2], axis = 1)\nY_val_pred_quad = QuadmodelAmtrak.predict(X_quad_valid)\nmse_quad = mean_squared_error(Y_valid, Y_val_pred_quad)  # Mean Squared Error (MSE)\nprint(round(mse_quad, 2))\n\n30271.15\n\n\nWe conclude that the linear model with a quadratic term is better than the linear regression model because the \\(\\text{MSE}_v\\) of the former is smaller than for the latter."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode-1",
    "href": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode-1",
    "title": "Predictive Models and Time Series",
    "section": "Mini-Activity (cooperative mode)",
    "text": "Mini-Activity (cooperative mode)\n\nConsider the dataset CanadianWorkHours.xlsx in Canvas.\nSplit the data into training and validation\nVisualize the series in Python. The response variable is Working Hours and the predictor is Year.\nUsing Python, answer the question: Which of the following models best fits the series?\n\n\nLinear trend regression model.\nQuadratic trend regression model.\nExponential trend regression model."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#identifying-heteroskedasticity",
    "href": "Module3/PredictiveModels.slides.html#identifying-heteroskedasticity",
    "title": "Predictive Models and Time Series",
    "section": "Identifying Heteroskedasticity",
    "text": "Identifying Heteroskedasticity\n\nHeteroskedasticity arises when the dispersion of model errors is not constant over time.\nTo see it, let’s go back to the Airline data, which contains the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-15",
    "href": "Module3/PredictiveModels.slides.html#section-15",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Airline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-16",
    "href": "Module3/PredictiveModels.slides.html#section-16",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "For illustrative purposes, we will not split the time series into training and validation datasets.\n\n\nCode\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_data)\nplt.xlabel('Time')\nplt.ylabel('Number of passengers')\nplt.title('Number of passengers across time')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-17",
    "href": "Module3/PredictiveModels.slides.html#section-17",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s fit a linear regression model.\n\n# Set predictor.\nX_full = Airline_data.filter(['T'])\n\n# Set response.\nY_full = Airline_data.filter(['Number of passengers'])\n\n# 1. Create linear regression\nLRmodelAirline = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirline.fit(X_full, Y_full)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-2",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-2",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\nHeteroskedasticity: Dispersion of the residuals increases with the predicted value.\n\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#solution",
    "href": "Module3/PredictiveModels.slides.html#solution",
    "title": "Predictive Models and Time Series",
    "section": "Solution",
    "text": "Solution\n\nIf we identify heteroskedasticity in the regression model errors, we have several transformation options for our original series.\n\nA common transformations to the time series \\(Y_i\\) is the Natural Logarithm\nIf the original time series contains negative values, it can be lagged by adding the negative of its minimum value."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-3",
    "href": "Module3/PredictiveModels.slides.html#in-python-3",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nThe easiest way to apply the logarithm in Python is to use the log() function from the numpy library\n\nlog_Y_full = np.log( Y_full )\n\nNow, the response to use is in log_Y_full.\n\nThe steps to fit a linear regression model are similar.\n\n# 1. Create linear regression\nLRmodelAirlineTransformed = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirlineTransformed.fit(X_full, log_Y_full)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-3",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-3",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\n\nWith transformation\n\n\nCode\nY_pred_log = LRmodelAirlineTransformed.predict(X_full)\nresiduals_log = log_Y_full - Y_pred_log\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred_log, y=residuals_log, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWithout transformation\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#what-do-i-do-if-the-transformation-doesnt-work",
    "href": "Module3/PredictiveModels.slides.html#what-do-i-do-if-the-transformation-doesnt-work",
    "title": "Predictive Models and Time Series",
    "section": "What do I do if the transformation doesn’t work?",
    "text": "What do I do if the transformation doesn’t work?\n\n\nIf the log transformation doesn’t significantly reduce heteroskedasticity, there are models for modeling variance called GARCH.\nYou can find literature on these models and their software implementations in a time series textbook such as Time Series Analysis with Applications in R by Cryer and Chan."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#seasonality",
    "href": "Module3/PredictiveModels.slides.html#seasonality",
    "title": "Predictive Models and Time Series",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to repetitive or cyclical behavior that occurs with a constant frequency.\n\n\n\nExamples:\n\nDemand for winter clothing\nDemand for tourist travel\nAmount of rainfall throughout the year."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#capturing-seasonality",
    "href": "Module3/PredictiveModels.slides.html#capturing-seasonality",
    "title": "Predictive Models and Time Series",
    "section": "Capturing seasonality",
    "text": "Capturing seasonality\n\n\n\nThe linear regression model can be extended to capture seasonal patterns in the time series.\nTo do this, an additional categorical predictor is created that indicates the season to which each data item belongs.\nBehind the scenes, the additional categorical predictor is transformed into several auxiliary numerical predictors."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#analyzing-seasonal-series-in-python",
    "href": "Module3/PredictiveModels.slides.html#analyzing-seasonal-series-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Analyzing seasonal series in Python",
    "text": "Analyzing seasonal series in Python\nConsider the data in Amtrak_train with the additional predictor of Season to model seasonality.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-18",
    "href": "Module3/PredictiveModels.slides.html#section-18",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "To fit a linear regression model with a categorical variable like Season, we must transform the text categories into numbers. To do this, we use dummy variables constructed using the following commands.\n\ndummy_data = pd.get_dummies(Amtrak_train['Season'], dtype = 'int')\ndummy_data.head(4)\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-19",
    "href": "Module3/PredictiveModels.slides.html#section-19",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Simply put, the matrix above contains one column for each month. Each column indicates the observations that belong to the month of the column. For example, the column Apr has the values 0 and 1. The value 1 indicates that the corresponding observation belongs to the month of April. A 0 indicates otherwise.\n\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-20",
    "href": "Module3/PredictiveModels.slides.html#section-20",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Unfortunately, we cannot use the matrix as is in the linear regression model. This is due to multicollinearity issues. Technically, this happens because if you add all the columns, the resulting column is a column of 1s, which is already used by the intercept. Therefore, you cannot fit a model with the intercept and all the columns of the dummy variables.\n\nTo solve this problem, we arbitrarily remove a column from the matrix above. For example, let’s remove Dec.\n\ndummy_data = dummy_data.drop([\"Dec\"], axis = 1)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-21",
    "href": "Module3/PredictiveModels.slides.html#section-21",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Now, let’s build the complete matrix of predictors, including the column for time, time squared, and the dummy variables.\n\nX_quad_season = pd.concat([Amtrak_train['t'], Amtrak_train['t']**2, dummy_data], \n                          axis = 1)\n\n\nNext, we fit the model with all the terms in the matrix above.\n\n# 0. Ensure that we have the response in `Y_full`.\nY_train = Amtrak_train['Ridership (in 000s)']\n\n# 1. Create linear regression model.\nSeasonmodelAmtrak = LinearRegression()\n\n# 2. Fit the linear regression model.\nSeasonmodelAmtrak.fit(X_quad_season, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#estimated-model-coefficients",
    "href": "Module3/PredictiveModels.slides.html#estimated-model-coefficients",
    "title": "Predictive Models and Time Series",
    "section": "Estimated model coefficients",
    "text": "Estimated model coefficients\n\n\nprint(\"Intercept = \", SeasonmodelAmtrak.intercept_)\nprint(\"Coefficients = \", SeasonmodelAmtrak.coef_)\n\nIntercept =  1924.3205889003025\nCoefficients =  [-6.12171259e+00  4.96085283e-02  1.22309969e+01  1.77720172e+02\n -2.83088245e+02 -2.25659158e+02  1.15685204e+02 -7.31735621e+00\n  1.59876604e+01  5.17037414e+01 -4.77796001e+01 -3.19949172e+01\n -1.45366826e+02]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-4",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-4",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\nCode\nY_pred = SeasonmodelAmtrak.predict(X_quad_season)\nresiduals = Y_train - Y_pred\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=Amtrak_data['t'], y=residuals)\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#predictions-on-the-validation-dataset",
    "href": "Module3/PredictiveModels.slides.html#predictions-on-the-validation-dataset",
    "title": "Predictive Models and Time Series",
    "section": "Predictions on the validation dataset",
    "text": "Predictions on the validation dataset\nPrepare the validation data using dummy variables.\n\nY_valid = Amtrak_validation['Ridership (in 000s)']\n\ndummy_valid = pd.get_dummies(Amtrak_validation['Season'], dtype = 'int')\ndummy_valid = dummy_valid.drop([\"Dec\"], axis = 1)\n\nX_qs_valid = pd.concat([Amtrak_validation['t'], Amtrak_validation['t']**2, \n                        dummy_valid], axis = 1)\n\n\nNow, we compute the validation \\(\\text{MSE}_v\\).\n\nY_pred_valid = SeasonmodelAmtrak.predict(X_qs_valid)\n\nmse_season = mean_squared_error(Y_valid, Y_pred_valid) \nprint(round(mse_season, 2))\n\n4890.5"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#disadvantages-of-linear-regression-models",
    "href": "Module3/PredictiveModels.slides.html#disadvantages-of-linear-regression-models",
    "title": "Predictive Models and Time Series",
    "section": "Disadvantages of linear regression models",
    "text": "Disadvantages of linear regression models\n\nDespite their simplicity and versatility, linear regression models are not the best for describing a time series.\nThis is because they do not assume a dependency between consecutive values in the time series. That is, they do not use the fact that, for example, \\(Y_1\\) can help us predict \\(Y_2\\), and \\(Y_2\\) can help us predict \\(Y_3\\), etc.\nModels that help us use past observations to predict future values of the response variable \\(Y\\) are autoregressive models."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#yogi-berra",
    "href": "Module3/PredictiveModels.slides.html#yogi-berra",
    "title": "Predictive Models and Time Series",
    "section": "Yogi Berra",
    "text": "Yogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "Module4/Clustering.slides.html#agenda",
    "href": "Module4/Clustering.slides.html#agenda",
    "title": "Clustering Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nUnsupervised Learning\nClustering Methods\nK-Means Method\nHierarchical Clustering"
  },
  {
    "objectID": "Module4/Clustering.slides.html#load-the-libraries",
    "href": "Module4/Clustering.slides.html#load-the-libraries",
    "title": "Clustering Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn, and scipy libraries in Python."
  },
  {
    "objectID": "Module4/Clustering.slides.html#types-of-learning",
    "href": "Module4/Clustering.slides.html#types-of-learning",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/Clustering.slides.html#types-of-learning-1",
    "href": "Module4/Clustering.slides.html#types-of-learning-1",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-1",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nGoal: organize or group data to gain insights. It answers questions like these\n\nIs there an informative way to visualize the data?\nCan we discover subgroups among variables or observations?\n\n\nUnsupervised learning is more challenging than supervised learning because it is subjective and there is no simple objective for the analysis, such as predicting a response.\n\n\nIt is also known as exploratory data analysis."
  },
  {
    "objectID": "Module4/Clustering.slides.html#examples-of-unsupervised-learning",
    "href": "Module4/Clustering.slides.html#examples-of-unsupervised-learning",
    "title": "Clustering Methods",
    "section": "Examples of Unsupervised Learning",
    "text": "Examples of Unsupervised Learning\n\n\nMarketing. Identify a segment of customers with a high tendency to purchase a specific product.\nRetail. Group customers based on their preferences, style, clothing choices, and store preferences.\nMedical Science. Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.\nSociology. Classify people based on their demographics, lifestyle, socioeconomic status, etc."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-methods",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-methods",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-methods-1",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-methods-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.slides.html#clustering-methods-1",
    "href": "Module4/Clustering.slides.html#clustering-methods-1",
    "title": "Clustering Methods",
    "section": "Clustering methods",
    "text": "Clustering methods\nThey group data in different ways to discover groups with common traits."
  },
  {
    "objectID": "Module4/Clustering.slides.html#clustering-methods-2",
    "href": "Module4/Clustering.slides.html#clustering-methods-2",
    "title": "Clustering Methods",
    "section": "Clustering methods",
    "text": "Clustering methods\n\nTwo classic clustering methods are:\n\nK-Means Method. We seek to divide the observations into K groups.\nHierarchical Clustering. We divide the n observations into 1 group, 2 groups, 3 groups, …, up to n groups. We visualize the divisions using a graph called a dendrogram."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-1",
    "href": "Module4/Clustering.slides.html#example-1",
    "title": "Clustering Methods",
    "section": "Example 1",
    "text": "Example 1\nThe “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:\n\n\n\n\nBill length in millimeters.\nBill depth in millimeters.\nFlipper length in millimeters.\nBody mass in grams."
  },
  {
    "objectID": "Module4/Clustering.slides.html#data",
    "href": "Module4/Clustering.slides.html#data",
    "title": "Clustering Methods",
    "section": "Data",
    "text": "Data\n\npenguins_data = pd.read_excel(\"penguins.xlsx\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "Module4/Clustering.slides.html#data-visualization",
    "href": "Module4/Clustering.slides.html#data-visualization",
    "title": "Clustering Methods",
    "section": "Data visualization",
    "text": "Data visualization\nCan we group penguins based on their characteristics?\n\n\nCode\nplt.figure(figsize=(8, 5)) # Set figure size.\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\") # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.slides.html#the-k-means-method",
    "href": "Module4/Clustering.slides.html#the-k-means-method",
    "title": "Clustering Methods",
    "section": "The K-Means method",
    "text": "The K-Means method\n\nGoal: Find K groups of observations such that each observation is in a different group."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section",
    "href": "Module4/Clustering.slides.html#section",
    "title": "Clustering Methods",
    "section": "",
    "text": "For this, the method requires two elements:\n\n\nA measure of “closeness” between observations.\nAn algorithm that groups observations that are close to each other.\n\n\n\nGood clustering is one in which observations within a group are close together and observations in different groups are far apart."
  },
  {
    "objectID": "Module4/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "href": "Module4/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "title": "Clustering Methods",
    "section": "How do we measure the distance between observations?",
    "text": "How do we measure the distance between observations?\nFor quantitative predictors, we use the Euclidean distance.\nFor example, if we have two predictors \\(X_1\\) and \\(X_2\\) with observations given in the table:\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#euclidean-distance",
    "href": "Module4/Clustering.slides.html#euclidean-distance",
    "title": "Clustering Methods",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\n\n\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }\\]"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-1",
    "href": "Module4/Clustering.slides.html#section-1",
    "title": "Clustering Methods",
    "section": "",
    "text": "We can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, with 3 predictors we have\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\\(X_{1,3}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\\(X_{2,3}\\)\n\n\n\n\nWhere the Euclidean distance is\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }\\]"
  },
  {
    "objectID": "Module4/Clustering.slides.html#problem-with-euclidean-distance",
    "href": "Module4/Clustering.slides.html#problem-with-euclidean-distance",
    "title": "Clustering Methods",
    "section": "Problem with Euclidean distance",
    "text": "Problem with Euclidean distance\n\n\nThe Euclidean distance depends on the units of measurement of the predictors!\nPredictors with certain units have greater importance in calculating the distance.\nThis is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.\nThe solution is to standardize the units of the predictors."
  },
  {
    "objectID": "Module4/Clustering.slides.html#k-means-algorithm",
    "href": "Module4/Clustering.slides.html#k-means-algorithm",
    "title": "Clustering Methods",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\n\n\n\nChoose a value for K, the number of groups.\n\nRandomly assign observations to one of the K groups.\nFind the centroids (average points) of each group.\nReassign observations to the group with the closest centroid.\nRepeat steps 3 and 4 until there are no more changes."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-1-cont.",
    "href": "Module4/Clustering.slides.html#example-1-cont.",
    "title": "Clustering Methods",
    "section": "Example 1 (cont.)",
    "text": "Example 1 (cont.)\nLet’s apply the algorithm to the predictors bill_depth_mm and bill_length_mm of the penguins dataset.\n\n\nCode\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm'])\nX_penguins.head()\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\n\n\n\n\n0\n18.7\n39.1\n\n\n1\n17.4\n39.5\n\n\n2\n18.0\n40.3\n\n\n3\n19.3\n36.7\n\n\n4\n20.6\n39.3"
  },
  {
    "objectID": "Module4/Clustering.slides.html#standarization",
    "href": "Module4/Clustering.slides.html#standarization",
    "title": "Clustering Methods",
    "section": "Standarization",
    "text": "Standarization\nSince the K-means algorithm works with Euclidean distance, we must standardize the predictors before we start. In this way, all of them will be equally informative in the process.\n\n\nCode\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-2",
    "href": "Module4/Clustering.slides.html#section-2",
    "title": "Clustering Methods",
    "section": "",
    "text": "In Python, we use the KMeans() function of sklearn to apply K-means clustering. KMeans() tells Python we want to train a K-means clustering algorithm and .fit_predict() actually trains it using the data.\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\nThe argument n_clusters sets the desired number of clusters and random_state allows us to reproduce the analysis."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-3",
    "href": "Module4/Clustering.slides.html#section-3",
    "title": "Clustering Methods",
    "section": "",
    "text": "The clusters created are contained in the clusters object.\n\n\nCode\nclusters\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n       0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2], dtype=int32)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-4",
    "href": "Module4/Clustering.slides.html#section-4",
    "title": "Clustering Methods",
    "section": "",
    "text": "To visualize the clusters, we augment the original dataset X_penguins (without standarization) with the clusters object. usign the code below.\n\nclustered_penguins = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\nclustered_penguins.head()\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\nCluster\n\n\n\n\n0\n18.7\n39.1\n1\n\n\n1\n17.4\n39.5\n1\n\n\n2\n18.0\n40.3\n1\n\n\n3\n19.3\n36.7\n1\n\n\n4\n20.6\n39.3\n1"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-5",
    "href": "Module4/Clustering.slides.html#section-5",
    "title": "Clustering Methods",
    "section": "",
    "text": "Code\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data = clustered_penguins, x = 'bill_length_mm', y = 'bill_depth_mm', \n                hue = 'Cluster', palette = 'Set1')\nplt.title('K-means Clustering of Penguins')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "href": "Module4/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "title": "Clustering Methods",
    "section": "The truth: 3 groups of penguins",
    "text": "The truth: 3 groups of penguins\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\",\n                hue=\"species\", palette = 'Set1') # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.slides.html#lets-try-using-more-predictors",
    "href": "Module4/Clustering.slides.html#lets-try-using-more-predictors",
    "title": "Clustering Methods",
    "section": "Let’s try using more predictors",
    "text": "Let’s try using more predictors\n\n\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', \n                          'flipper_length_mm', 'body_mass_g'])\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\n# Save new clusters into the original data\nclustered_X = (X_penguins\n              .assign(Cluster = clusters)\n              )"
  },
  {
    "objectID": "Module4/Clustering.slides.html#these-are-the-three-species",
    "href": "Module4/Clustering.slides.html#these-are-the-three-species",
    "title": "Clustering Methods",
    "section": "These are the three species",
    "text": "These are the three species\n\n\n\nAdelie\n\n\nGentoo\n\n\nChinstrap"
  },
  {
    "objectID": "Module4/Clustering.slides.html#determining-the-number-of-clusters",
    "href": "Module4/Clustering.slides.html#determining-the-number-of-clusters",
    "title": "Clustering Methods",
    "section": "Determining the number of clusters",
    "text": "Determining the number of clusters\n\nA simple way to determine the number of clusters is recording the quality of clustering for different numbers of clusters.\nIn sklearn, we can record the inertia of a partition into clusters. Technically, the inertia is the sum of squared distances of observations to their closest cluster center.\nThe lower the intertia the better because this means that all observations are close to their cluster centers overall."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-8",
    "href": "Module4/Clustering.slides.html#section-8",
    "title": "Clustering Methods",
    "section": "",
    "text": "To record the intertias for different numbers of clusters, we use the code below.\n\ninertias = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(Xs_penguins)\n    inertias.append(kmeans.inertia_)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-9",
    "href": "Module4/Clustering.slides.html#section-9",
    "title": "Clustering Methods",
    "section": "",
    "text": "Next, we plot the intertias and look for the elbow in the plot.\nThe elbow represents a number of clusters for which there is no significant improvement in the quality of the clustering.\nIn this case, the number of clusters recommended by this elbow method is 3.\n\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#comments",
    "href": "Module4/Clustering.slides.html#comments",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\nSelecting the number of clusters K is more of an art than a science. You’d better get K right, or you’ll be detecting patterns where none really exist.\nWe need to standardize all predictors.\nThe performance of K-means clustering is affected by the presence of outliers.\nThe algorithm’s solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported."
  },
  {
    "objectID": "Module4/Clustering.slides.html#hierarchical-clustering-1",
    "href": "Module4/Clustering.slides.html#hierarchical-clustering-1",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\n\n\n\n\n\nStart with each observation standing alone in its own group.\nThen, gradually merge the groups that are close together.\nContinue this process until all the observations are in one large group.\nFinally, step back and see which grouping works best."
  },
  {
    "objectID": "Module4/Clustering.slides.html#essential-elements",
    "href": "Module4/Clustering.slides.html#essential-elements",
    "title": "Clustering Methods",
    "section": "Essential elements",
    "text": "Essential elements\n\n\n\nDistance between two observations.\n\nWe use Euclidean distance.\nWe must standardize the predictors!\n\nDistance between two groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#distance-between-two-groups",
    "href": "Module4/Clustering.slides.html#distance-between-two-groups",
    "title": "Clustering Methods",
    "section": "Distance between two groups",
    "text": "Distance between two groups\n\n\n\n\nThe distance between two groups of observations is called linkage.\nThere are several types of linking. The most commonly used are:\n\nComplete linkage\nAverage linkage"
  },
  {
    "objectID": "Module4/Clustering.slides.html#complete-linkage",
    "href": "Module4/Clustering.slides.html#complete-linkage",
    "title": "Clustering Methods",
    "section": "Complete linkage",
    "text": "Complete linkage\nThe distance between groups is measured using the largest distance between observations."
  },
  {
    "objectID": "Module4/Clustering.slides.html#average-linkage",
    "href": "Module4/Clustering.slides.html#average-linkage",
    "title": "Clustering Methods",
    "section": "Average linkage",
    "text": "Average linkage\nThe distance between groups is the average of all the distances between observations."
  },
  {
    "objectID": "Module4/Clustering.slides.html#hierarchical-clustering-algorithm",
    "href": "Module4/Clustering.slides.html#hierarchical-clustering-algorithm",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering algorithm",
    "text": "Hierarchical clustering algorithm\n\nThe steps of the algorithm are as follows:\n\n\nAssign each observation to a cluster.\nMeasure the linkage between all clusters.\nMerge the two most similar clusters.\nThen, merge the next two most similar clusters.\nContinue until all clusters have been merged."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-2",
    "href": "Module4/Clustering.slides.html#example-2",
    "title": "Clustering Methods",
    "section": "Example 2",
    "text": "Example 2\n\nLet’s consider a dataset called “Cereals.xlsx.” The data includes nutritional information for 77 cereals, among other data.\n\ncereal_data = pd.read_excel(\"cereals.xlsx\")"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-10",
    "href": "Module4/Clustering.slides.html#section-10",
    "title": "Clustering Methods",
    "section": "",
    "text": "Here, we will restrict to 7 numeric predictors.\n\nX_cereal = cereal_data.filter(['calories', 'protein', 'fat', 'sodium', 'fiber',\n                              'carbo', 'sugars', 'potass', 'vitamins'])\nX_cereal.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25"
  },
  {
    "objectID": "Module4/Clustering.slides.html#do-not-forget-to-standardize",
    "href": "Module4/Clustering.slides.html#do-not-forget-to-standardize",
    "title": "Clustering Methods",
    "section": "Do not forget to standardize",
    "text": "Do not forget to standardize\n\nSince the hierarchical clustering algorithm also works with distances, we must standardize the predictors to have an accurate analysis.\n\nscaler = StandardScaler()\nXs_cereal = scaler.fit_transform(X_cereal)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-11",
    "href": "Module4/Clustering.slides.html#section-11",
    "title": "Clustering Methods",
    "section": "",
    "text": "Unfortunately, the Agglomerative() function in sklearn is not as user friendly compared to other available functions in Python. In particular, the scipy library has a function called linkage() for hierarchical clustering that works as follows.\n\nClust_Cereal = linkage(Xs_cereal, method = 'complete')\n\nThe argument method sets the type of linkage to be used."
  },
  {
    "objectID": "Module4/Clustering.slides.html#results-dendrogram",
    "href": "Module4/Clustering.slides.html#results-dendrogram",
    "title": "Clustering Methods",
    "section": "Results: Dendrogram",
    "text": "Results: Dendrogram\n\n\n\n\n\n\nA dendrogram is a tree diagram that summarizes and visualizes the clustering process.\nObservations are on the horizontal axis and at the bottom of the diagram.\nThe vertical axis shows the distance between groups.\nIt is read from top to bottom."
  },
  {
    "objectID": "Module4/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "href": "Module4/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "title": "Clustering Methods",
    "section": "What to do with a dendrogram?",
    "text": "What to do with a dendrogram?\n\n\n\n\nWe draw a horizontal line at a specific height to define the groups.\nThis line defines three groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-12",
    "href": "Module4/Clustering.slides.html#section-12",
    "title": "Clustering Methods",
    "section": "",
    "text": "This line defines 5 groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#dendrogram-in-python",
    "href": "Module4/Clustering.slides.html#dendrogram-in-python",
    "title": "Clustering Methods",
    "section": "Dendrogram in Python",
    "text": "Dendrogram in Python\nTo produce a nice dendrogram in Python, we use the function dendrogram from scipy.\n\n\nCode\nplt.figure(figsize=(8, 4))\ndendrogram(Clust_Cereal, color_threshold=None)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#comments-1",
    "href": "Module4/Clustering.slides.html#comments-1",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\n\nRemember to standardize the predictors!\nIt’s not easy to choose the correct number of clusters using the dendrogram.\nThe results depend on the linkage measure used.\n\nComplete linkage results in narrower clusters.\nAverage linkage strikes a balance between narrow and thinner clusters.\n\nHierarchical clustering is useful for detecting outliers."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-13",
    "href": "Module4/Clustering.slides.html#section-13",
    "title": "Clustering Methods",
    "section": "",
    "text": "With these methods, there is no single correct answer; any solution that exposes some interesting aspect of the data should be considered.\n\nJames et al. (2017)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#agenda",
    "href": "Tools/Tools2.slides.html#agenda",
    "title": "Data Wrangling and Visualization",
    "section": "Agenda",
    "text": "Agenda\n\n\nBasic Data Wrangling\nData Visualization"
  },
  {
    "objectID": "Tools/Tools2.slides.html#data-wrangling",
    "href": "Tools/Tools2.slides.html#data-wrangling",
    "title": "Data Wrangling and Visualization",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n\nData wrangling is the process of transforming raw data into a clean and structured format.\nIt involves merging, reshaping, filtering, and organizing data for analysis.\nHere, we illustrate some special functions of the pandas for cleaning common issues with a dataset."
  },
  {
    "objectID": "Tools/Tools2.slides.html#example-1",
    "href": "Tools/Tools2.slides.html#example-1",
    "title": "Data Wrangling and Visualization",
    "section": "Example 1",
    "text": "Example 1\nConsider an industrial engineer who receives a messy Excel file from a manufacturing client. The data file is called “industrial_dataset.xlsx”, which file includes data about machine maintenance logs, production output, and operator comments.\nThe goal is to clean and prepare this dataset using pandas so it can be analyzed.\nLet’s load pandas and read the data set into Python.\n\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\nclient_data = pd.read_excel(\"industrial_dataset.xlsx\")"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section",
    "href": "Tools/Tools2.slides.html#section",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "# Preview the dataset.\nclient_data.head()\n\n\n\n\n\n\n\n\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n\n\n3\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n\n\n4\n103\n950\n2023-01-13\nCharlie\nAll good\\n"
  },
  {
    "objectID": "Tools/Tools2.slides.html#remove-duplicate-rows",
    "href": "Tools/Tools2.slides.html#remove-duplicate-rows",
    "title": "Data Wrangling and Visualization",
    "section": "Remove duplicate rows",
    "text": "Remove duplicate rows\nDuplicate rows have the same entries in every column in the dataset. If only one row is needed for the analysis, we can remove the duplicates using .drop_duplicates(). For example, we can see the unique names of the operators.\n\noperators = complete_data['Operator'].str.title()\n\n(operators\n.drop_duplicates()\n)\n\n0         Ana\n2         Bob\n3     Charlie\n5     Unknown\n6        Dave\n14        Eve\nName: Operator, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-1",
    "href": "Tools/Tools2.slides.html#section-1",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "The new column is appended to the end of the dataframe.\n\nclient_data_single.head()\n\n\n\n\n\n\n\n\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\nID\n\n\n\n\n0\n101\n1200\n2023-01-10\nAna\nok\n1\n\n\n1\n101\n1200\n2023-01-10\nAna\nok\n2\n\n\n2\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n3\n\n\n3\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n4\n\n\n4\n103\n950\n2023-01-13\nCharlie\nAll good\\n\n5"
  },
  {
    "objectID": "Tools/Tools2.slides.html#add-an-index-column",
    "href": "Tools/Tools2.slides.html#add-an-index-column",
    "title": "Data Wrangling and Visualization",
    "section": "Add an index column",
    "text": "Add an index column\n\nIn some cases, it is useful to have a unique identifier for each row in the dataset. We can create an identifier using the function .assign with some extra syntax.\n\nclient_data_single = (client_data\n                      .assign(ID = lambda x: x.index + 1)\n                      )"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-2",
    "href": "Tools/Tools2.slides.html#section-2",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "To bring it to the begining of the array, we can use the .filter() function.\n\nclient_data_single = (client_data_single\n.filter(['ID', 'Machine ID',    'Output (units)',   \n        'Maintenance Date', 'Operator', 'Comment'])\n)\nclient_data_single.head(3)  \n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-3",
    "href": "Tools/Tools2.slides.html#section-3",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "complete_data.head()\n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!\n\n\n3\n4\n103\nerror\n2023-01-13\nCharlie\nAll good\\n\n\n\n4\n5\n103\n950\n2023-01-13\nCharlie\nAll good\\n"
  },
  {
    "objectID": "Tools/Tools2.slides.html#fill-blank-cells",
    "href": "Tools/Tools2.slides.html#fill-blank-cells",
    "title": "Data Wrangling and Visualization",
    "section": "Fill blank cells",
    "text": "Fill blank cells\nIn the dataset, there are columns with missing values. If we would like to fill them with specific values or text, we use the .fillna() function. In this function, we use the syntaxis 'Variable': 'Replace', where the Variable is the column in the dataset and Replace is the text or number to fill the entry in.\nLet’s fill in the missing entries of the columns Operator, Maintenance Date, and Comment.\n\ncomplete_data = (client_data_single\n                .fillna({'Operator': 'Unknown', \n                'Maintenance Date': '2023-01-01',\n                'Comment': 'None'})\n                )"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-4",
    "href": "Tools/Tools2.slides.html#section-4",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "We can replace the “error” in this column by a user-specified value, say, 0. To this end, we use the function .replace(). The function has two inputs. The first one is the value to replace and the second one is the replacement value.\n\ncomplete_data['Output (units)'] = complete_data['Output (units)'].replace('error', 0)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#replace-values",
    "href": "Tools/Tools2.slides.html#replace-values",
    "title": "Data Wrangling and Visualization",
    "section": "Replace values",
    "text": "Replace values\n\nThere are some cases in which columns have some undesired or unwatned values. Consider the Output (units) as an example.\n\ncomplete_data['Output (units)'].head()\n\n0     1200\n1     1200\n2     1050\n3    error\n4      950\nName: Output (units), dtype: object\n\n\nThe column has the numbers of units but also text such as “error”."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-5",
    "href": "Tools/Tools2.slides.html#section-5",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Let’s check the new column.\n\ncomplete_data['Output (units)']\n\n0     1200\n1     1200\n2     1050\n3        0\n4      950\n      ... \n95     800\n96    1100\n97     950\n98     950\n99    1100\nName: Output (units), Length: 100, dtype: int64\n\n\nNote that the new column is now numeric."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-6",
    "href": "Tools/Tools2.slides.html#section-6",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "The column has some values such as “Requires part: valve” and “Delay: maintenance” that we may want to split into columns.\n\n\n0                       ok\n1                       ok\n2               Needs oil!\n3               All good\\n\n4               All good\\n\n              ...         \n95    Requires part: valve\n96                      ok\n97    Delay: maintenance\\n\n98              Needs oil!\n99              All good\\n\nName: Comment, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#split-column-into-multiple-ones",
    "href": "Tools/Tools2.slides.html#split-column-into-multiple-ones",
    "title": "Data Wrangling and Visualization",
    "section": "Split column into multiple ones",
    "text": "Split column into multiple ones\nThere are some cases in which we want to split a column according to a character. For example, consider the column Comment from the dataset.\n\ncomplete_data['Comment']\n\n0                       ok\n1                       ok\n2               Needs oil!\n3               All good\\n\n4               All good\\n\n              ...         \n95    Requires part: valve\n96                      ok\n97    Delay: maintenance\\n\n98              Needs oil!\n99              All good\\n\nName: Comment, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-7",
    "href": "Tools/Tools2.slides.html#section-7",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "We can split the values in the column according to the colon “:”.\nThat is, everything before the colon will be in a column. Everything after the colon will be in another column. To achieve this, we use the function str.split().\nOne input of the function is the symbol or character for which we cant to make a split. The other input, expand = True tells Python that we want to create new columns.\n\ncomplete_data['Comment'].str.split(':', expand = True)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-8",
    "href": "Tools/Tools2.slides.html#section-8",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "The result is two columns.\n\nsplit_column = complete_data['Comment'].str.split(':', expand = True)\nsplit_column.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nok\nNone\n\n\n1\nok\nNone\n\n\n2\nNeeds oil!\nNone\n\n\n3\nAll good\\n\nNone\n\n\n4\nAll good\\n\nNone"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-9",
    "href": "Tools/Tools2.slides.html#section-9",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "We can assign them to new columns in the dataset using the following code.\n\naugmented_data = (complete_data\n                  .assign(First_comment = split_column.filter([0]),\n                  Second_comment = split_column.filter([1]))\n                  )"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-10",
    "href": "Tools/Tools2.slides.html#section-10",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "augmented_data.head()\n\n\n\n\n\n\n\n\nID\nMachine ID\nOutput (units)\nMaintenance Date\nOperator\nComment\nFirst_comment\nSecond_comment\n\n\n\n\n0\n1\n101\n1200\n2023-01-10\nAna\nok\nok\nNone\n\n\n1\n2\n101\n1200\n2023-01-10\nAna\nok\nok\nNone\n\n\n2\n3\n102\n1050\n2023-01-12\nBob\nNeeds oil!\nNeeds oil!\nNone\n\n\n3\n4\n103\n0\n2023-01-13\nCharlie\nAll good\\n\nAll good\\n\nNone\n\n\n4\n5\n103\n950\n2023-01-13\nCharlie\nAll good\\n\nAll good\\n\nNone"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-11",
    "href": "Tools/Tools2.slides.html#section-11",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Let’s see the cleaned column.\n\naugmented_data['First_comment']\n\n0                ok\n1                ok\n2        Needs oil!\n3          All good\n4          All good\n          ...      \n95    Requires part\n96               ok\n97            Delay\n98       Needs oil!\n99         All good\nName: First_comment, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#remove-characters",
    "href": "Tools/Tools2.slides.html#remove-characters",
    "title": "Data Wrangling and Visualization",
    "section": "Remove characters",
    "text": "Remove characters\n\nSomething that we notice is that the column First_Comment has some extra characters like “” that may be useless when working with the data.\nWe can remove them using the function str.strip(). The input of the function is the character to remove.\n\naugmented_data['First_comment'] = augmented_data['First_comment'].str.strip(\"\\n\")"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-12",
    "href": "Tools/Tools2.slides.html#section-12",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "We can also remove other characters.\n\naugmented_data['First_comment'].str.strip(\"!\")\n\n0                ok\n1                ok\n2         Needs oil\n3          All good\n4          All good\n          ...      \n95    Requires part\n96               ok\n97            Delay\n98        Needs oil\n99         All good\nName: First_comment, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-13",
    "href": "Tools/Tools2.slides.html#section-13",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "The dataset is in the file “glass.xlsx”. Let’s load it using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nglass_data = pd.read_excel(\"glass.xlsx\")\n\n\nThe variable Type is categorical. So, let’s ensure Python knows this using the code below.\n\nglass_data['Type'] = pd.Categorical(glass_data['Type'])"
  },
  {
    "objectID": "Tools/Tools2.slides.html#transform-text-case",
    "href": "Tools/Tools2.slides.html#transform-text-case",
    "title": "Data Wrangling and Visualization",
    "section": "Transform text case",
    "text": "Transform text case\nWhen working with text columns such as those containing names, it might be possible to have different ways of writing. A common case is when having lower case or upper case names or a combination thereof.\nFor example, consider the column Operator containing the names of the operators.\n\ncomplete_data['Operator'].head()\n\n0       Ana \n1        Ana\n2        Bob\n3    Charlie\n4    Charlie\nName: Operator, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#remove-extra-spaces",
    "href": "Tools/Tools2.slides.html#remove-extra-spaces",
    "title": "Data Wrangling and Visualization",
    "section": "Remove extra spaces",
    "text": "Remove extra spaces\n\nTo deal with names, we first use the .str.strip() to remove leading and trailing characters from strings.\n\ncomplete_data['Operator'] = complete_data['Operator'].str.strip()\ncomplete_data['Operator']\n\n0         Ana\n1         Ana\n2         Bob\n3     Charlie\n4     Charlie\n       ...   \n95    Charlie\n96        Ana\n97        Ana\n98    Charlie\n99        ana\nName: Operator, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#change-to-lowercase-letters",
    "href": "Tools/Tools2.slides.html#change-to-lowercase-letters",
    "title": "Data Wrangling and Visualization",
    "section": "Change to lowercase letters",
    "text": "Change to lowercase letters\n\nWe can turn all names to lowercase using the function str.lower().\n\ncomplete_data['Operator'].str.lower()\n\n0         ana\n1         ana\n2         bob\n3     charlie\n4     charlie\n       ...   \n95    charlie\n96        ana\n97        ana\n98    charlie\n99        ana\nName: Operator, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#change-to-uppercase-letters",
    "href": "Tools/Tools2.slides.html#change-to-uppercase-letters",
    "title": "Data Wrangling and Visualization",
    "section": "Change to uppercase letters",
    "text": "Change to uppercase letters\n\nWe can turn all names to lowercase using the function str.upper().\n\ncomplete_data['Operator'].str.upper()\n\n0         ANA\n1         ANA\n2         BOB\n3     CHARLIE\n4     CHARLIE\n       ...   \n95    CHARLIE\n96        ANA\n97        ANA\n98    CHARLIE\n99        ANA\nName: Operator, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#capitalize-the-first-letter",
    "href": "Tools/Tools2.slides.html#capitalize-the-first-letter",
    "title": "Data Wrangling and Visualization",
    "section": "Capitalize the first letter",
    "text": "Capitalize the first letter\n\nWe can convert all names to title case using the function str.title().\n\ncomplete_data['Operator'].str.title()\n\n0         Ana\n1         Ana\n2         Bob\n3     Charlie\n4     Charlie\n       ...   \n95    Charlie\n96        Ana\n97        Ana\n98    Charlie\n99        Ana\nName: Operator, Length: 100, dtype: object"
  },
  {
    "objectID": "Tools/Tools2.slides.html#example-2",
    "href": "Tools/Tools2.slides.html#example-2",
    "title": "Data Wrangling and Visualization",
    "section": "Example 2",
    "text": "Example 2\n\nA criminologist is developing a rule-based system to classify the types of glasses encountered in criminal investigations.\nThe data consist of 214 glass samples labeled as one of seven class categories.\nThere are nine predictors, including refractive index and percentages of eight elements: Na, Mg, AL, Is, K, Ca, Ba, and Fe. The response is the type of glass."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-14",
    "href": "Tools/Tools2.slides.html#section-14",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "We create the bar chart using the function countplot() from seaborn.\n\n\nCode\n# Create plot.\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.countplot(data = glass_data, x = 'Type') # Show the plot.\nplt.title(\"Bar chart of Type of Glasses\") # Set plot title.\nplt.ylabel(\"Frequency\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#matplotlib-library",
    "href": "Tools/Tools2.slides.html#matplotlib-library",
    "title": "Data Wrangling and Visualization",
    "section": "matplotlib library",
    "text": "matplotlib library\n\nmatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\nIt is widely used in the data science community for plotting data in various formats.\nIdeal for creating simple visualizations like line plots, bar charts, scatter plots, and more.\nhttps://matplotlib.org/"
  },
  {
    "objectID": "Tools/Tools2.slides.html#seaborn-library",
    "href": "Tools/Tools2.slides.html#seaborn-library",
    "title": "Data Wrangling and Visualization",
    "section": "seaborn library",
    "text": "seaborn library\n\nseaborn is a Python library built on top of Matplotlib.\nDesigned to make statistical data visualization easy and beautiful.\nIdeal for creating informative and attractive visualizations with minimal code.\nhttps://seaborn.pydata.org/index.html"
  },
  {
    "objectID": "Tools/Tools2.slides.html#importing-the-libraries",
    "href": "Tools/Tools2.slides.html#importing-the-libraries",
    "title": "Data Wrangling and Visualization",
    "section": "Importing the libraries",
    "text": "Importing the libraries\n\nThe matplotlib and seaborn libraries are pre-installed in Google Colab. However, we need to inform Google Colab that we want to use them and its functions using the following command:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSimilar to pandas, the command as sns allows us to have a short name for seaborn. Similarly, we rename matplotlib as plt."
  },
  {
    "objectID": "Tools/Tools2.slides.html#histogram",
    "href": "Tools/Tools2.slides.html#histogram",
    "title": "Data Wrangling and Visualization",
    "section": "Histogram",
    "text": "Histogram\n\nGraphical display that gives an idea of the “shape” of the sample, indicating regions where sample points are concentrated and regions where they are sparse.\n\nThe bars of the histogram touch each other. A space indicates that there are no observations in that interval."
  },
  {
    "objectID": "Tools/Tools2.slides.html#histogram-of-na",
    "href": "Tools/Tools2.slides.html#histogram-of-na",
    "title": "Data Wrangling and Visualization",
    "section": "Histogram of Na",
    "text": "Histogram of Na\nTo create a histogram, we use the function histplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = glass_data, x = 'Na') # Create the histogram.\nplt.title(\"Histogram of Na\") # Plot title.\nplt.xlabel(\"Na\") # X label\nplt.show() # Display the plot"
  },
  {
    "objectID": "Tools/Tools2.slides.html#box-plot",
    "href": "Tools/Tools2.slides.html#box-plot",
    "title": "Data Wrangling and Visualization",
    "section": "Box plot",
    "text": "Box plot\n\nA box plot is a graphic that presents the median, the first and third quartiles, and any “outliers” present in the sample.\n\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile (\\(Q_3 - Q_1\\)). This is the distance needed to span the middle half of the data."
  },
  {
    "objectID": "Tools/Tools2.slides.html#anatomy-of-a-box-plot",
    "href": "Tools/Tools2.slides.html#anatomy-of-a-box-plot",
    "title": "Data Wrangling and Visualization",
    "section": "Anatomy of a box plot",
    "text": "Anatomy of a box plot\n\nSee also https://towardsdatascience.com/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097"
  },
  {
    "objectID": "Tools/Tools2.slides.html#box-plot-of-na",
    "href": "Tools/Tools2.slides.html#box-plot-of-na",
    "title": "Data Wrangling and Visualization",
    "section": "Box plot of Na",
    "text": "Box plot of Na\nTo create a boxplot, we use the function boxplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the figure.\nsns.boxplot(data = glass_data, y = 'Na') # Create boxplot.\nplt.title(\"Box plot of Na\") # Add title.\nplt.show() # Show the plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#outliers",
    "href": "Tools/Tools2.slides.html#outliers",
    "title": "Data Wrangling and Visualization",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers are points that are much larger or smaller than the rest of the sample points.\nOutliers may be data entry errors or they may be points that really are different from the rest.\nOutliers should not be deleted without considerable thought—sometimes calculations and analyses will be done with and without outliers and then compared."
  },
  {
    "objectID": "Tools/Tools2.slides.html#scatter-plot",
    "href": "Tools/Tools2.slides.html#scatter-plot",
    "title": "Data Wrangling and Visualization",
    "section": "Scatter plot",
    "text": "Scatter plot\n\nData for which items consists of a pair of numeric values is called bivariate. The graphical summary for bivariate data is a scatterplot.\nThe variables \\(X\\) and \\(Y\\) are placed on the horizontal and vertical axes, respectively. Each point on the graph marks the position of a pair of values of \\(X\\) and \\(Y\\).\nA scatterplot allows us to explore lineal and nonlinear relationships between two variables."
  },
  {
    "objectID": "Tools/Tools2.slides.html#scatter-plot-of-na-versus-ri",
    "href": "Tools/Tools2.slides.html#scatter-plot-of-na-versus-ri",
    "title": "Data Wrangling and Visualization",
    "section": "Scatter plot of Na versus RI",
    "text": "Scatter plot of Na versus RI\nTo create a scatter plot, we use the function scatter() from seabron. In this function, you must state the\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.scatterplot(data = glass_data, x = 'Na', y = 'RI') # Show the plot.\nplt.title(\"Scatter plot of Na vs RI\") # Set plot title.\nplt.xlabel(\"Na\") # Set label for X axis.\nplt.ylabel(\"RI\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#bar-charts",
    "href": "Tools/Tools2.slides.html#bar-charts",
    "title": "Data Wrangling and Visualization",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts are commonly used to describe qualitative data classified into various categories based on sector, region, different time periods, or other such factors.\nDifferent sectors, different regions, or different time periods are then labeled as specific categories.\nA bar chart is constructed by creating categories that are represented by labeling each category and which are represented by intervals of equal length on a horizontal axis.\nThe count or frequency within the corresponding category is represented by a bar of height proportional to the frequency."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-15",
    "href": "Tools/Tools2.slides.html#section-15",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "plt.figure(figsize=(5, 5))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Relative Frequency of Each Category', fontsize = 12)\nplt.ylabel('Relative Frequency', fontsize = 12)\nplt.xlabel('Category', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#saving-plots",
    "href": "Tools/Tools2.slides.html#saving-plots",
    "title": "Data Wrangling and Visualization",
    "section": "Saving plots",
    "text": "Saving plots\n\nWe save a figure using the save.fig function from matplotlib. The dpi argument of this function sets the resolution of the image. The higher the dpi, the better the resolution.\n\nplt.figure(figsize=(5, 7))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Frequency of Each Category')\nplt.ylabel('Frequency')\nplt.xlabel('Category')\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#improving-the-figure",
    "href": "Tools/Tools2.slides.html#improving-the-figure",
    "title": "Data Wrangling and Visualization",
    "section": "Improving the figure",
    "text": "Improving the figure\n\nWe can also use other functions to improve the aspect of the figure:\n\nplt.title(fontsize): Font size of the title.\nplt.ylabel(fontsize): Font size of y axis title.\nplt.xlabel(fontsize): Font size of x axis title.\nplt.yticks(fontsize): Font size of the y axis labels.\nplt.xticks(fontsize): Font size of the x axis labels."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-16",
    "href": "Tools/Tools2.slides.html#section-16",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "plt.figure(figsize=(5, 5))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Relative Frequency of Each Category', fontsize = 12)\nplt.ylabel('Relative Frequency', fontsize = 12)\nplt.xlabel('Category', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#agenda",
    "href": "Tools/Tools1.slides.html#agenda",
    "title": "Introduction to Python and Pandas",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Python\nReading data with Pandas"
  },
  {
    "objectID": "Tools/Tools1.slides.html#python",
    "href": "Tools/Tools1.slides.html#python",
    "title": "Introduction to Python and Pandas",
    "section": "Python",
    "text": "Python\n\n\n\n\nA versatile programming language.\nIt is free!\nIt is widely used for data cleaning, data visualization, and data modelling.\nIt can be extended with packages (libraries) developed by other users."
  },
  {
    "objectID": "Tools/Tools1.slides.html#google-colab",
    "href": "Tools/Tools1.slides.html#google-colab",
    "title": "Introduction to Python and Pandas",
    "section": "Google Colab",
    "text": "Google Colab\nGoogle’s free cloud collaboration platform for creating Python documents.\n\nRun Python and collaborate on Jupyter notebooks for free.\nHarness the power of GPUs for free to accelerate your data science projects.\nEasily save and upload your notebooks to Google Drive."
  },
  {
    "objectID": "Tools/Tools1.slides.html#python-libraries",
    "href": "Tools/Tools1.slides.html#python-libraries",
    "title": "Introduction to Python and Pandas",
    "section": "Python libraries",
    "text": "Python libraries\n\nLibraries are the fundamental units of reproducible Python code. They include reusable Python functions, documentation describing how to use them, and sample data.\nIn this course, we will be working mostly with the following libraries:\n\npandas for data manipulation\nmatplotlib and seaborn for data visualization\nstatsmodels and scikit-learn for data modelling"
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-in-python",
    "href": "Tools/Tools1.slides.html#loading-data-in-python",
    "title": "Introduction to Python and Pandas",
    "section": "Loading data in Python",
    "text": "Loading data in Python\nWe assume that data is stored in an Excel file, where the rows are the observations and the columns contain the variables (e.g., predictors or responses). As an example, let’s use the file penguins.xlsx."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section",
    "href": "Tools/Tools1.slides.html#section",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "The dataset penguins.xlsx contains data from penguins living in three islands."
  },
  {
    "objectID": "Tools/Tools1.slides.html#pandas-library",
    "href": "Tools/Tools1.slides.html#pandas-library",
    "title": "Introduction to Python and Pandas",
    "section": "pandas library",
    "text": "pandas library\n\n\n\n\n\n\n\n\n\n\npandas is an open-source Python library for data manipulation and analysis.\nIt is built on top of numpy for high-performance data operations.\nIt allows the user to import, clean, transform, and analyze data efficiently.\nhttps://pandas.pydata.org/"
  },
  {
    "objectID": "Tools/Tools1.slides.html#importing-pandas",
    "href": "Tools/Tools1.slides.html#importing-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Importing pandas",
    "text": "Importing pandas\nFortunately, the pandas library is already pre-installed in Google Colab.\n\nHowever, we need to inform Google Colab that we want to use pandas and its functions using the following command:\n\nimport pandas as pd\n\n\nThe command as pd allows us to have a short name for pandas. To use a function of pandas, we use the command pd.function()."
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "href": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Loading data using pandas",
    "text": "Loading data using pandas\n\nThe following code shows how to read the data in the file “penguins.xlsx” into Python.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")"
  },
  {
    "objectID": "Tools/Tools1.slides.html#the-function-head",
    "href": "Tools/Tools1.slides.html#the-function-head",
    "title": "Introduction to Python and Pandas",
    "section": "The function head()",
    "text": "The function head()\nThe function head() allows you to print the first rows of a pandas data frame.\n\n# Print the first 4 rows of the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "href": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing variables a dataset",
    "text": "Indexing variables a dataset\nWe can select a specific variables of a data frame using the syntaxis below.\n\npenguins_data['bill_length_mm']\n\n0      39.1\n1      39.5\n2      40.3\n3       NaN\n4      36.7\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 344, dtype: float64\n\n\nHere, we selected the variable bill_length_mm in the penguins_data dataset."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-1",
    "href": "Tools/Tools1.slides.html#section-1",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To index multiple variables of a data frame, we put the names of the variables in a list object. For example, we select bill_length_mm, species, and island as follows:\n\nsub_penguins_data = penguins_data[ ['bill_length_mm',  'species', 'island'] ]\nsub_penguins_data.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n0\n39.1\nAdelie\nTorgersen\n\n\n1\n39.5\nAdelie\nTorgersen\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows",
    "href": "Tools/Tools1.slides.html#indexing-rows",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing rows",
    "text": "Indexing rows\nTo index rows in a dataset, we use the argument loc from pandas. For example, we select the rows 3 to 6 of the penguins_dataset dataset:\n\nrows_penguins_data = penguins_data.loc[2:5]\nrows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-2",
    "href": "Tools/Tools1.slides.html#section-2",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "rows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\nNote that the index 2 and 5 refer to observations 3 and 7, respectively, in the dataset. This is because the first index in Python is 0."
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "href": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "title": "Introduction to Python and Pandas",
    "section": "Indexing rows and columns",
    "text": "Indexing rows and columns\nUsing loc, we can also retrieve a subset from the dataset by selecting specific columns and rows.\n\nsub_rows_pdata = penguins_data.loc[2:5, ['bill_length_mm',  'species', 'island'] ]\nsub_rows_pdata\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen\n\n\n5\n39.3\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "href": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "Chaining operations with pandas",
    "text": "Chaining operations with pandas\nOne of the most important techniques in pandas is chaining, which allows for cleaner and more readable data manipulation.\nThe general structure of chaining looks like this:"
  },
  {
    "objectID": "Tools/Tools1.slides.html#key-pandas-methods",
    "href": "Tools/Tools1.slides.html#key-pandas-methods",
    "title": "Introduction to Python and Pandas",
    "section": "Key pandas methods",
    "text": "Key pandas methods\npandas provides methods or functions to solve common data manipulation tasks:\n\n\n.filter() selects specific columns or rows.\n.query() filters observations based on conditions.\n.assign() adds new variables that are functions of existing variables.\n.sort_values() changes the order of rows.\n.agg() reduces multiple values to a single numerical summary."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-3",
    "href": "Tools/Tools1.slides.html#section-3",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To practice, we will use the dataset penguins_data."
  },
  {
    "objectID": "Tools/Tools1.slides.html#example-1",
    "href": "Tools/Tools1.slides.html#example-1",
    "title": "Introduction to Python and Pandas",
    "section": "Example 1",
    "text": "Example 1\nLet’s load the dataset and the pandas library.\n\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Preview the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "href": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "title": "Introduction to Python and Pandas",
    "section": "Selecting columns with .filter()",
    "text": "Selecting columns with .filter()\nSelect the columns species, body_mass_g and sex.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\n3750.0\nmale\n\n\n1\nAdelie\n3800.0\nfemale\n\n\n2\nAdelie\n3250.0\nfemale\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\n3450.0\nfemale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-4",
    "href": "Tools/Tools1.slides.html#section-4",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "The axis argument tells .filter() whether to select rows (0) or columns (1) from the dataframe.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\nThe .head() command allows us to print the first six rows of the newly produced dataframe. We must remove it to have the entire new dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-5",
    "href": "Tools/Tools1.slides.html#section-5",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can also use .filter() to select rows too. To this end, we set axis = 1. We can select specific rows, such as 0 and 10.\n\n(penguins_data\n  .filter([0, 10], axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n10\nAdelie\nTorgersen\n37.8\n17.1\n186.0\n3300.0\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-6",
    "href": "Tools/Tools1.slides.html#section-6",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "Or, we can select a set of rows using the function range(). For example, let’s select the first 5 rows.\n\n(penguins_data\n  .filter(range(5), axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "href": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "title": "Introduction to Python and Pandas",
    "section": "Filtering rows with .query()",
    "text": "Filtering rows with .query()\n\nAn alternative way of selecting rows is .query(). Compared to .filter(), .query() allows us to filter the data using statements or queries involving the variables.\n\nFor example, let’s filter the data for the species “Gentoo.”\n\n(penguins_data\n  .query(\"species == 'Gentoo'\")\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-7",
    "href": "Tools/Tools1.slides.html#section-7",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "(penguins_data\n  .query(\"species == 'Gentoo'\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n152\nGentoo\nBiscoe\n46.1\n13.2\n211.0\n4500.0\nfemale\n2007\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n154\nGentoo\nBiscoe\n48.7\n14.1\n210.0\n4450.0\nfemale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-8",
    "href": "Tools/Tools1.slides.html#section-8",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can also filter the data to get penguins with a body mass greater than 5000g.\n\n(penguins_data\n  .query(\"body_mass_g &gt; 5000\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007\n\n\n159\nGentoo\nBiscoe\n46.7\n15.3\n219.0\n5200.0\nmale\n2007\n\n\n161\nGentoo\nBiscoe\n46.8\n15.4\n215.0\n5150.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-9",
    "href": "Tools/Tools1.slides.html#section-9",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can even combine .filter() and .query(). For example, let’s select the columns species, body_mass_g and sex, then filter the data for the “Gentoo” species.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n  .query(\"species == 'Gentoo'\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n152\nGentoo\n4500.0\nfemale\n\n\n153\nGentoo\n5700.0\nmale\n\n\n154\nGentoo\n4450.0\nfemale\n\n\n155\nGentoo\n5700.0\nmale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "href": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "title": "Introduction to Python and Pandas",
    "section": "Create new columns with .assign()",
    "text": "Create new columns with .assign()\nWith .assign(), we can create new columns (variables) that are functions of existing ones. This function uses a special Python keyword called lambda. Technically, this keyword defines an anonymous function.\nFor example, we create a new variable LDRatio equaling the ratio of bill_length_mm and bill_depth_mm.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-10",
    "href": "Tools/Tools1.slides.html#section-10",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "In this code, the df after lambda indicates that the dataframe (penguins_data) will be referred to as df inside the function. The colon : sets the start of the function.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)\n\nThe code appends the new variable to the end of the resulting dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-11",
    "href": "Tools/Tools1.slides.html#section-11",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "We can see the new variable using .filter().\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"LDRatio\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nLDRatio\n\n\n\n\n0\n39.1\n18.7\n2.090909\n\n\n1\n39.5\n17.4\n2.270115\n\n\n2\n40.3\n18.0\n2.238889\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n1.901554"
  },
  {
    "objectID": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "href": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "title": "Introduction to Python and Pandas",
    "section": "Sorting with .sort_values()",
    "text": "Sorting with .sort_values()\nWe can sort the data based on a column like bill_length_mm.\n\n(penguins_data\n  .sort_values(\"bill_length_mm\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n142\nAdelie\nDream\n32.1\n15.5\n188.0\n3050.0\nfemale\n2009\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n70\nAdelie\nTorgersen\n33.5\n19.0\n190.0\n3600.0\nfemale\n2008\n\n\n92\nAdelie\nDream\n34.0\n17.1\n185.0\n3400.0\nfemale\n2008"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-12",
    "href": "Tools/Tools1.slides.html#section-12",
    "title": "Introduction to Python and Pandas",
    "section": "",
    "text": "To sort in descending order, use ascending=False inside sort_values().\n\n(penguins_data\n  .sort_values(\"bill_length_mm\", ascending=False)\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n185\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nmale\n2007\n\n\n293\nChinstrap\nDream\n58.0\n17.8\n181.0\n3700.0\nfemale\n2007\n\n\n253\nGentoo\nBiscoe\n55.9\n17.0\n228.0\n5600.0\nmale\n2009\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n267\nGentoo\nBiscoe\n55.1\n16.0\n230.0\n5850.0\nmale\n2009"
  },
  {
    "objectID": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "href": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "title": "Introduction to Python and Pandas",
    "section": "Summarizing with .agg()",
    "text": "Summarizing with .agg()\nWe can calculate summary statistics of the columns bill_length_mm, bill_depth_mm, and body_mass_g.\n\n(penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386\n\n\n\n\n\n\n\n\n\nBy default, agg() ignores missing values."
  },
  {
    "objectID": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "href": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "title": "Introduction to Python and Pandas",
    "section": "Saving results in new objects",
    "text": "Saving results in new objects\n\nAfter performing operations on our data, we can save the modified dataset as a new object.\n\nmean_penguins_data = (penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\nmean_penguins_data\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386"
  },
  {
    "objectID": "Tools/Tools1.slides.html#more-on-pandas",
    "href": "Tools/Tools1.slides.html#more-on-pandas",
    "title": "Introduction to Python and Pandas",
    "section": "More on pandas",
    "text": "More on pandas\n\n\nhttps://wesmckinney.com/book/"
  },
  {
    "objectID": "Module4/PCA.slides.html#agenda",
    "href": "Module4/PCA.slides.html#agenda",
    "title": "Principal Component Analysis",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nDispersion in one or more dimensions\nPrincipal component analysis"
  },
  {
    "objectID": "Module4/PCA.slides.html#load-the-libraries",
    "href": "Module4/PCA.slides.html#load-the-libraries",
    "title": "Principal Component Analysis",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, and sklearn libraries in Python."
  },
  {
    "objectID": "Module4/PCA.slides.html#types-of-learning",
    "href": "Module4/PCA.slides.html#types-of-learning",
    "title": "Principal Component Analysis",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/PCA.slides.html#unsupervised-learning-methods",
    "href": "Module4/PCA.slides.html#unsupervised-learning-methods",
    "title": "Principal Component Analysis",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/PCA.slides.html#dispersion-in-one-dimension",
    "href": "Module4/PCA.slides.html#dispersion-in-one-dimension",
    "title": "Principal Component Analysis",
    "section": "Dispersion in one dimension",
    "text": "Dispersion in one dimension\n\nThe concept of principal components requires an understanding of the dispersion or variability of the data.\nSuppose we have data for a single predictor."
  },
  {
    "objectID": "Module4/PCA.slides.html#dispersion-in-two-dimensions",
    "href": "Module4/PCA.slides.html#dispersion-in-two-dimensions",
    "title": "Principal Component Analysis",
    "section": "Dispersion in two dimensions",
    "text": "Dispersion in two dimensions"
  },
  {
    "objectID": "Module4/PCA.slides.html#capturing-dispersion",
    "href": "Module4/PCA.slides.html#capturing-dispersion",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension."
  },
  {
    "objectID": "Module4/PCA.slides.html#capturing-dispersion-1",
    "href": "Module4/PCA.slides.html#capturing-dispersion-1",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\nA single predictor \\(X_2\\) captures much of the spread in the data."
  },
  {
    "objectID": "Module4/PCA.slides.html#lets-see-another-example",
    "href": "Module4/PCA.slides.html#lets-see-another-example",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example"
  },
  {
    "objectID": "Module4/PCA.slides.html#lets-see-another-example-1",
    "href": "Module4/PCA.slides.html#lets-see-another-example-1",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example\n\n\n\n\n\n\n\n\n\n\nA single predictor captures much of the dispersion in the data. In this case, the new predictor has the form \\(Z_1 = a X_1 + b X_2 + c.\\)"
  },
  {
    "objectID": "Module4/PCA.slides.html#section",
    "href": "Module4/PCA.slides.html#section",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Alternatively, we can use two alternative dimensions to capture the dispersion."
  },
  {
    "objectID": "Module4/PCA.slides.html#a-new-coordinate-system",
    "href": "Module4/PCA.slides.html#a-new-coordinate-system",
    "title": "Principal Component Analysis",
    "section": "A new coordinate system",
    "text": "A new coordinate system\n\n\n\n\n\nThe new coordinate axis is given by two new predictors, \\(Z_1\\) and \\(Z_2\\). Both are given by linear equations of the new predictors.\nThe first axis, \\(Z_1\\), captures a large portion of the dispersion, while \\(Z_2\\) captures a small portion from another angle.\nThe new axes, \\(Z_1\\) and \\(Z_2\\), are called principal components."
  },
  {
    "objectID": "Module4/PCA.slides.html#dimension-reduction",
    "href": "Module4/PCA.slides.html#dimension-reduction",
    "title": "Principal Component Analysis",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nPrincipal Components Analysis (PCA) helps us reduce the dimension of the data.\n\n\nIt creates a new coordinate axis in two (or more) dimensions.\nTechnically, it creates new predictors by combining highly correlated predictors. The new predictors are uncorrelated."
  },
  {
    "objectID": "Module4/PCA.slides.html#setup",
    "href": "Module4/PCA.slides.html#setup",
    "title": "Principal Component Analysis",
    "section": "Setup",
    "text": "Setup\n\nStep 1. We start with a database with \\(n\\) observations and \\(p\\) predictors.\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n15\n14\n5\n\n\n2\n1\n6\n\n\n10\n3\n17\n\n\n8\n18\n9\n\n\n12\n16\n11"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-1",
    "href": "Module4/PCA.slides.html#section-1",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. We standardize each predictor individually.\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}\\]\n\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n\n1.15\n0.46\n-0.96\n\n\n\n-1.52\n-1.20\n-0.75\n\n\n\n0.12\n-0.95\n1.55\n\n\n\n-0.29\n0.97\n-0.13\n\n\n\n0.53\n0.72\n0.29\n\n\nSum\n0\n0\n0\n\n\nVariance\n1\n1\n1"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-2",
    "href": "Module4/PCA.slides.html#section-2",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. We assume that the standardized database is an \\(n\\times p\\) matrix \\(\\mathbf{X}\\).\n\\[\\mathbf{X} = \\begin{pmatrix}\n1.15    &   0.46    &   -0.96   \\\\\n-1.52   &   -1.20   &   -0.75   \\\\\n0.12    &   -0.95   &   1.55    \\\\\n-0.29   &   0.97    &   -0.13   \\\\\n0.53    &   0.72    &   0.29    \\\\\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "Module4/PCA.slides.html#algorithm",
    "href": "Module4/PCA.slides.html#algorithm",
    "title": "Principal Component Analysis",
    "section": "Algorithm",
    "text": "Algorithm\n\nThe PCA algorithm has its origins in linear algebra.\n\nIts basic idea is:\n\nCreate a matrix \\(\\mathbf{C}\\) with the correlations between the predictors of the matrix \\(\\mathbf{X}\\).\nSplit the matrix \\(\\mathbf{C}\\) into three parts, which give us the new coordinate axis and the importance of each axis."
  },
  {
    "objectID": "Module4/PCA.slides.html#correlation-matrix",
    "href": "Module4/PCA.slides.html#correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nContinuing with our example, the correlation matrix contains the correlations between two columns of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "Module4/PCA.slides.html#partitioning-the-correlation-matrix",
    "href": "Module4/PCA.slides.html#partitioning-the-correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Partitioning the correlation matrix",
    "text": "Partitioning the correlation matrix\n\nThe \\(\\mathbf{C}\\) matrix is partitioned using the eigenvalue and eigenvector decomposition method."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-3",
    "href": "Module4/PCA.slides.html#section-3",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The columns of \\(\\mathbf{B}\\) define the axes of the new coordinate system. These axes are called principal components.\nThe diagonal values in \\(\\mathbf{A}\\) define the individual importance of each principal component (axis)."
  },
  {
    "objectID": "Module4/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "href": "Module4/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "title": "Principal Component Analysis",
    "section": "Proportion of the dispersion explained by the component",
    "text": "Proportion of the dispersion explained by the component\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion of the dispersion in the data that is captured by the first component is \\(\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53\\)."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-4",
    "href": "Module4/PCA.slides.html#section-4",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion captured by the second component is \\(\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36\\).\nThe proportion captured by the third component is \\(\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11.\\)"
  },
  {
    "objectID": "Module4/PCA.slides.html#comments",
    "href": "Module4/PCA.slides.html#comments",
    "title": "Principal Component Analysis",
    "section": "Comments",
    "text": "Comments\n\nPrincipal components can be used to approximate a matrix.\nFor example, we can approximate the matrix \\(\\mathbf{C}\\) by setting the third component equal to zero.\n\n\\[\\begin{pmatrix}\n-0.68   &   0.35    &   0.00    \\\\\n-0.72   &   -0.13   &   0.00    \\\\\n0.16    &   0.93    &   0.00\\\\\n\\end{pmatrix} \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.00    \\\\\n\\end{pmatrix} \\begin{pmatrix}\n-0.68   &   -0.72   &   0.16    \\\\\n0.35    &   -0.13   &   0.93    \\\\\n0.00    &   0.00    &   0.00    \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.86    &   0.73    &   0.18    \\\\\n0.73    &   0.85    &   -0.30   \\\\\n0.18    &   -0.30   &   0.96    \\\\\n\\end{pmatrix}\\]\n\n\n\\[\\approx \\begin{pmatrix}\n1.00    &   0.58    &   0.11    \\\\\n0.58    &   1.00    &   -0.23   \\\\\n0.11    &   -0.23   &   1.00    \\\\\n\\end{pmatrix} = \\mathbf{C}\\]"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-5",
    "href": "Module4/PCA.slides.html#section-5",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Approximations are useful for storing large matrices.\nThis is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\nThis is the idea behind image compression."
  },
  {
    "objectID": "Module4/PCA.slides.html#example-1",
    "href": "Module4/PCA.slides.html#example-1",
    "title": "Principal Component Analysis",
    "section": "Example 1",
    "text": "Example 1\n\nConsider a database of the 100 most popular songs on TikTok. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements.\nEnergy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\nThe overall volume of a track in decibels (dB). Loudness values are averaged across the entire track."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-6",
    "href": "Module4/PCA.slides.html#section-6",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Other predictors are:\n\nSpeech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\nA confidence measure from 0 to 1 about whether the track is acoustic.\nDetects the presence of an audience in the recording.\nA measure from 0 to 1 that describes the musical positivity a track conveys."
  },
  {
    "objectID": "Module4/PCA.slides.html#the-data",
    "href": "Module4/PCA.slides.html#the-data",
    "title": "Principal Component Analysis",
    "section": "The data",
    "text": "The data\n\ntiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\ntiktok_data.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nalbum\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\n0\nSay So\nDoja Cat\nHot Pink\n0.787\n0.673\n-4.583\n0.1590\n0.26400\n0.0904\n0.779\n110.962\n\n\n1\nBlinding Lights\nThe Weeknd\nAfter Hours\n0.514\n0.730\n-5.934\n0.0598\n0.00146\n0.0897\n0.334\n171.005\n\n\n2\nSupalonely (feat. Gus Dapperton)\nBENEE\nHey u x\n0.862\n0.631\n-4.746\n0.0515\n0.29100\n0.1230\n0.841\n128.978\n\n\n3\nSavage\nMegan Thee Stallion\nSuga\n0.843\n0.741\n-5.609\n0.3340\n0.02520\n0.0960\n0.680\n168.983\n\n\n4\nMoral of the Story\nAshe\nMoral of the Story\n0.572\n0.406\n-8.624\n0.0427\n0.58700\n0.1020\n0.265\n119.812"
  },
  {
    "objectID": "Module4/PCA.slides.html#standardize-the-data",
    "href": "Module4/PCA.slides.html#standardize-the-data",
    "title": "Principal Component Analysis",
    "section": "Standardize the data",
    "text": "Standardize the data\n\nRemember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n\n# Select the predictors\nfeatures = ['danceability', 'energy', 'loudness', 'speechiness',\n            'acousticness', 'liveness', 'valence', 'tempo']\nX_tiktok = tiktok_data.filter(features)  \n\n# Standardize the data\nscaler = StandardScaler()\nXs_tiktok = scaler.fit_transform(X_tiktok)"
  },
  {
    "objectID": "Module4/PCA.slides.html#pca-in-python",
    "href": "Module4/PCA.slides.html#pca-in-python",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n\nWe tell Python that we want to apply PCA using the function PCA() from sklearn. Next, we run the algorithm using .fit_transform().\n\npca = PCA()\nPCA_tiktok = pca.fit_transform(Xs_tiktok)"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-7",
    "href": "Module4/PCA.slides.html#section-7",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The Screen or Summary Plot tells you the variability captured by each component. This variability is given by the Eigenvalue. From 1 to 8 components.\nThe first component covers most of the data dispersion.\nThis graph is used to define the total number of components to use."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-8",
    "href": "Module4/PCA.slides.html#section-8",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate a scree plot is below.\n\nexplained_var = pca.explained_variance_ratio_\n\nplt.figure(figsize=(5, 5))\nplt.plot(range(1, len(explained_var) + 1), explained_var, \n         marker='o', linestyle='-')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.xticks(range(1, len(explained_var) + 1))\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#biplot",
    "href": "Module4/PCA.slides.html#biplot",
    "title": "Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\nDisplays the graphical observations on the new coordinate axis given by the first two components.\nHelps visualize data for three or more predictors using a two-dimensional scatter plot.\nA red line indicates the growth direction of the labeled variable."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-9",
    "href": "Module4/PCA.slides.html#section-9",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate the biplot is lenghty but it can be broken into three steps.\nStep 1. Create a DataFrame with the PCA results\n\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\n0\n1.103065\n0.558086\n-0.800688\n0.446496\n0.605944\n-0.044089\n0.287325\n-0.413604\n\n\n1\n0.805080\n-0.766973\n1.580513\n-2.215856\n0.359655\n0.708123\n-0.882761\n0.113058\n\n\n2\n1.330433\n0.728161\n-0.288982\n0.376298\n0.786185\n-1.134308\n0.178388\n-0.242497\n\n\n3\n1.496277\n2.095014\n1.351398\n-0.621691\n0.390949\n0.494101\n0.024648\n-0.080720\n\n\n4\n-1.973362\n-0.966108\n-0.302071\n-1.266269\n0.414639\n-0.335677\n-0.076711\n-0.140126"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-10",
    "href": "Module4/PCA.slides.html#section-10",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. Create biplot of first two principal components\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-11",
    "href": "Module4/PCA.slides.html#section-11",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. Add more information to the biplot.\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-12",
    "href": "Module4/PCA.slides.html#section-12",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "With some extra lines of code, we label the points in the plot.\n\n\nCode\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df = (pca_df\n          .assign(songs = tiktok_data['track_name'])\n          )\n\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add labels for each song\nfor i in range(pca_df.shape[0]):\n    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n             pca_df['songs'][i], fontsize=8, alpha=0.7)\n\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module3/Nonparametric.slides.html#agenda",
    "href": "Module3/Nonparametric.slides.html#agenda",
    "title": "Nonparametric Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nLogistic regression\nEstimating a logistic regression model\nClassification performance"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#agenda",
    "href": "Module2/EnsembleMethods.slides.html#agenda",
    "title": "Ensemble Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "href": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "title": "Ensemble Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#decision-trees",
    "title": "Ensemble Methods",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "title": "Ensemble Methods",
    "section": "Limitations of decision trees",
    "text": "Limitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, decision trees can be combined to build effective algorithms for these problems."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "href": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "title": "Ensemble Methods",
    "section": "Ensamble methods",
    "text": "Ensamble methods\n\nEnsemble methods refer to frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "href": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "title": "Ensemble Methods",
    "section": "Bootstrap samples",
    "text": "Bootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-1",
    "href": "Module2/EnsembleMethods.slides.html#bagging-1",
    "title": "Ensemble Methods",
    "section": "Bagging",
    "text": "Bagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions",
    "href": "Module2/EnsembleMethods.slides.html#predictions",
    "title": "Ensemble Methods",
    "section": "Predictions",
    "text": "Predictions\n\nLet \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values. For classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\mathbf{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section",
    "href": "Module2/EnsembleMethods.slides.html#section",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Using the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-1",
    "href": "Module2/EnsembleMethods.slides.html#section-1",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Compute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-2",
    "href": "Module2/EnsembleMethods.slides.html#section-2",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Classify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\)."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#implementation",
    "href": "Module2/EnsembleMethods.slides.html#implementation",
    "title": "Ensemble Methods",
    "section": "Implementation",
    "text": "Implementation\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#example-1",
    "href": "Module2/EnsembleMethods.slides.html#example-1",
    "title": "Ensemble Methods",
    "section": "Example 1",
    "text": "Example 1\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "href": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "title": "Ensemble Methods",
    "section": "Read the dataset",
    "text": "Read the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "href": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "title": "Ensemble Methods",
    "section": "Selected predictors.",
    "text": "Selected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nX_full = Adult_data.filter(['age', 'sex', 'race', 'education.num', \n                            'hours.per.week'])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "href": "Module2/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "title": "Ensemble Methods",
    "section": "Pre-processing for categorical predictors",
    "text": "Pre-processing for categorical predictors\n\nUnfortunately, bagging does not work with categorical predictors. We must transform them into dummy variables using the code below.\n\n# Turn categorical predictors into dummy variables.\nX_dummies = pd.get_dummies(X_full[['sex', 'race']])\n\n# Drop original predictors from the test.\nX_other = X_full.drop(['sex', 'race'], axis=1)\n\n# Update the predictor matrix.\nX_full = pd.concat([X_other, X_dummies], axis=1)"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "href": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "title": "Ensemble Methods",
    "section": "Set the target class",
    "text": "Set the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_data.filter(['income'])\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nincome_large\nincome_small\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-3",
    "href": "Module2/EnsembleMethods.slides.html#section-3",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Here we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['income_large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: income_large, dtype: int64"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "href": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "title": "Ensemble Methods",
    "section": "Training and validation datasets",
    "text": "Training and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2)\n\nWe use 80% of the dataset for training and the rest for validation."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "href": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "title": "Ensemble Methods",
    "section": "Bagging in Python",
    "text": "Bagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions-1",
    "href": "Module2/EnsembleMethods.slides.html#predictions-1",
    "title": "Ensemble Methods",
    "section": "Predictions",
    "text": "Predictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 0, ..., 1, 0, 0])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "title": "Ensemble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy",
    "href": "Module2/EnsembleMethods.slides.html#accuracy",
    "title": "Ensemble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "href": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "title": "Ensemble Methods",
    "section": "A single deep tree",
    "text": "A single deep tree\nTo compare the bagging, let’s use a single deep tree.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.76"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#advantages",
    "href": "Module2/EnsembleMethods.slides.html#advantages",
    "title": "Ensemble Methods",
    "section": "Advantages",
    "text": "Advantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations",
    "href": "Module2/EnsembleMethods.slides.html#limitations",
    "title": "Ensemble Methods",
    "section": "Limitations",
    "text": "Limitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#other-issues",
    "href": "Module2/EnsembleMethods.slides.html#other-issues",
    "title": "Ensemble Methods",
    "section": "Other issues",
    "text": "Other issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-4",
    "href": "Module2/EnsembleMethods.slides.html#section-4",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Bagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "title": "Ensemble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "href": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "title": "Ensemble Methods",
    "section": "Why does it work?",
    "text": "Why does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "href": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "title": "Ensemble Methods",
    "section": "Tuning parameter",
    "text": "Tuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "href": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "title": "Ensemble Methods",
    "section": "The final product is a black box",
    "text": "The final product is a black box\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "title": "Ensemble Methods",
    "section": "Random Forest in Python",
    "text": "Random Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "title": "Ensemble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "href": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "title": "Ensemble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#agenda",
    "href": "Module2/IntrotoDataScience.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nData Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "href": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data Science is …",
    "text": "Data Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#similar-concepts",
    "href": "Module2/IntrotoDataScience.slides.html#similar-concepts",
    "title": "Introduction to Data Science",
    "section": "Similar concepts",
    "text": "Similar concepts\n\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#in-2004",
    "href": "Module2/IntrotoDataScience.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004…",
    "text": "In 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section",
    "href": "Module2/IntrotoDataScience.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#the-result",
    "href": "Module2/IntrotoDataScience.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "title": "Introduction to Data Science",
    "section": "Cross-Industry Standard Process (CRISP) for Data Science",
    "text": "Cross-Industry Standard Process (CRISP) for Data Science"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "href": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "title": "Introduction to Data Science",
    "section": "CRISP Model",
    "text": "CRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nDeployment: How do stakeholders access the results?"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "href": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business understanding",
    "text": "Business understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "title": "Introduction to Data Science",
    "section": "Problems in Data Science",
    "text": "Problems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-1",
    "href": "Module2/IntrotoDataScience.slides.html#section-1",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Clustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#discussion",
    "href": "Module2/IntrotoDataScience.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data understanding I",
    "text": "Data understanding I\n\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data understanding II",
    "text": "Data understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#example",
    "href": "Module2/IntrotoDataScience.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\n\nIn the 1980s, credit cards had uniform pricing — companies lacked the systems for mass differential pricing.\nBy 1990, Richard Fairbanks and Nigel Morris saw that IT could power predictive models to customize offers (pricing, credit limits, low introductory rates, cash back, loyalty points).\nSignet Bank’s strategy: model profitability, not just probability of default, since a small fraction of customers generate most profits."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-2",
    "href": "Module2/IntrotoDataScience.slides.html#section-2",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Problem: They lacked data on how different credit terms affected profitability.\n\n\n\nSolution: Acquire data at a cost — run experiments offering varied terms to different customers. Losses from these offers were considered investments in data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#what-happened",
    "href": "Module2/IntrotoDataScience.slides.html#what-happened",
    "title": "Introduction to Data Science",
    "section": "What happened?",
    "text": "What happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "href": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "title": "Introduction to Data Science",
    "section": "Richard Fairbanks and Nigel Morris",
    "text": "Richard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#terminology",
    "href": "Module2/IntrotoDataScience.slides.html#terminology",
    "title": "Introduction to Data Science",
    "section": "Terminology",
    "text": "Terminology\n\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a whole collection of \\(p\\) predictors.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "href": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "title": "Introduction to Data Science",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised learning…",
    "text": "Supervised learning…\n\nIncludes algorithms that learn by example. That is, we provide the supervised algorithm with a data set with known predictor and response values. The algorithm must find a way to determine the responses from the predictors.\nSince we have the correct (true) responses, the algorithm can identify patterns in the data, learn from its mistakes, and make better predictions of the responses.\nThe algorithm is trained to reach a high level of accuracy and performance for predicting the responses."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#mathematically",
    "href": "Module2/IntrotoDataScience.slides.html#mathematically",
    "title": "Introduction to Data Science",
    "section": "Mathematically",
    "text": "Mathematically\n\nWe want to establish the following relationship\n\\[\nY = f(\\boldsymbol{X}) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error.\n\n\n\\(f(\\boldsymbol{X})\\) represents the true relationship between the response (\\(Y\\)) and predictors (\\(\\boldsymbol{X}\\)).\n\n\n\n\nHowever, \\(f(\\boldsymbol{X})\\) is unknown and very complex!"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-4",
    "href": "Module2/IntrotoDataScience.slides.html#section-4",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A supervised algorithm attempts to construct an approximation \\(\\hat{f}(\\boldsymbol{X})\\) to the true function \\(f(\\boldsymbol{X})\\) using available data on the predictors and response.\nIdeally, the algorithm builds an \\(\\hat{f}(\\boldsymbol{X})\\) that is interpretable, but not necessarily."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "href": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "title": "Introduction to Data Science",
    "section": "Two data sets",
    "text": "Two data sets\nIn supervised learning, there are two main types of data:\n\nTraining data is data used by the supervised algorithm to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data NOT used in the algorithm’s training process, but is used to evaluate the quality of \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular supervised algorithms",
    "text": "Popular supervised algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Unsupervised learning…",
    "text": "Unsupervised learning…\n\nstudies data of the predictors (\\(\\boldsymbol{X}\\)) to identify patterns. There are no responses.\nAn unsupervised algorithm identifies correlations and relationships by analyzing available training data. So, the unsupervised algorithm is left to interpret the data set and organize it in some way to describe its structure.\nIn technical terms, we want the algorithm to say something about the joint probability distribution of the predictors \\(P(X_1, X_2, \\ldots, X_p)\\)."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Unsupervised Algorithms",
    "text": "Popular Unsupervised Algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#lets-play-with-supervised-models.",
    "href": "Module2/IntrotoDataScience.slides.html#lets-play-with-supervised-models.",
    "title": "Introduction to Data Science",
    "section": "Let’s play with supervised models.",
    "text": "Let’s play with supervised models.\n\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "index.slides.html#course-topics",
    "href": "index.slides.html#course-topics",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "Course topics",
    "text": "Course topics\nModule 1\n\nIndicators (slides)\nSelection of Indicators (slides)\n\nModule 2\n\nIntroduction to Data Science (slides)\nIntroduction to Python and pandas (slides) (colab)\nData wrangling and visualization (slides) (colab) (pandas cheat sheet)\nDecision trees for classification (slides) (colab)\nEnsemble methods (slides) (colab)\nK-nearest neighbours (slides) (colab)\n\nModule 3\n\nPredictive models (slides) (colab)\nAutocorrelation models (slides) (colab)\nEnsemble methods for time series\n\nModule 4\n\nClustering methods (slides) (colab)\nPrincipal component analysis (slides) (colab)"
  },
  {
    "objectID": "index.slides.html#about-the-author",
    "href": "index.slides.html#about-the-author",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "About the author",
    "text": "About the author\nAlan R. Vazquez is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\nLicense\nIN2004B Generation of Value with Data Analytics © 2025 by Alan R Vazquez is licensed under CC BY-NC-SA 4.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "",
    "text": "Course topics\n\nModule 1\n\nIndicators (slides)\nSelection of Indicators (slides)\n\n\n\nModule 2\n\nIntroduction to Data Science (slides)\nIntroduction to Python and pandas (slides) (colab)\nData wrangling and visualization (slides) (colab) (pandas cheat sheet)\nDecision trees for classification (slides) (colab)\nEnsemble methods (slides) (colab)\nK-nearest neighbours (slides) (colab)\n\n\n\nModule 3\n\nPredictive models (slides) (colab)\nAutocorrelation models (slides) (colab)\nEnsemble methods for time series\n\n\n\nModule 4\n\nClustering methods (slides) (colab)\nPrincipal component analysis (slides) (colab)\n\n\n\n\nAbout the author\nAlan R. Vazquez is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\n\nLicense\nIN2004B Generation of Value with Data Analytics © 2025 by Alan R Vazquez is licensed under CC BY-NC-SA 4.0",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Module1/Indicators.slides.html#agenda",
    "href": "Module1/Indicators.slides.html#agenda",
    "title": "Indicators",
    "section": "Agenda",
    "text": "Agenda\n\n\nBasic Indicator Concepts\nLagging and Leading Indicators"
  },
  {
    "objectID": "Module1/Indicators.slides.html#management",
    "href": "Module1/Indicators.slides.html#management",
    "title": "Indicators",
    "section": "Management",
    "text": "Management\n\n\n“Management is the process by which an environment is designed and maintained in which individuals working in groups effectively accomplish specific goals.”\n\nKoontz, Harold, “Management,” 14th Edition, McGraw Hill, 2012"
  },
  {
    "objectID": "Module1/Indicators.slides.html#functions-of-management",
    "href": "Module1/Indicators.slides.html#functions-of-management",
    "title": "Indicators",
    "section": "Functions of management",
    "text": "Functions of management\nThe management process is broken down into five management functions:\n\n\n\nPlan: Choosing missions and objectives, and the actions to achieve them.\nOrganizing: Establishing an intentional structure of roles for people to perform in an organization.\nStaffing: Filling and maintaining positions in the organizational structure.\nDirecting: Influencing people to contribute to organizational and group goals.\nControlling: Measuring and correcting individual and organizational performance to ensure that events conform to plans."
  },
  {
    "objectID": "Module1/Indicators.slides.html#integration-of-planning-and-control",
    "href": "Module1/Indicators.slides.html#integration-of-planning-and-control",
    "title": "Indicators",
    "section": "Integration of planning and control",
    "text": "Integration of planning and control"
  },
  {
    "objectID": "Module1/Indicators.slides.html#planning",
    "href": "Module1/Indicators.slides.html#planning",
    "title": "Indicators",
    "section": "Planning",
    "text": "Planning\nAs a result of the planning process, some of the following types of plans can be generated:\n\n\nObjectives or goals. These are specific and measurable results.\nStrategies. Approaches or plans to achieve objectives.\nProcedures. These detail the execution of an activity.\nPrograms. Organized sets of activities to achieve objectives.\nBudgets. Detailed financial plans to facilitate economic decision-making.\n\n\n\nHigh need to measure progress"
  },
  {
    "objectID": "Module1/Indicators.slides.html#planning-1",
    "href": "Module1/Indicators.slides.html#planning-1",
    "title": "Indicators",
    "section": "Planning",
    "text": "Planning\nAs a result of the planning process, some of the following types of plans can be generated:\n\n\nObjectives or goals. These are specific and measurable results.\nStrategies. Approaches or plans to achieve objectives.\nProcedures. These detail the execution of an activity.\nPrograms. Organized sets of activities to achieve objectives.\nBudgets. Detailed financial plans to facilitate economic decision-making.\n\n\n\nHigh need to measure progress"
  },
  {
    "objectID": "Module1/Indicators.slides.html#definition-of-smart-objectives",
    "href": "Module1/Indicators.slides.html#definition-of-smart-objectives",
    "title": "Indicators",
    "section": "Definition of SMART Objectives",
    "text": "Definition of SMART Objectives\n\n\n\nSpecific: An objective should be clear and specific, avoiding ambiguity. It should answer the questions what, who, when, where, and why.\nMeasurable: An objective should be quantifiable or at least evaluable to determine progress and success. It should be possible to measure it with tangible indicators or criteria.\nAchievable: A goal should be realistic and achievable, considering available resources, time, and necessary skills.\nRelevant: A goal should be relevant and aligned with the broader goals of the organization or individual.\nTime-bounded: A goal should have a clearly defined timeframe or deadline."
  },
  {
    "objectID": "Module1/Indicators.slides.html#examples",
    "href": "Module1/Indicators.slides.html#examples",
    "title": "Indicators",
    "section": "Examples",
    "text": "Examples\nExample 1:\n\n\n\nObjective: Complete staff training.\nSMART: Complete at least 90% of the 2014 training program for all company operating personnel by November 30 of this year.\n\n\n\n\nExample 2\n\n\n\nObjective: Increase sales by 20%.\nSMART: Achieve a sales increase of product X of at least 17% by the end of the first half of 2015, while maintaining a profitability for the company of at least 5%."
  },
  {
    "objectID": "Module1/Indicators.slides.html#the-need-to-measure",
    "href": "Module1/Indicators.slides.html#the-need-to-measure",
    "title": "Indicators",
    "section": "The need to measure",
    "text": "The need to measure\n\nTo carry out the Planning, Implementation, and Control processes, we need an information system to evaluate whether the planned objectives are being achieved and whether the implemented actions are being carried out in accordance with the plans."
  },
  {
    "objectID": "Module1/Indicators.slides.html#what-is-an-indicator",
    "href": "Module1/Indicators.slides.html#what-is-an-indicator",
    "title": "Indicators",
    "section": "What is an indicator?",
    "text": "What is an indicator?\n\n\n“It is the result of a quantitative or qualitative measurement, or some other criterion, by which the performance, efficiency, achievement, etc., of a person or organization can be evaluated, often compared to a standard or goal.”\n\nCollins English Dictionary."
  },
  {
    "objectID": "Module1/Indicators.slides.html#what-is-an-indicator-1",
    "href": "Module1/Indicators.slides.html#what-is-an-indicator-1",
    "title": "Indicators",
    "section": "What is an indicator?",
    "text": "What is an indicator?\n\n\n“The qualitative and/or quantitative information on an examined phenomenon (or a process, or a result), which makes it possible to analyze its evolution and to check whether quality targets are met, driving actions and decisions.”\n\nFranceschini, Fiorenzo & Galetto, Maurizio & Maisano, Domenico. (2007). Management by Measurement: Designing Key Indicators and Performance Measurement Systems. 10.1007/978-3-540-73212-9."
  },
  {
    "objectID": "Module1/Indicators.slides.html#examples-of-indicators",
    "href": "Module1/Indicators.slides.html#examples-of-indicators",
    "title": "Indicators",
    "section": "Examples of indicators",
    "text": "Examples of indicators"
  },
  {
    "objectID": "Module1/Indicators.slides.html#characteristics-of-an-indicator",
    "href": "Module1/Indicators.slides.html#characteristics-of-an-indicator",
    "title": "Indicators",
    "section": "Characteristics of an indicator",
    "text": "Characteristics of an indicator\n\nFundamental:\n\nValidity: The indicator must accurately reflect the actual behavior of the phenomenon, variable, result, etc., to be measured.\nStability: The indicator must be defined, calculated, and interpreted in the same way over time (allowing for comparisons and observing trends)."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-2",
    "href": "Module1/Indicators.slides.html#section-2",
    "title": "Indicators",
    "section": "",
    "text": "Ideal:\n\nSimple and easy to interpret.\nAble to indicate trends over time.\nSensitive to changes within and outside the organization.\nEasy to collect and process data.\nQuick and easy to update."
  },
  {
    "objectID": "Module1/Indicators.slides.html#usefulness-of-indicators",
    "href": "Module1/Indicators.slides.html#usefulness-of-indicators",
    "title": "Indicators",
    "section": "Usefulness of indicators",
    "text": "Usefulness of indicators"
  },
  {
    "objectID": "Module1/Indicators.slides.html#dimensions-of-analysis",
    "href": "Module1/Indicators.slides.html#dimensions-of-analysis",
    "title": "Indicators",
    "section": "Dimensions of analysis",
    "text": "Dimensions of analysis\n\n\nIndicators and the data that drive them are often stratified with respect to other variables.\nVariables used as stratification criteria are called “Analysis Dimensions” (they are dimensions from the perspective of data).\n\n\nExample: In a sales process, monthly sales can be stratified by distribution channel, region of the country, product family, etc., for analysis and visualization purposes."
  },
  {
    "objectID": "Module1/Indicators.slides.html#for-example",
    "href": "Module1/Indicators.slides.html#for-example",
    "title": "Indicators",
    "section": "For example",
    "text": "For example"
  },
  {
    "objectID": "Module1/Indicators.slides.html#selecting-indicators",
    "href": "Module1/Indicators.slides.html#selecting-indicators",
    "title": "Indicators",
    "section": "Selecting indicators",
    "text": "Selecting indicators\n\nSelecting indicators is a critical factor for an organization to move closer to fulfilling its mission and turning its strategies into reality. Indicators and strategies are inextricably linked.\n\n\nA strategy without indicators is useless; indicators without a strategy are irrelevant!"
  },
  {
    "objectID": "Module1/Indicators.slides.html#two-main-types-of-indicators",
    "href": "Module1/Indicators.slides.html#two-main-types-of-indicators",
    "title": "Indicators",
    "section": "Two main types of indicators",
    "text": "Two main types of indicators\n\nA lagging indicator measures the outcome of performance at the end of a period. It is backward-looking because it shows us the consequences of what has already been done. They are also known as result indicators.\n\n\nA leading indicator measures the performance of factors that are critical now to achieving a desired outcome in the future."
  },
  {
    "objectID": "Module1/Indicators.slides.html#lagging-indicator",
    "href": "Module1/Indicators.slides.html#lagging-indicator",
    "title": "Indicators",
    "section": "Lagging Indicator",
    "text": "Lagging Indicator\n\nGoal: Measures the outcome of performance at the end of a period.\n\nExample: Annual sales, market share, ROI.\n\nAdvantage: They are objective.\nDisadvantage: They reflect the effect of past actions."
  },
  {
    "objectID": "Module1/Indicators.slides.html#leading-indicator",
    "href": "Module1/Indicators.slides.html#leading-indicator",
    "title": "Indicators",
    "section": "Leading indicator",
    "text": "Leading indicator\n\nGoal: Measures processes, activities, behaviors that tend to change before the system starts to follow a particular pattern or trend.\n\nExample: # of customers visited, # of courses offered\n\nAdvantage: They are predictive, allowing for strategy corrections\nDisadvantage: Based on cause-and-effect hypotheses."
  },
  {
    "objectID": "Module1/Indicators.slides.html#other-examples",
    "href": "Module1/Indicators.slides.html#other-examples",
    "title": "Indicators",
    "section": "Other examples",
    "text": "Other examples\nIn the context of a YouTuber:\n\nLeading indicator: Number of views of a YouTube video in the first 24 hours.\nLagging indicator: Monthly revenue generated from monetization.\n\n\nIn the context of Netflix’s sales department:\n\nLeading indicator: Number of users who start a free trial in a given month.\nLagging indicator: Monthly revenue from subscriptions."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-3",
    "href": "Module1/Indicators.slides.html#section-3",
    "title": "Indicators",
    "section": "",
    "text": "In the context of a GenAI company:\n\nLeading indicator: Number of programmers hired with expertise in AI.\nLagging indicator: Number of licenses sold per year.\n\n\nIn the context of a mini-split maintenance company:\n\nLeading indicator: Average response time from technical support to support requests.\nLagging indicator: Number of positive reviews on Google Maps in a month."
  },
  {
    "objectID": "Module1/Indicators.slides.html#how-do-i-define-an-indicator",
    "href": "Module1/Indicators.slides.html#how-do-i-define-an-indicator",
    "title": "Indicators",
    "section": "How do I define an indicator?",
    "text": "How do I define an indicator?\n\nSelection criteria:\n\nDirect relationship with the objective to be measured\nEase of communication focused on the strategy\nRepeatability and reliability\nUpdate frequency\nUsefulness in goal setting\nUsefulness in assigning responsibilities\nUsefulness for downward deployment"
  },
  {
    "objectID": "Module1/Indicators.slides.html#basic-and-derived-indicators",
    "href": "Module1/Indicators.slides.html#basic-and-derived-indicators",
    "title": "Indicators",
    "section": "Basic and Derived Indicators",
    "text": "Basic and Derived Indicators\nA basic indicator is obtained from the direct measurement of a phenomenon or fact. For example: Number of orders delivered completely and on time during the week.\nA derived indicator combines the information from two or more basic or derived indicators. For example: Percentage of orders delivered completely and on time during the week.\n\n\nExample: in Failure Modes and Effects Analysis\n\nBasic indicators: Severity, Occurrence, Detection\nDerived indicator: Risk Priority Number = S \\(\\times\\) O \\(\\times\\) D\n\n\n\nSeverity (S): Measures the impact or severity of the failure if it occurs. It is measured on a scale of 1 to 10, where 1 indicates a negligible effect and 10 represents a catastrophic effect for the user or the system.\nOccurrence (O): Evaluates the probability of the failure occurring. It is evaluated on a scale of 1 to 10, where 1 indicates that the occurrence is very rare and 10 indicates that it is highly probable or frequent.\nDetection (D): Represents the system’s ability to detect the failure before it reaches the customer or end user. It is rated on a scale of 1 to 10, where 1 means that detection is almost certain and 10 means that it is very difficult or impossible to detect before a problem occurs.\n\nDerived Indicator: Risk Priority Number = S\\(\\times\\)O\\(\\times\\)D"
  },
  {
    "objectID": "Module1/Indicators.slides.html#the-format-of-an-indicator",
    "href": "Module1/Indicators.slides.html#the-format-of-an-indicator",
    "title": "Indicators",
    "section": "The format of an indicator",
    "text": "The format of an indicator\nAn indicator should be measured numerically using:\nAbsolute Numbers: Results of a measurement or counting process (volume produced, share price, number of employees, fixed costs, etc.)\nRates: Relationship between two variables with different units (number of units / number of workers, energy consumption / liters produced, etc.)\nIndexes: Dimensionless quantity resulting from dividing the current value of a variable by a reference base value for that variable (consumer price index)"
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-4",
    "href": "Module1/Indicators.slides.html#section-4",
    "title": "Indicators",
    "section": "",
    "text": "Proportions: Relationships between two variables measured in the same units (men vs. women, admitted vs. rejected)\nGrowth or Decrease Percentages: (Current Value – Previous Value)*100/Previous Value.\nEvaluations: Evaluations of a qualitative variable on a Likert-type ordinal scale (low, medium, high, terrible, bad, average, good, excellent)."
  },
  {
    "objectID": "Module1/Indicators.slides.html#activity-1.1-solo-mode",
    "href": "Module1/Indicators.slides.html#activity-1.1-solo-mode",
    "title": "Indicators",
    "section": "Activity 1.1 (solo mode)",
    "text": "Activity 1.1 (solo mode)\nFor the following concepts, propose one leading and one lagging (basic and derived) quantitative indicators:\n\n\nMonthly productivity of a furniture production line\nAnnual staff turnover in a manufacturing company\nCustomer service level of a company that manufactures plastic packaging and delivers regionally\nBusiness profitability for a medium-sized wholesale grocery company\nPerformance of the fundraising process of an association supporting homeless children"
  },
  {
    "objectID": "Module1/Indicators.slides.html#remember-that",
    "href": "Module1/Indicators.slides.html#remember-that",
    "title": "Indicators",
    "section": "Remember that…",
    "text": "Remember that…\n\n\n\nThe purposes of an indicator are:\n\nEstablish quantitative goals.\nOrganizational motivation, induction of desirable behaviors.\nStrategy evaluation and strategic learning."
  }
]
[
  {
    "objectID": "Module2/EnsembleMethods.slides.html#agenda",
    "href": "Module2/EnsembleMethods.slides.html#agenda",
    "title": "Ensamble Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "href": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "title": "Ensamble Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#decision-trees",
    "title": "Ensamble Methods",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "title": "Ensamble Methods",
    "section": "Limitations of decision trees",
    "text": "Limitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, decision trees can be combined to build effective algorithms for these problems."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "href": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "title": "Ensamble Methods",
    "section": "Ensamble methods",
    "text": "Ensamble methods\n\nEnsemble methods refer to frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "href": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "title": "Ensamble Methods",
    "section": "Bootstrap samples",
    "text": "Bootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-1",
    "href": "Module2/EnsembleMethods.slides.html#bagging-1",
    "title": "Ensamble Methods",
    "section": "Bagging",
    "text": "Bagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions",
    "href": "Module2/EnsembleMethods.slides.html#predictions",
    "title": "Ensamble Methods",
    "section": "Predictions",
    "text": "Predictions\n\nLet \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values. For classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\mathbf{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section",
    "href": "Module2/EnsembleMethods.slides.html#section",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Using the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-1",
    "href": "Module2/EnsembleMethods.slides.html#section-1",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Compute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-2",
    "href": "Module2/EnsembleMethods.slides.html#section-2",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Classify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\)."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#implementation",
    "href": "Module2/EnsembleMethods.slides.html#implementation",
    "title": "Ensamble Methods",
    "section": "Implementation",
    "text": "Implementation\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#example-1",
    "href": "Module2/EnsembleMethods.slides.html#example-1",
    "title": "Ensamble Methods",
    "section": "Example 1",
    "text": "Example 1\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "href": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "title": "Ensamble Methods",
    "section": "Read the dataset",
    "text": "Read the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "href": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "title": "Ensamble Methods",
    "section": "Selected predictors.",
    "text": "Selected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nX_full = Adult_data.filter(['age', 'sex', 'race', 'education.num', \n                            'hours.per.week'])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "href": "Module2/EnsembleMethods.slides.html#pre-processing-for-categorical-predictors",
    "title": "Ensamble Methods",
    "section": "Pre-processing for categorical predictors",
    "text": "Pre-processing for categorical predictors\n\nUnfortunately, bagging does not work with categorical predictors. We must transform them into dummy variables using the code below.\n\n# Turn categorical predictors into dummy variables.\nX_dummies = pd.get_dummies(X_full[['sex', 'race']])\n\n# Drop original predictors from the test.\nX_other = X_full.drop(['sex', 'race'], axis=1)\n\n# Update the predictor matrix.\nX_full = pd.concat([X_other, X_dummies], axis=1)"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "href": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "title": "Ensamble Methods",
    "section": "Set the target class",
    "text": "Set the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_data.filter(['income'])\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nincome_large\nincome_small\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-3",
    "href": "Module2/EnsembleMethods.slides.html#section-3",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Here we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['income_large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: income_large, dtype: int64"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "href": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "title": "Ensamble Methods",
    "section": "Training and validation datasets",
    "text": "Training and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2)\n\nWe use 80% of the dataset for training and the rest for validation."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "href": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "title": "Ensamble Methods",
    "section": "Bagging in Python",
    "text": "Bagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions-1",
    "href": "Module2/EnsembleMethods.slides.html#predictions-1",
    "title": "Ensamble Methods",
    "section": "Predictions",
    "text": "Predictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 1, ..., 0, 0, 0])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "title": "Ensamble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy",
    "href": "Module2/EnsembleMethods.slides.html#accuracy",
    "title": "Ensamble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "href": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "title": "Ensamble Methods",
    "section": "A single deep tree",
    "text": "A single deep tree\nTo compare the bagging, let’s use a single deep tree.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.76"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#advantages",
    "href": "Module2/EnsembleMethods.slides.html#advantages",
    "title": "Ensamble Methods",
    "section": "Advantages",
    "text": "Advantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations",
    "href": "Module2/EnsembleMethods.slides.html#limitations",
    "title": "Ensamble Methods",
    "section": "Limitations",
    "text": "Limitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#other-issues",
    "href": "Module2/EnsembleMethods.slides.html#other-issues",
    "title": "Ensamble Methods",
    "section": "Other issues",
    "text": "Other issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-4",
    "href": "Module2/EnsembleMethods.slides.html#section-4",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Bagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "title": "Ensamble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "href": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "title": "Ensamble Methods",
    "section": "Why does it work?",
    "text": "Why does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "href": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "title": "Ensamble Methods",
    "section": "Tuning parameter",
    "text": "Tuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "href": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "title": "Ensamble Methods",
    "section": "The final product is a black box",
    "text": "The final product is a black box\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "title": "Ensamble Methods",
    "section": "Random Forest in Python",
    "text": "Random Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "title": "Ensamble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "href": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "title": "Ensamble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.77"
  }
]
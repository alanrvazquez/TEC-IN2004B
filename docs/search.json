[
  {
    "objectID": "Module1/Indicators.slides.html#agenda",
    "href": "Module1/Indicators.slides.html#agenda",
    "title": "Indicators",
    "section": "Agenda",
    "text": "Agenda\n\n\nConceptos Básicos de Indicadores\nModelos para Definir Indicadores\nDocumentación de Indicadores"
  },
  {
    "objectID": "Module1/Indicators.slides.html#la-administración",
    "href": "Module1/Indicators.slides.html#la-administración",
    "title": "Indicators",
    "section": "La administración",
    "text": "La administración\n\n“Administración es el proceso mediante el cual se diseña y mantiene un ambiente en el que individuos que trabajan en grupos cumplen metas específicas de manera eficaz”.\n\nKoontz, Harold, “Administración”, 14 Edición, Mc Graw Hill, México, 2012"
  },
  {
    "objectID": "Module1/Indicators.slides.html#funciones-de-la-administración",
    "href": "Module1/Indicators.slides.html#funciones-de-la-administración",
    "title": "Indicators",
    "section": "Funciones de la Administración",
    "text": "Funciones de la Administración\nEl proceso administrativo se desglosa en cinco funciones gerenciales:\n\nPlanear: elegir misiones y objetivos, y las acciones para lograrlos\nOrganizar: establecer una estructura intencional de funciones que las personas desempeñen en una organización\nIntegrar personal: Cubrir y mantener cubiertos los puestos en la estructura organizacional\nDirigir: Influir en las personas para que contribuyan a las metas organizacionales y de grupo\nControlar: Medir y corregir el desempeño individual y organizacional para asegurar que los hechos se conformen a los planes."
  },
  {
    "objectID": "Module1/Indicators.slides.html#integración-de-la-planeación-y-el-control",
    "href": "Module1/Indicators.slides.html#integración-de-la-planeación-y-el-control",
    "title": "Indicators",
    "section": "Integración de la Planeación y el Control",
    "text": "Integración de la Planeación y el Control"
  },
  {
    "objectID": "Module1/Indicators.slides.html#planeación",
    "href": "Module1/Indicators.slides.html#planeación",
    "title": "Indicators",
    "section": "Planeación",
    "text": "Planeación\nComo resultado del proceso de planeación pueden generarse algunos de estos tipos de planes:\n\nObjetivos o metas. Son resultados específicos y medibles.\nEstrategias. Enfoques o planes para lograr objetivos.\nProcedimientos. Detallan la ejecución de una actividad.\nProgramas. Conjuntos organizados de actividades para alcazar objetivos.\nPresupuestos. Planes financieros detallados para facilitar la toma de decisiones económicas.\n\nAlta necesidad de medir progreso"
  },
  {
    "objectID": "Module1/Indicators.slides.html#definición-de-objetivos-smart",
    "href": "Module1/Indicators.slides.html#definición-de-objetivos-smart",
    "title": "Indicators",
    "section": "Definición de Objetivos SMART",
    "text": "Definición de Objetivos SMART\nSpecific: Un objetivo debe ser claro y específico, evitando la ambigüedad. Debe responder a las preguntas qué, quién, cuándo, dónde y por qué.\nMeasurable: Un objetivo debe ser cuantificable o al menos evaluable para determinar el progreso y el éxito. Debe ser posible medirlo con indicadores o criterios tangibles.\nAchievable: Un objetivo debe ser realista y alcanzable, teniendo en cuenta los recursos disponibles, el tiempo y las habilidades necesarias.\nRelevant: Un objetivo debe ser relevante y estar alineado con los objetivos más amplios de la organización o del individuo.\nTime-bound: Un objetivo debe tener un plazo o fecha límite claramente definidos."
  },
  {
    "objectID": "Module1/Indicators.slides.html#ejemplo-1",
    "href": "Module1/Indicators.slides.html#ejemplo-1",
    "title": "Indicators",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nObjetivo: Cumplir con capacitación de personal\nSMART: Cumplir con al menos el 90% del programa de capacitación 2014 para todo el personal operativo de la empresa para el 30 de noviembre del presente año."
  },
  {
    "objectID": "Module1/Indicators.slides.html#ejemplo-2",
    "href": "Module1/Indicators.slides.html#ejemplo-2",
    "title": "Indicators",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nObjetivo: Aumentar las ventas un 20%.\nSMART: Lograr un incremento de ventas del producto X de al menos 17% al termino del primer semestre del 2015, manteniendo una rentabilidad para la empresa de al menos 5%."
  },
  {
    "objectID": "Module1/Indicators.slides.html#necesidad-de-medir",
    "href": "Module1/Indicators.slides.html#necesidad-de-medir",
    "title": "Indicators",
    "section": "Necesidad de medir",
    "text": "Necesidad de medir\n\n\n\nPara poder llevar a cabo los procesos de Planeación, Instrumentación y Control, es necesario contar con un sistema de información que permita evaluar si se están logrando los fines que se planearon, y si las acciones instrumentadas se están llevando también de acuerdo con los planes.\nLa información es necesaria para corregir los planes o su instrumentación, y para producir nuevos planes."
  },
  {
    "objectID": "Module1/Indicators.slides.html#qué-es-un-indicador",
    "href": "Module1/Indicators.slides.html#qué-es-un-indicador",
    "title": "Indicators",
    "section": "¿Qué es un indicador?",
    "text": "¿Qué es un indicador?\n\n“Es el resultado de una medición cuantitativa o cualitativa, o algún otro criterio, mediante el cual se puede evaluar el desempeño, la eficiencia, el logro, etc., de una persona u organización, frecuentemente comparándolo con un estándar o con una meta”.\n\nCollins English Dictionary.\n\n“The qualitative and/or quantitative information on an examined phenomenon (or a process, or a result), which makes it possible to analyze its evolution and to check whether quality targets are met, driving actions and decisions”.\n\nFranceschini, Fiorenzo & Galetto, Maurizio & Maisano, Domenico. (2007). Management by Measurement: Designing Key Indicators and Performance Measurement Systems. 10.1007/978-3-540-73212-9."
  },
  {
    "objectID": "Module1/Indicators.slides.html#ejemplos-de-indicadores",
    "href": "Module1/Indicators.slides.html#ejemplos-de-indicadores",
    "title": "Indicators",
    "section": "Ejemplos de indicadores",
    "text": "Ejemplos de indicadores"
  },
  {
    "objectID": "Module1/Indicators.slides.html#características-de-un-indicador",
    "href": "Module1/Indicators.slides.html#características-de-un-indicador",
    "title": "Indicators",
    "section": "Características de un indicador",
    "text": "Características de un indicador\nFundamentales:\n\nValidez: el indicador debe mostrar fielmente el comportamiento real del fenómeno, variable, resultado, etc., que se desea medir.\nEstabilidad: el indicador debe definirse, calcularse e interpretarse de la misma manera a través del tiempo (permite comparaciones y observar tendencias)."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-1",
    "href": "Module1/Indicators.slides.html#section-1",
    "title": "Indicators",
    "section": "",
    "text": "Ideales:\n\nSimple y fácil de interpretar.\nCapaz de indicar tendencias a través del tiempo.\nSensible a cambios dentro y fuera de la organización.\nFácil recolección y procesamiento de los datos.\nFácil y rápida actualización."
  },
  {
    "objectID": "Module1/Indicators.slides.html#utilidad-de-los-indicadores",
    "href": "Module1/Indicators.slides.html#utilidad-de-los-indicadores",
    "title": "Indicators",
    "section": "Utilidad de los indicadores",
    "text": "Utilidad de los indicadores"
  },
  {
    "objectID": "Module1/Indicators.slides.html#dimensiones-de-análisis",
    "href": "Module1/Indicators.slides.html#dimensiones-de-análisis",
    "title": "Indicators",
    "section": "Dimensiones de análisis",
    "text": "Dimensiones de análisis\n\nLos indicadores y los datos que conducen a ellos, muchas veces se estratifican con respecto a otras variables.\nA las variables que se utilizan como criterios de estratificación se les denomina “Dimensiones de análisis” (son dimensiones desde el punto de vista de hipercubos de datos).\n\nEjemplo: En un proceso de ventas, las ventas mensuales de pueden estratificar por: canal de distribución, región del país, familia de productos, etc., para efectos de su análisis y visualización."
  },
  {
    "objectID": "Module1/Indicators.slides.html#por-ejemplo",
    "href": "Module1/Indicators.slides.html#por-ejemplo",
    "title": "Indicators",
    "section": "Por ejemplo",
    "text": "Por ejemplo"
  },
  {
    "objectID": "Module1/Indicators.slides.html#elección-de-indicadores",
    "href": "Module1/Indicators.slides.html#elección-de-indicadores",
    "title": "Indicators",
    "section": "Elección de indicadores",
    "text": "Elección de indicadores\nLa elección de los indicadores es un factor crítico para que una organización se acerque al cumplimiento de su misión y haga realidad sus estrategias. Los indicadores y las estrategias están inevitablemente vinculados.\n\nUna estrategia sin indicadores es inútil, los indicadores sin una estrategia son irrelevantes!\n\nExisten dos tipos principales de indicadores\n\nAdelantados\nRetrasados"
  },
  {
    "objectID": "Module1/Indicators.slides.html#indicadores-adelantados-y-retrasados-leading-lagging",
    "href": "Module1/Indicators.slides.html#indicadores-adelantados-y-retrasados-leading-lagging",
    "title": "Indicators",
    "section": "Indicadores adelantados y retrasados (leading – lagging)",
    "text": "Indicadores adelantados y retrasados (leading – lagging)\nUn indicador retrasado (lagging) mide el resultado del desempeño al final de un periodo, tiene una orientación hacia el pasado porque nos muestra las consecuencias de lo que ya se hizo. También se conocen como indicadores de resultado.\nUn indicador adelantado (leading) mide el desempeño de los factores que son críticos ahora, para obtener un resultado deseado en el futuro. También se conocen como indicadores de actuación."
  },
  {
    "objectID": "Module1/Indicators.slides.html#indicador-retrasado",
    "href": "Module1/Indicators.slides.html#indicador-retrasado",
    "title": "Indicators",
    "section": "Indicador Retrasado",
    "text": "Indicador Retrasado\nPropósito: Mide el resultado del desempeño al final de un periodo\nEjemplo: Ventas anuales, market share, ROI\n\nVentaja: Son objetivos\nDesventaja: Reflejan el efecto de acciones del pasado."
  },
  {
    "objectID": "Module1/Indicators.slides.html#indicador-adelantado",
    "href": "Module1/Indicators.slides.html#indicador-adelantado",
    "title": "Indicators",
    "section": "Indicador Adelantado",
    "text": "Indicador Adelantado\nPropósito: Mide procesos, actividades, comportamientos\nEjemplo: # de clientes visitados, # de cursos ofrecidos\n\nVentaja: Son predictivos, permiten corregir la estrategia\nDesventaja: Basados en hipótesis causa-efecto."
  },
  {
    "objectID": "Module1/Indicators.slides.html#otros-ejemplos",
    "href": "Module1/Indicators.slides.html#otros-ejemplos",
    "title": "Indicators",
    "section": "Otros Ejemplos",
    "text": "Otros Ejemplos\nEn el contexto de un YouTuber:\n\nIndicador adelantado: Número de vistas de un video de YouTube en las primeras 24 horas.\nIndicador retrasado: Ingreso mensual generado por la monetización.\n\nEn el contexto del departamento de ventas de Netflix:\n\nIndicador adelantado: Número de usuarios que comienzan una prueba gratuita en un mes dado.\nIndicador retrasado: Ingreso mensuales por suscripciones."
  },
  {
    "objectID": "Module1/Indicators.slides.html#otros-ejemplos-1",
    "href": "Module1/Indicators.slides.html#otros-ejemplos-1",
    "title": "Indicators",
    "section": "Otros Ejemplos",
    "text": "Otros Ejemplos\nEn el contexto de una empresa de GenAI:\n\nIndicador adelantado: Número de programadores contratados con expertise en IA.\nIndicador retrasado: Numero de licensias vendidas al año.\n\nEn el contexto de una empresa de mantenimiento de minisplits:\n\nIndicador adelantado: El tiempo promedio de respuesta del servicio técnico a solicitudes de soporte.\nIndicador retrasado: Numero de reseñas positivas en Google Maps en un mes."
  },
  {
    "objectID": "Module1/Indicators.slides.html#cómo-defino-un-indicador",
    "href": "Module1/Indicators.slides.html#cómo-defino-un-indicador",
    "title": "Indicators",
    "section": "¿Cómo defino un indicador?",
    "text": "¿Cómo defino un indicador?\nCriterios de selección:\n\nRelación directa con el objetivo a medir\nFacilidad de comunicación enfocada a la estrategia\nRepetibilidad y confiabilidad\nFrecuencia de actualización\nUtilidad en la fijación de metas\nUtilidad para asignar responsabilidades\nUtilidad para el despliegue hacia abajo"
  },
  {
    "objectID": "Module1/Indicators.slides.html#indicadores-básicos-y-derivados",
    "href": "Module1/Indicators.slides.html#indicadores-básicos-y-derivados",
    "title": "Indicators",
    "section": "Indicadores Básicos y Derivados",
    "text": "Indicadores Básicos y Derivados\nUn indicador básico se obtiene de la medición directa de un fenómeno o hecho. Por ejemplo: Número de pedidos entregados completos y a tiempo en la semana.\nUn indicador derivado combina la información de dos o más indicadores básicos o derivados. Por ejemplo: Porcentaje de pedidos entregados completos y a tiempo en la semana.\nEjemplo: en Análisis de modos de falla y efectos\n\nIndicadores básicos: Severidad, Ocurencia, Detección\nIndicador derivado: Risk Priority Number = SOD"
  },
  {
    "objectID": "Module1/Indicators.slides.html#ejemplo-3",
    "href": "Module1/Indicators.slides.html#ejemplo-3",
    "title": "Indicators",
    "section": "Ejemplo 3",
    "text": "Ejemplo 3\nEn el análisis de modos de falla y efectos, tenemos los siguientes indicadores básicos:\n\nSeveridad (S): Mide el impacto o la gravedad de la falla en caso de que ocurra. Se mide en una escala del 1 al 10, donde 1 indica un efecto insignificante y 10 representa un efecto catastrófico para el usuario o el sistema.\nOcurrencia (O): Evalúa la probabilidad de que la falla ocurra. Se evalua en una escala de 1 al 10, donde 1 indica que la ocurrencia es muy rara y 10 que es altamente probable o frecuente.\nDetección (D): Representa la capacidad del sistema para detectar la falla antes de que llegue al cliente o al usuario final. Se califica en una escala del 1 al 10, donde 1 significa que la detección es casi segura y 10 que es muy difícil o imposible de detectar antes de que ocurra un problema.\n\nIndicador derivado: Número de Prioridad de Riesgo = S\\(\\times\\)O\\(\\times\\)D"
  },
  {
    "objectID": "Module1/Indicators.slides.html#el-formato-de-un-indicador",
    "href": "Module1/Indicators.slides.html#el-formato-de-un-indicador",
    "title": "Indicators",
    "section": "El Formato de un Indicador",
    "text": "El Formato de un Indicador\nUn indicador se debe medir númericamente usando:\nNúmeros absolutos: Resultantes de un proceso de medición o conteo (volumen producido, precio de la acción, número de empleados, costos fijos…)\nTasas: Relación entre dos variables con diferentes unidades (número de unidades / número de operarios, consumo energético / litros producidos…)\nÍndices: Cantidad adimensional que resulta de dividir el valor actual de una variable entre un valor base de referencia de esa variable (índice de precios al consumidor)"
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-2",
    "href": "Module1/Indicators.slides.html#section-2",
    "title": "Indicators",
    "section": "",
    "text": "Proporciones: Relaciones entre dos variables que se miden en las mismas unidades (hombres vs mujeres, admitidos vs rechazados)\nPorcentajes de crecimiento o decrecimiento: (Valor actual – Valor anterior)*100/Valor anterior.\nEvaluaciones: Evaluaciones de una variable cualitativa en una escala ordinal tipo Likert (bajo medio alto, pésimo malo regular bueno excelente)."
  },
  {
    "objectID": "Module1/Indicators.slides.html#actividad-1.1-cooperative-mode",
    "href": "Module1/Indicators.slides.html#actividad-1.1-cooperative-mode",
    "title": "Indicators",
    "section": "Actividad 1.1 (cooperative mode)",
    "text": "Actividad 1.1 (cooperative mode)\nJúntate en equipos de 3 y pregúntale a ChatGPT! Para los siguientes conceptos sugieran al menos un indicador cuantitativo adelantado (leading) y un indicador retrasado (lagging):\n\nProductividad mensual de una línea de producción de muebles\nRotación anual de personal en una empresa de manufactura\nNivel de servicio al cliente de una empresa que fabrica envases de plástico y entrega regionalmente\nRentabilidad del negocio para una empresa mediana mayorista de abarrotes.\nDesempeño del proceso de recaudación de fondos de una asociación de apoyo a niños en situación de calle."
  },
  {
    "objectID": "Module1/Indicators.slides.html#recuerda-que",
    "href": "Module1/Indicators.slides.html#recuerda-que",
    "title": "Indicators",
    "section": "Recuerda que …",
    "text": "Recuerda que …\n\n\nLos propósitos de un indicador son:\n\nEstablecer metas cuantitativas\nMotivación organizacional, inducción de conductas deseables\nEvaluación de la estrategia y aprendizaje estratégico."
  },
  {
    "objectID": "Module1/Indicators.slides.html#cuáles-indicadores-debemos-utilizar",
    "href": "Module1/Indicators.slides.html#cuáles-indicadores-debemos-utilizar",
    "title": "Indicators",
    "section": "¿Cuáles indicadores debemos utilizar?",
    "text": "¿Cuáles indicadores debemos utilizar?\n\nUna vez que conocemos qué son los indicadores, su papel dentro del proceso administrativo, y los tipos de indicadores existen, nos preguntamos ¿Cuántos indicadores debemos tener? ¿Cuál es un conjunto apropiado de indicadores? ¿Cómo los documentamos y compartimos?\nAunque hay algunos indicadores que pudieran aplicarse de manera general a cualquier empresa, cada empresa tiene su propia estrategia, sus propias prioridades, su entorno competitivo particular, por lo tanto, el conjunto más conveniente de indicadores depende de cada organización.\n\nAquí describimos modelos para encontrar el número y conjunto apropiado de indicadores."
  },
  {
    "objectID": "Module1/Indicators.slides.html#estrategia-y-ejecución",
    "href": "Module1/Indicators.slides.html#estrategia-y-ejecución",
    "title": "Indicators",
    "section": "Estrategia y Ejecución",
    "text": "Estrategia y Ejecución\n\n\nLa estructura de planeación y control de la organización provee un marco de trabajo para identificar y estructurar el sistema de indicadores.\nYa que uno de los retos primordiales de cualquier organización consiste en alinear la ejecución con la estrategia, o la estrategia con la ejecución.\nLos sistemas de indicadores para la medición del desempeño deben apoyar a mantener el vínculo entre la estrategia y la ejecución.\n\n\n\n\nhttps://nationalpost.com/news/wal-marts-epic-strategy-fail"
  },
  {
    "objectID": "Module1/Indicators.slides.html#modelos-para-definir-indicadores-1",
    "href": "Module1/Indicators.slides.html#modelos-para-definir-indicadores-1",
    "title": "Indicators",
    "section": "Modelos para Definir Indicadores",
    "text": "Modelos para Definir Indicadores\nLos modelos para definir indicadores dependen del tipo de planeación estratégica.\nTres marcos de planeación estratégica son:\n\nAdministración por Objetivos (APO; Peter Drucker, 1954)\nHoshin Kanri\nBalanced Score Card (BSC; Kaplan & Norton, 1992, 1996)"
  },
  {
    "objectID": "Module1/Indicators.slides.html#balance-score-card-bsc",
    "href": "Module1/Indicators.slides.html#balance-score-card-bsc",
    "title": "Indicators",
    "section": "Balance Score Card (BSC)",
    "text": "Balance Score Card (BSC)\nEs un modelo que ayuda a las organizaciones a traducir la estrategia en objetivos operacionales (medibles), que resulta en acciones, conductas y desempeño.\nBSC incluye todos los factores críticos de éxito en un sistema de medición, que brinda a las organizaciones una mejor posibilidad de alcanzar sus metas."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-3",
    "href": "Module1/Indicators.slides.html#section-3",
    "title": "Indicators",
    "section": "",
    "text": "https://www.youtube.com/watch?v=QCi09LlI7Hs"
  },
  {
    "objectID": "Module1/Indicators.slides.html#objetivos-del-bsc",
    "href": "Module1/Indicators.slides.html#objetivos-del-bsc",
    "title": "Indicators",
    "section": "Objetivos del BSC",
    "text": "Objetivos del BSC\n\nTraducir la estrategia a objetivos medibles.\nAlinear los componentes de la estrategia: objetivos, indicadores e iniciativas.\n\n3.Comunicar la estrategia a la organización.\n\nCrear la base para una administración estratégica."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-4",
    "href": "Module1/Indicators.slides.html#section-4",
    "title": "Indicators",
    "section": "",
    "text": "BSC convierte la estrategia en un sistema integrado definido a través de 4 perspectivas:\n\n\n\nFinanciera\nClientes\nProcesos Internos\nAprendizaje y crecimiento"
  },
  {
    "objectID": "Module1/Indicators.slides.html#las-4-perspectivas",
    "href": "Module1/Indicators.slides.html#las-4-perspectivas",
    "title": "Indicators",
    "section": "Las 4 perspectivas",
    "text": "Las 4 perspectivas\nBSC convierte la estrategia en un sistema integrado definido a través de 4 perspectivas:\n\n\n\nFinanciera. Incluye objetivos relacionados con la rentabilidad, productividad, utilidades, precio de las acciones, etc., son los objetivos que debe lograr la organización desde la perspectiva de los accionistas."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-5",
    "href": "Module1/Indicators.slides.html#section-5",
    "title": "Indicators",
    "section": "",
    "text": "Clientes. Incluye objetivos relacionados con la propuesta de valor de la empresa, están orientados al mercado y se establecen desde la perspectiva de los clientes. Incluye objetivos de percepción de los clientes en cuanto al servicio, tiempo de entrega, calidad, valor/precio."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-6",
    "href": "Module1/Indicators.slides.html#section-6",
    "title": "Indicators",
    "section": "",
    "text": "Procesos Internos. Incluye objetivos relacionados con el desempeño de los procesos que son críticos para cumplir con los objetivos de la perspectiva de clientes. Objetivos de desempeño de la Cadena de Valor primaria del negocio."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-7",
    "href": "Module1/Indicators.slides.html#section-7",
    "title": "Indicators",
    "section": "",
    "text": "Aprendizaje y crecimiento. Son los objetivos relacionados con los habilitadores para lograr los objetivos de las otras perspectivas. Son objetivos de desarrollo de competencias, ambiente laboral, ambiente físico, infraestructura tecnológica, etc."
  },
  {
    "objectID": "Module1/Indicators.slides.html#componentes-del-modelo",
    "href": "Module1/Indicators.slides.html#componentes-del-modelo",
    "title": "Indicators",
    "section": "Componentes del Modelo",
    "text": "Componentes del Modelo\nPara cada perspectiva se debe definir:\n\nUn conjunto pequeño de objetivos estratégicos\nPara cada objetivo una (o más si es indispensable) métrica como indicador de desempeño\nPara cada indicador establecer metas de largo y corto plazo\nIniciativas (programas, proyectos, acciones) para cerrar las brechas entre el desempeño actual y el deseado de acuerdo con las metas."
  },
  {
    "objectID": "Module1/Indicators.slides.html#mapa-estratégico",
    "href": "Module1/Indicators.slides.html#mapa-estratégico",
    "title": "Indicators",
    "section": "Mapa Estratégico",
    "text": "Mapa Estratégico\n\nEl mapa estratégico muestra los objetivos estratégicos dentro de cada perspectiva usando una matriz.\nSe muestran también posibles relaciones causales entre los objetivos mediante flechas.\nSi se revisan los estatutos estratégicos de la organización (Visión y Misión) se pueden incluir Temas Estratégicos que mostrarán si la organización está atendiendo dichos temas de manera explícita en la planeación."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-9",
    "href": "Module1/Indicators.slides.html#section-9",
    "title": "Indicators",
    "section": "",
    "text": "https://www.youtube.com/watch?v=Brv3w1MZpRg&ab_channel=EstrategiaenAcci%C3%B3nconIv%C3%A1nMart%C3%ADnezLima"
  },
  {
    "objectID": "Module1/Indicators.slides.html#actividad-1.2-cooperative-mode",
    "href": "Module1/Indicators.slides.html#actividad-1.2-cooperative-mode",
    "title": "Indicators",
    "section": "Actividad 1.2 (cooperative mode)",
    "text": "Actividad 1.2 (cooperative mode)\n\nJúntate en equipos de 3.\nConstruye un mapa estratégico como parte de la aplicación del modelo Balanced Scorecard.\n\nLee la introducción del mini-caso Muebles Finos MF en CANVAS"
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-12",
    "href": "Module1/Indicators.slides.html#section-12",
    "title": "Indicators",
    "section": "",
    "text": "La actividad consiste en asignar los objetivos de la empresa a las perspectivas del modelo BSC y a las líneas estratégicas del negocio. Una vez que hayan colocado los objetivos en el mapa, vincula los objetivos entre sí, estableciendo relaciones causa-efecto entre los objetivos. Se te pide también seleccionar 3 de los objetivos y proponer al menos un métrico sugerido para cada uno de ellos. Finalmente, escribirás la justificación de las relaciones causales establecidas"
  },
  {
    "objectID": "Module1/Indicators.slides.html#comentarios-finales",
    "href": "Module1/Indicators.slides.html#comentarios-finales",
    "title": "Indicators",
    "section": "Comentarios Finales",
    "text": "Comentarios Finales\n\nUna vez que se ha diseñado el modelo de BSC para la empresa, se procede a desarrollar modelos específicos por áreas funcionales.\nEn las áreas funcionales se trabaja a partir de los objetivos estratégicos, identificando objetivos particulares del área funcional.\nEs más frecuente utilizar indicadores de actuación que de impacto a nivel departamental.\nNo necesariamente se mantienen todos los temas estratégicos en las áreas funcionales, ni necesariamente todas las áreas tienen objetivos en todas las perspectivas del BSC."
  },
  {
    "objectID": "Module1/Indicators.slides.html#section-13",
    "href": "Module1/Indicators.slides.html#section-13",
    "title": "Indicators",
    "section": "",
    "text": "Es indispensable que los gerentes funcionales entiendan perfectamente el BSC de la empresa y la contribución de su área a los objetivos globales.\nEl responsable del despliegue de BSC debe asegurar la congruencia y alineación de los BSC funcionales.\nAlgunos indicadores a nivel organizacional son simplemente agregaciones de los indicadores departamentales."
  },
  {
    "objectID": "Module1/Indicators.slides.html#documentación-de-indicadores-1",
    "href": "Module1/Indicators.slides.html#documentación-de-indicadores-1",
    "title": "Indicators",
    "section": "Documentación de Indicadores",
    "text": "Documentación de Indicadores\nCuando se están seleccionado y/o diseñando los indicadores, es necesario documentar formalmente la definición de cada uno de los indicadores.\nLa documentación es muy útil porque:\n\nAyuda a clarificar el significado de los indicadores.\nFacilita la comunicación entre los usuarios y creadores de los indicadores.\nSirve como referencia futura cuando se revise el sistema."
  },
  {
    "objectID": "Module1/Indicators.slides.html#formato-básico",
    "href": "Module1/Indicators.slides.html#formato-básico",
    "title": "Indicators",
    "section": "Formato Básico",
    "text": "Formato Básico"
  },
  {
    "objectID": "Module1/Indicators.slides.html#return-to-main-page",
    "href": "Module1/Indicators.slides.html#return-to-main-page",
    "title": "Indicators",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#load-the-libraries",
    "href": "Module2/KNearestNeighbours.slides.html#load-the-libraries",
    "title": "K nearest neighbors",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "href": "Module2/KNearestNeighbours.slides.html#k-nearest-neighbors-knn",
    "title": "K nearest neighbors",
    "section": "K-nearest neighbors (KNN)",
    "text": "K-nearest neighbors (KNN)\n\nKNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n\nBasic idea: Predict a new observation using the K closest observations in the training dataset.\n\nTo predict the response for a new observation, KNN uses the K nearest neighbors (observations) in terms of the predictors!\nThe predicted response for the new observation is the most common response among the K nearest neighbors."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "href": "Module2/KNearestNeighbours.slides.html#the-algorithm-has-3-steps",
    "title": "K nearest neighbors",
    "section": "The algorithm has 3 steps:",
    "text": "The algorithm has 3 steps:\n\n\n\nChoose the number of nearest neighbors (K).\nFor a new observation, find the K closest observations in the training data (ignoring the response).\nFor the new observation, the algorithm predicts the value of the most common response among the K nearest observations."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-1",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-2",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "href": "Module2/KNearestNeighbours.slides.html#nearest-neighbour-3",
    "title": "K nearest neighbors",
    "section": "Nearest neighbour",
    "text": "Nearest neighbour\nSuppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.\n\nA new observation arrives, and we don’t know which group it belongs to. If we had chosen \\(K=3\\), then the three nearest neighbors would vote on which group the new observation belongs to."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-1",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-1",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 2 votes for “genuine” and 2 for “fake.” So we classify it as “genius.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-2",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-2",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#banknote-data-3",
    "href": "Module2/KNearestNeighbours.slides.html#banknote-data-3",
    "title": "K nearest neighbors",
    "section": "Banknote data",
    "text": "Banknote data\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K = 3\\), that’s 3 votes for “counterfeit” and 0 for “genuine.” So we classify it as “counterfeit.”\nCloseness is based on Euclidean distance."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#implementation-details",
    "href": "Module2/KNearestNeighbours.slides.html#implementation-details",
    "title": "K nearest neighbors",
    "section": "Implementation Details",
    "text": "Implementation Details\n\nTies\n\nIf there are more than K nearest neighbors, include them all.\nIf there is a tie in the vote, set a rule to break the tie. For example, randomly select the class."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section",
    "href": "Module2/KNearestNeighbours.slides.html#section",
    "title": "K nearest neighbors",
    "section": "",
    "text": "KNN uses the Euclidean distance between points. So it ignores units.\n\nExample: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\nThese people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\nTherefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n\n\nTherefore, as a first step, we must transform the predictors so that they have the same units!"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#standardization",
    "href": "Module2/KNearestNeighbours.slides.html#standardization",
    "title": "K nearest neighbors",
    "section": "Standardization",
    "text": "Standardization\n\nStandardization refers to centering and scaling each numerical predictor individually. This places all predictors on the same scale.\nIn mathematical terms, we standardize a predictor \\(\\mathbf{X}\\) as:\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}},\\]\nwith \\(\\bar{X} = \\sum_{i=1}^n \\frac{x_i}{n}\\)."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#example",
    "href": "Module2/KNearestNeighbours.slides.html#example",
    "title": "K nearest neighbors",
    "section": "Example",
    "text": "Example\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "href": "Module2/KNearestNeighbours.slides.html#create-the-predictor-matrix-and-response-column",
    "title": "K nearest neighbors",
    "section": "Create the predictor matrix and response column",
    "text": "Create the predictor matrix and response column\nLet’s create the predictor matrix or response column\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \n\n# Vector with responses\nY_full = bank_data.filter(['Status'])\n\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "href": "Module2/KNearestNeighbours.slides.html#lets-partition-the-dataset",
    "title": "K nearest neighbors",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\nWe use 70% for training and the rest for validation.\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#standardization-in-python",
    "href": "Module2/KNearestNeighbours.slides.html#standardization-in-python",
    "title": "K nearest neighbors",
    "section": "Standardization in Python",
    "text": "Standardization in Python\n\nTo standardize numeric predictors, we use the StandardScaler() function. We also apply the function to variables using the fit_transform() function.\n\n\nscaler = StandardScaler()\nXs_train = scaler.fit_transform(X_train)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#knn-in-python",
    "href": "Module2/KNearestNeighbours.slides.html#knn-in-python",
    "title": "K nearest neighbors",
    "section": "KNN in Python",
    "text": "KNN in Python\n\nIn Python, we can use the KNeighborsClassifier() and fit() from scikit-learn to train a KNN.\nIn the KNeighborsClassifier function, we can define the number of nearest neighbors using the n_neighbors parameter.\n\n# For example, let's use KNN with three neighbours\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Now, we train the algorithm.\nknn.fit(Xs_train, Y_train)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#evaluation",
    "href": "Module2/KNearestNeighbours.slides.html#evaluation",
    "title": "K nearest neighbors",
    "section": "Evaluation",
    "text": "Evaluation\n\nTo evaluate KNN, we make predictions on the validation data (not used to train the KNN). To do this, we must first perform standardization operations on the predictors in the validation dataset.\n\nXs_valid = scaler.fit_transform(X_valid)\n\n\nNext, we make predictions.\n\nY_pred_knn = knn.predict(Xs_valid)"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#confusion-matrix",
    "href": "Module2/KNearestNeighbours.slides.html#confusion-matrix",
    "title": "K nearest neighbors",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Calcular matriz de confusión.\ncm = confusion_matrix(Y_valid, Y_pred_knn)\n\n# Mostrar matriz de confusión.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "href": "Module2/KNearestNeighbours.slides.html#finding-the-best-value-of-k",
    "title": "K nearest neighbors",
    "section": "Finding the best value of K",
    "text": "Finding the best value of K\nWe can determine the best value of K for the KNN algorithm. To this end, we evaluate the performance of the KNN for different values of \\(K\\) in terms of accuracy on the validation dataset.\n\nbest_k = 1\nbest_accuracy = 0\nk_values = range(1, 50)  # Test k values from 1 to 50\nvalidation_accuracies = []\n\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(Xs_train, Y_train)\n    val_accuracy = accuracy_score(Y_valid, model.predict(Xs_valid))\n    validation_accuracies.append(val_accuracy)\n\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_k = k"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#visualize",
    "href": "Module2/KNearestNeighbours.slides.html#visualize",
    "title": "K nearest neighbors",
    "section": "Visualize",
    "text": "Visualize\nWe can then visualize the accuracy for different values of \\(K\\) using the following graph and code.\n\n\nCode\nplt.figure(figsize=(6.3, 4.3))\nplt.plot(k_values, validation_accuracies, marker=\"o\", linestyle=\"-\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Validation Accuracy\")\nplt.title(\"Choosing the Best k for KNN\")\nplt.show()"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section-1",
    "href": "Module2/KNearestNeighbours.slides.html#section-1",
    "title": "K nearest neighbors",
    "section": "",
    "text": "Finally, we select the best number of nearest neighbors contained in the best_k object.\n\nKNN_final = KNeighborsClassifier(n_neighbors = best_k)\nKNN_final.fit(Xs_train, Y_train)\n\n\nThe accuracy of the best KNN is\n\nY_pred_KNNfinal = KNN_final.predict(Xs_valid)\nvalid_accuracy = accuracy_score(Y_valid, Y_pred_KNNfinal)\nprint(valid_accuracy)\n\n0.9833333333333333"
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#discussion",
    "href": "Module2/KNearestNeighbours.slides.html#discussion",
    "title": "K nearest neighbors",
    "section": "Discussion",
    "text": "Discussion\n\nKNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:\n\nWhen the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (“Lazy learner”).\nIn this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with."
  },
  {
    "objectID": "Module2/KNearestNeighbours.slides.html#section-2",
    "href": "Module2/KNearestNeighbours.slides.html#section-2",
    "title": "K nearest neighbors",
    "section": "",
    "text": "The predictive performance of KNN deteriorates as the number of predictors increases.\nThis is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\nThis is known as the curse of dimensionality.\n\n\n\n\nhttps://aiaspirant.com/curse-of-dimensionality/"
  },
  {
    "objectID": "Module2/Classification.slides.html#agenda",
    "href": "Module2/Classification.slides.html#agenda",
    "title": "Classification Trees",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Module2/Classification.slides.html#load-the-libraries",
    "href": "Module2/Classification.slides.html#load-the-libraries",
    "title": "Classification Trees",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems",
    "href": "Module2/Classification.slides.html#main-data-science-problems",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#main-data-science-problems-1",
    "href": "Module2/Classification.slides.html#main-data-science-problems-1",
    "title": "Classification Trees",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module2/Classification.slides.html#terminology",
    "href": "Module2/Classification.slides.html#terminology",
    "title": "Classification Trees",
    "section": "Terminology",
    "text": "Terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors."
  },
  {
    "objectID": "Module2/Classification.slides.html#section",
    "href": "Module2/Classification.slides.html#section",
    "title": "Classification Trees",
    "section": "",
    "text": "Response:\n\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-algorithms",
    "href": "Module2/Classification.slides.html#classification-algorithms",
    "title": "Classification Trees",
    "section": "Classification algorithms",
    "text": "Classification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n\nTechnically, they predict the probability that the record belongs to the target class."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-1",
    "href": "Module2/Classification.slides.html#section-1",
    "title": "Classification Trees",
    "section": "",
    "text": "Goal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n\n\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-1",
    "href": "Module2/Classification.slides.html#example-1",
    "title": "Classification Trees",
    "section": "Example 1",
    "text": "Example 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n\n\n\n\n\n\n\n\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-2",
    "href": "Module2/Classification.slides.html#section-2",
    "title": "Classification Trees",
    "section": "",
    "text": "Technically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]"
  },
  {
    "objectID": "Module2/Classification.slides.html#bayes-classifier",
    "href": "Module2/Classification.slides.html#bayes-classifier",
    "title": "Classification Trees",
    "section": "Bayes classifier",
    "text": "Bayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-3",
    "href": "Module2/Classification.slides.html#section-3",
    "title": "Classification Trees",
    "section": "",
    "text": "HOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n\nTo overcome this issue, we several standard solutions:\n\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits)."
  },
  {
    "objectID": "Module2/Classification.slides.html#two-datasets",
    "href": "Module2/Classification.slides.html#two-datasets",
    "title": "Classification Trees",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science algorithms needs two data sets:\n\n\nTraining data is data that we use to train or construct the estimated function \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-4",
    "href": "Module2/Classification.slides.html#section-4",
    "title": "Classification Trees",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "Module2/Classification.slides.html#validation-dataset",
    "href": "Module2/Classification.slides.html#validation-dataset",
    "title": "Classification Trees",
    "section": "Validation dataset",
    "text": "Validation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "href": "Module2/Classification.slides.html#example-2-identifying-counterfeit-banknotes",
    "title": "Classification Trees",
    "section": "Example 2: Identifying Counterfeit Banknotes",
    "text": "Example 2: Identifying Counterfeit Banknotes"
  },
  {
    "objectID": "Module2/Classification.slides.html#dataset",
    "href": "Module2/Classification.slides.html#dataset",
    "title": "Classification Trees",
    "section": "Dataset",
    "text": "Dataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "href": "Module2/Classification.slides.html#how-do-we-generate-validation-data",
    "title": "Classification Trees",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "href": "Module2/Classification.slides.html#create-the-predictor-matrix",
    "title": "Classification Trees",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .drop() from pandas. This function drops one or more columns from a data frame. Let’s drop the response column Status and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \nX_full.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\n131.0\n131.1\n9.0\n9.7\n\n\n1\n129.7\n129.7\n8.1\n9.5\n\n\n2\n129.7\n129.7\n8.7\n9.6\n\n\n3\n129.7\n129.6\n7.5\n10.4"
  },
  {
    "objectID": "Module2/Classification.slides.html#create-the-response-column",
    "href": "Module2/Classification.slides.html#create-the-response-column",
    "title": "Classification Trees",
    "section": "Create the response column",
    "text": "Create the response column\nWe use the function .filter() from pandas to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data.filter(['Status'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\n0\ngenuine\n\n\n1\ngenuine\n\n\n2\ngenuine\n\n\n3\ngenuine"
  },
  {
    "objectID": "Module2/Classification.slides.html#set-the-target-category",
    "href": "Module2/Classification.slides.html#set-the-target-category",
    "title": "Classification Trees",
    "section": "Set the target category",
    "text": "Set the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: Status_counterfeit, dtype: int64"
  },
  {
    "objectID": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "href": "Module2/Classification.slides.html#lets-partition-the-dataset",
    "title": "Classification Trees",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-5",
    "href": "Module2/Classification.slides.html#section-5",
    "title": "Classification Trees",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n167\n130.4\n130.1\n9.6\n11.2\n1\n\n\n163\n130.1\n130.2\n11.6\n10.9\n1\n\n\n35\n130.2\n130.2\n9.4\n9.7\n0\n\n\n179\n130.2\n130.4\n8.2\n11.8\n1"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-6",
    "href": "Module2/Classification.slides.html#section-6",
    "title": "Classification Trees",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n19\n130.2\n129.9\n8.6\n10.0\n0\n\n\n86\n129.9\n129.7\n8.7\n9.5\n0\n\n\n156\n129.7\n129.6\n10.3\n11.4\n1\n\n\n87\n129.6\n129.2\n8.4\n10.2\n0\n\n\n149\n129.9\n130.0\n9.9\n12.3\n1"
  },
  {
    "objectID": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "href": "Module2/Classification.slides.html#work-on-your-training-dataset",
    "title": "Classification Trees",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nModel development.\n\nWe will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your model!"
  },
  {
    "objectID": "Module2/Classification.slides.html#decision-tree",
    "href": "Module2/Classification.slides.html#decision-tree",
    "title": "Classification Trees",
    "section": "Decision tree",
    "text": "Decision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique."
  },
  {
    "objectID": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "href": "Module2/Classification.slides.html#basic-idea-of-a-decision-tree",
    "title": "Classification Trees",
    "section": "Basic idea of a decision tree",
    "text": "Basic idea of a decision tree\nStratify or segment the prediction space into several simpler regions."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "href": "Module2/Classification.slides.html#how-do-you-build-a-decision-tree",
    "title": "Classification Trees",
    "section": "How do you build a decision tree?",
    "text": "How do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "href": "Module2/Classification.slides.html#how-do-we-grow-a-tree",
    "title": "Classification Trees",
    "section": "How do we grow a tree?",
    "text": "How do we grow a tree?\n\n\nUsing the CART algorithm!\n\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-21",
    "href": "Module2/Classification.slides.html#section-21",
    "title": "Classification Trees",
    "section": "",
    "text": "We repeat the partitioning process until the terminal nodes have no less than, say, 5 observations."
  },
  {
    "objectID": "Module2/Classification.slides.html#what-is-impurity",
    "href": "Module2/Classification.slides.html#what-is-impurity",
    "title": "Classification Trees",
    "section": "What is impurity?",
    "text": "What is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "href": "Module2/Classification.slides.html#how-do-we-measure-impurity",
    "title": "Classification Trees",
    "section": "How do we measure impurity?",
    "text": "How do we measure impurity?\n\n\n\nThere are three different metrics for impurity:\n\nRisk of misclassification.\nCross entropy.\nGini impurity index.\n\n\n \n\nProportion of elements in a class"
  },
  {
    "objectID": "Module2/Classification.slides.html#pruning-the-tree",
    "href": "Module2/Classification.slides.html#pruning-the-tree",
    "title": "Classification Trees",
    "section": "Pruning the tree",
    "text": "Pruning the tree\nTo avoid overfitting, we pruned some of the tree’s branches. More specifically, we collapsed two internal (non-terminal) nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-24",
    "href": "Module2/Classification.slides.html#section-24",
    "title": "Classification Trees",
    "section": "",
    "text": "To prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThe algorithm has a tuning parameter called \\(\\alpha\\), which places greater weight on the number of tree nodes (or size):\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes."
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python",
    "href": "Module2/Classification.slides.html#in-python",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nIn Python, we can use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a decision tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(max_depth=5, random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameters max_depth of DecisionTreeClassifier() controls the depth of the tree. The parameter random_state allows you to reproduce the same tree in different runs of the Python code."
  },
  {
    "objectID": "Module2/Classification.slides.html#plotting-the-tree",
    "href": "Module2/Classification.slides.html#plotting-the-tree",
    "title": "Classification Trees",
    "section": "Plotting the tree",
    "text": "Plotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#implementation-details",
    "href": "Module2/Classification.slides.html#implementation-details",
    "title": "Classification Trees",
    "section": "Implementation details",
    "text": "Implementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability."
  },
  {
    "objectID": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "href": "Module2/Classification.slides.html#apply-penalty-for-large-trees",
    "title": "Classification Trees",
    "section": "Apply penalty for large trees",
    "text": "Apply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of the \\(\\alpha\\) parameter, which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-26",
    "href": "Module2/Classification.slides.html#section-26",
    "title": "Classification Trees",
    "section": "",
    "text": "The ccp_alphas and impurities objects contain the different values of the \\(\\alpha\\) parameter used, as well as the impurity performance of the generated trees.\nTo train a decision tree using different alpha values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=507134, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, Y_train)\n    clfs.append(clf)\n\nIn the next section, we will evaluate the performance of these decision trees."
  },
  {
    "objectID": "Module2/Classification.slides.html#evaluation",
    "href": "Module2/Classification.slides.html#evaluation",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-27",
    "href": "Module2/Classification.slides.html#section-27",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-28",
    "href": "Module2/Classification.slides.html#section-28",
    "title": "Classification Trees",
    "section": "",
    "text": "The predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571]])"
  },
  {
    "objectID": "Module2/Classification.slides.html#confusion-matrix",
    "href": "Module2/Classification.slides.html#confusion-matrix",
    "title": "Classification Trees",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems."
  },
  {
    "objectID": "Module2/Classification.slides.html#in-python-1",
    "href": "Module2/Classification.slides.html#in-python-1",
    "title": "Classification Trees",
    "section": "In Python",
    "text": "In Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[ 0 32]\n [ 0 28]]"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-29",
    "href": "Module2/Classification.slides.html#section-29",
    "title": "Classification Trees",
    "section": "",
    "text": "We can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/Classification.slides.html#accuracy",
    "href": "Module2/Classification.slides.html#accuracy",
    "title": "Classification Trees",
    "section": "Accuracy",
    "text": "Accuracy\nA simple metric for summarizing the information in the confusion matrix is accuracy. It is the proportion of correct classifications for both classes, out of the total classifications performed.\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.47\n\n\nThe higher the accuracy, the better the performance of the classifier."
  },
  {
    "objectID": "Module2/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "href": "Module2/Classification.slides.html#lets-get-back-to-the-penalized-trees",
    "title": "Classification Trees",
    "section": "Let’s get back to the penalized trees",
    "text": "Let’s get back to the penalized trees\n\nNow, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-30",
    "href": "Module2/Classification.slides.html#section-30",
    "title": "Classification Trees",
    "section": "",
    "text": "We visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#choosing-the-best-tree",
    "href": "Module2/Classification.slides.html#choosing-the-best-tree",
    "title": "Classification Trees",
    "section": "Choosing the best tree",
    "text": "Choosing the best tree\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.01.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.01)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-31",
    "href": "Module2/Classification.slides.html#section-31",
    "title": "Classification Trees",
    "section": "",
    "text": "Once this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, filled=True, rounded=True)\nplt.show()"
  },
  {
    "objectID": "Module2/Classification.slides.html#evaluation-1",
    "href": "Module2/Classification.slides.html#evaluation-1",
    "title": "Classification Trees",
    "section": "Evaluation",
    "text": "Evaluation\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.93"
  },
  {
    "objectID": "Module2/Classification.slides.html#comments",
    "href": "Module2/Classification.slides.html#comments",
    "title": "Classification Trees",
    "section": "Comments",
    "text": "Comments\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading."
  },
  {
    "objectID": "Module2/Classification.slides.html#an-example",
    "href": "Module2/Classification.slides.html#an-example",
    "title": "Classification Trees",
    "section": "An example",
    "text": "An example\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-32",
    "href": "Module2/Classification.slides.html#section-32",
    "title": "Classification Trees",
    "section": "",
    "text": "In other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway."
  },
  {
    "objectID": "Module2/Classification.slides.html#another-example",
    "href": "Module2/Classification.slides.html#another-example",
    "title": "Classification Trees",
    "section": "Another example",
    "text": "Another example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease."
  },
  {
    "objectID": "Module2/Classification.slides.html#classification-specific-metrics",
    "href": "Module2/Classification.slides.html#classification-specific-metrics",
    "title": "Classification Trees",
    "section": "Classification-specific metrics",
    "text": "Classification-specific metrics\n\nTo overcome this limitation of accuracy and error rate, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix."
  },
  {
    "objectID": "Module2/Classification.slides.html#section-33",
    "href": "Module2/Classification.slides.html#section-33",
    "title": "Classification Trees",
    "section": "",
    "text": "Sensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-34",
    "href": "Module2/Classification.slides.html#section-34",
    "title": "Classification Trees",
    "section": "",
    "text": "Precision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-35",
    "href": "Module2/Classification.slides.html#section-35",
    "title": "Classification Trees",
    "section": "",
    "text": "In Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n1.0\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.47"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-36",
    "href": "Module2/Classification.slides.html#section-36",
    "title": "Classification Trees",
    "section": "",
    "text": "Type I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”"
  },
  {
    "objectID": "Module2/Classification.slides.html#section-37",
    "href": "Module2/Classification.slides.html#section-37",
    "title": "Classification Trees",
    "section": "",
    "text": "Unfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n1.0"
  },
  {
    "objectID": "Module2/Classification.slides.html#discussion",
    "href": "Module2/Classification.slides.html#discussion",
    "title": "Classification Trees",
    "section": "Discussion",
    "text": "Discussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error."
  },
  {
    "objectID": "Module2/Classification.slides.html#activity-2.1-classification-and-metrics-solo-mode",
    "href": "Module2/Classification.slides.html#activity-2.1-classification-and-metrics-solo-mode",
    "title": "Classification Trees",
    "section": "Activity 2.1: Classification and Metrics (solo mode)",
    "text": "Activity 2.1: Classification and Metrics (solo mode)\nUsing the data in the “weight-height.csv” table, apply the CART procedure to build a decision tree useful for predicting a person’s sex based on their weight and height.\nIn this example, the predictor variables are continuous, and the predictor variable is binary.\nInterpret the Precision, Accuracy, Sensitivity, and Type 1 Error values for the validation set. If the software doesn’t report them, perform the calculations using the confusion matrix. Use “Female” as the target class. Discuss the effectiveness of the resulting model."
  },
  {
    "objectID": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "href": "Module2/Classification.slides.html#disadvantages-of-decision-trees",
    "title": "Classification Trees",
    "section": "Disadvantages of decision trees",
    "text": "Disadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#agenda",
    "href": "Module3/Autocorrelation.slides.html#agenda",
    "title": "Autocorrelation Models",
    "section": "Agenda",
    "text": "Agenda\n\n\nAutocorrelation\nThe ARIMA model\nThe SARIMA model"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#load-the-libraries",
    "href": "Module3/Autocorrelation.slides.html#load-the-libraries",
    "title": "Autocorrelation Models",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.tools import diff\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn and statsmodels libraries in Python."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#problem-with-linear-regression-models",
    "href": "Module3/Autocorrelation.slides.html#problem-with-linear-regression-models",
    "title": "Autocorrelation Models",
    "section": "Problem with linear regression models",
    "text": "Problem with linear regression models\n\nLinear regression models do not incorporate the dependence between consecutive values in a time series.\nThis is unfortunate because responses recorded over close time periods tend to be correlated. This correlation is called the autocorrelation of the time series.\nAutocorrelation helps us develop a model that can make better predictions of future responses."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#what-is-correlation",
    "href": "Module3/Autocorrelation.slides.html#what-is-correlation",
    "title": "Autocorrelation Models",
    "section": "What is correlation?",
    "text": "What is correlation?\n\n\n\n\n\nIt is a measure of the strength and direction of the linear relationship between two numerical variables.\nSpecifically, it is used to assess the relationship between two sets of observations.\nCorrelation is between \\(-1\\) and 1."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-measure-autocorrelation",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-measure-autocorrelation",
    "title": "Autocorrelation Models",
    "section": "How do we measure autocorrelation?",
    "text": "How do we measure autocorrelation?\n\nThere are two formal tools for measuring the correlation between observations in a time series:\n\n\nThe autocorrelation function.\nThe partial autocorrelation function."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-1",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-1",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-2",
    "href": "Module3/Autocorrelation.slides.html#the-autocorrelation-function-2",
    "title": "Autocorrelation Models",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before. This would be the correlation between these two columns"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-1",
    "href": "Module3/Autocorrelation.slides.html#example-1",
    "title": "Autocorrelation Models",
    "section": "Example 1",
    "text": "Example 1\nLet’s consider again the dataset in the file “Amtrak.xlsx.” The file contains records of Amtrak passenger numbers from January 1991 to March 2004.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\n\nThe autocorrelation function measures the correlation between responses that are separated by a specific number of periods.\nThe autocorrelation function is commonly visualized using a bar chart.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the correlations between observations at different lags."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-plot",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-plot",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation plot",
    "text": "Autocorrelation plot\n\nIn Python, we use the plot_acf function from the statsmodels library.\n\nplt.figure(figsize=(10, 6))\nplot_acf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()\n\nThe lags parameter controls the number of periods for which to compute the autocorrelation function."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#the-resulting-plot",
    "href": "Module3/Autocorrelation.slides.html#the-resulting-plot",
    "title": "Autocorrelation Models",
    "section": "The resulting plot",
    "text": "The resulting plot\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-1",
    "href": "Module3/Autocorrelation.slides.html#section-1",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The autocorrelation plot shows that the responses and those from zero periods ago have a correlation of 1.\nThe autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45.\nThe autocorrelation plot shows that the responses and those from 24 periods ago have a correlation of around 0.5.\n\n\n\n\n\n\n&lt;Figure size 384x384 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-patterns",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-patterns",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation patterns",
    "text": "Autocorrelation patterns\n\n\nA strong autocorrelation (positive or negative) with a lag \\(j\\) greater than 1 and its multiples (\\(2k, 3k, \\ldots\\)) typically reflects a cyclical pattern or seasonality.\nPositive lag-1 autocorrelation describes a series in which consecutive values generally move in the same direction.\nNegative lag-1 autocorrelation reflects oscillations in the series, where high values (generally) are immediately followed by low values and vice versa."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#more-about-the-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#more-about-the-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "More about the autocorrelation function",
    "text": "More about the autocorrelation function\nConsider the problem of predicting the average price of a kilo of avocado this month.\nFor this, we have the average price from last month and the month before that."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-2",
    "href": "Module3/Autocorrelation.slides.html#section-2",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The autocorrelation function for \\(Y_t\\) and \\(Y_{t-2}\\) includes the direct and indirect effect of \\(Y_{t-2}\\) on \\(Y_t\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#partial-autocorrelation-function",
    "href": "Module3/Autocorrelation.slides.html#partial-autocorrelation-function",
    "title": "Autocorrelation Models",
    "section": "Partial autocorrelation function",
    "text": "Partial autocorrelation function\n\nMeasures the correlation between responses that are separated by \\(j\\) periods, excluding correlation due to responses separated by intervening periods."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-3",
    "href": "Module3/Autocorrelation.slides.html#section-3",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "In technical terms, the partial autocorrelation function fits the following linear regression model\n\\[\\hat{Y}_t = \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}\\] Where:\n\n\\(\\hat{Y}_{t}\\) is the predicted response at the current time (\\(t\\)).\n\\(\\hat{\\beta}_1\\) is the direct effect of \\(Y_{t-1}\\) on predicting \\(Y_{t}\\).\n\\(\\hat{\\beta}_2\\) is the direct effect of \\(Y_{t-2}\\) on predicting \\(Y_{t}\\).\n\nThe partial autocorrelation between \\(Y_t\\) and \\(Y_{t-2}\\) is equal to \\(\\hat{\\beta}_2\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-4",
    "href": "Module3/Autocorrelation.slides.html#section-4",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The partial autocorrelation function is visualized using a graph similar to that for autocorrelation.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the partial correlations between observations at different lags.\n\nIn Python, we use the plot_pacf function from statsmodels.\n\nplt.figure(figsize=(10, 6))\nplot_pacf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-5",
    "href": "Module3/Autocorrelation.slides.html#section-5",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The partial autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45. This is the same for the autocorrelation plot.\nThe partial autocorrelation plot shows that the responses and those from two periods ago have a correlation near 0.\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autoregressive-models",
    "href": "Module3/Autocorrelation.slides.html#autoregressive-models",
    "title": "Autocorrelation Models",
    "section": "Autoregressive models",
    "text": "Autoregressive models\nAutoregressive models are a type of linear regression model that directly incorporate the autocorrelation of the time series to predict the current response.\nTheir main characteristic is that the predictors of the current value of the series are its past values.\n\nAn autoregressive model of order 2 has the mathematical form: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}.\\)\nAn order 3 model looks like this: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2} + \\hat{\\beta}_3 Y_{t-3}.\\)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima-models",
    "href": "Module3/Autocorrelation.slides.html#arima-models",
    "title": "Autocorrelation Models",
    "section": "ARIMA models",
    "text": "ARIMA models\n\nA special class of autoregressive models are ARIMA (Autoregressive Integrated Moving Average).\n\nAn ARIMA model consists of three elements:\n\n\nIntegrated operators (integrated).\nAutoregressive terms (autoregressive).\nStochastic terms (moving average)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#integrated-or-differentiatedoperators-i",
    "href": "Module3/Autocorrelation.slides.html#integrated-or-differentiatedoperators-i",
    "title": "Autocorrelation Models",
    "section": "1. Integrated or differentiatedoperators (I)",
    "text": "1. Integrated or differentiatedoperators (I)\n\nThey create a new variable \\(Z_t\\), which equals the difference between the current response and the delayed response by a number of periods or lags.\nThere are three common levels of differentiation:\n\nLevel 0: \\(Z_t = Y_t\\).\nLevel 1: \\(Z_t = Y_t - Y_{t-1}\\).\nLevel 2: \\(Z_t = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-2",
    "href": "Module3/Autocorrelation.slides.html#example-2",
    "title": "Autocorrelation Models",
    "section": "Example 2",
    "text": "Example 2\nWe consider the time series “CanadianWorkHours.xlsx” that contains the average hours worked by a certain group of workers over a certain range of years.\n\nCanadianWorkHours = pd.read_excel('CanadianWorkHours.xlsx')\nCanadianWorkHours.head(4)\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n0\n1966\n37.2\n\n\n1\n1967\n37.0\n\n\n2\n1968\n37.4\n\n\n3\n1969\n37.5"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#creating-a-train-and-a-validation-data",
    "href": "Module3/Autocorrelation.slides.html#creating-a-train-and-a-validation-data",
    "title": "Autocorrelation Models",
    "section": "Creating a train and a validation data",
    "text": "Creating a train and a validation data\nRecall that we would like to train the model on earlier time periods and test it on later ones. To this end, we make the split using the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(CanadianWorkHours) * split_ratio)\n\n# Split the data\nCanadian_train = CanadianWorkHours[:split_point]\nCanadian_validation = CanadianWorkHours[split_point:]\n\nWe use 80% of the time series for training and the rest for validation."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#training-data",
    "href": "Module3/Autocorrelation.slides.html#training-data",
    "title": "Autocorrelation Models",
    "section": "Training data",
    "text": "Training data\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='Year', y='Hours per Week', data = Canadian_train)\nplt.xlabel('Year')\nplt.ylabel('Hours per Week')\nplt.title('Hours per Week Over Time')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-6",
    "href": "Module3/Autocorrelation.slides.html#section-6",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "In statsmodels, we apply the integration operator using the pre-loaded diff() function. The function’s k_diff argument specifies the order or level of the operator.\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Canadian_train['Hours per Week'], k_diff = 1)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-7",
    "href": "Module3/Autocorrelation.slides.html#section-7",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The time series with a level-1 operator looks like this.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_one)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 1')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-8",
    "href": "Module3/Autocorrelation.slides.html#section-8",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "A level-2 operator would work like this.\n\nZ_series_two = diff(Canadian_train['Hours per Week'], k_diff = 2)\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_two)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 2')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-9",
    "href": "Module3/Autocorrelation.slides.html#section-9",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "We see that the level 2 operator is more successful in removing the trend from the original time series."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#comments",
    "href": "Module3/Autocorrelation.slides.html#comments",
    "title": "Autocorrelation Models",
    "section": "Comments",
    "text": "Comments\n\nThe differentiation operator removes or de-trends the time series.\n\nThe level 0 differentiation operator leaves the time series intact.\nThe level 1 differentiation operator removes its linear trend.\nThe level 2 differentiation operator removes its quadratic trend."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-operator-level",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-operator-level",
    "title": "Autocorrelation Models",
    "section": "How do we determine the operator level?",
    "text": "How do we determine the operator level?\n\nVisualizing the time series and determining whether there is a linear or quadratic trend.\nIf level 1 and level 2 operators yield similar results, we choose level 1 because it is simpler.\nOnce this is done, we set our transformed variable \\(Z_t\\) as the new response variable."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autoregressive-ar-terms",
    "href": "Module3/Autocorrelation.slides.html#autoregressive-ar-terms",
    "title": "Autocorrelation Models",
    "section": "2. Autoregressive (AR) terms",
    "text": "2. Autoregressive (AR) terms\n\nHere we use autoregressive models, but with the new response variable \\(Z_t\\).\nWe can have different levels of order (or number of terms) in the autoregression model. For example:\n\nOrder 1 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1}\\).\nOrder 2 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2}\\).\n\nIf necessary, we can exclude the constant coefficient \\(\\hat{\\beta}_0\\) from the model."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-number-of-terms",
    "href": "Module3/Autocorrelation.slides.html#how-do-we-determine-the-number-of-terms",
    "title": "Autocorrelation Models",
    "section": "How do we determine the number of terms?",
    "text": "How do we determine the number of terms?\n\nUsing the correlation functions (ACF) and partial correlation functions (PACF) of the differenced series."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-10",
    "href": "Module3/Autocorrelation.slides.html#section-10",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nTo achieve this, we have some rules."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-1",
    "href": "Module3/Autocorrelation.slides.html#rule-1",
    "title": "Autocorrelation Models",
    "section": "Rule 1",
    "text": "Rule 1\n\nA first-order autoregressive model has:\n\nAn ACF with a single peak at the first period difference (lag).\nAn ACF with exponentially decreasing correlations."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-2",
    "href": "Module3/Autocorrelation.slides.html#rule-2",
    "title": "Autocorrelation Models",
    "section": "Rule 2",
    "text": "Rule 2\n\nA second-order autoregressive model has:\n\nA PACF with two peaks at the first period differences (lags).\nAn ACF with correlations that decrease positively and negatively but approach zero."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#rule-3",
    "href": "Module3/Autocorrelation.slides.html#rule-3",
    "title": "Autocorrelation Models",
    "section": "Rule 3",
    "text": "Rule 3\n\nIf the PACF of the differenced series \\(Z_t\\) shows a higher partial correlation than the others and/or the lag-1 autocorrelation is positive, then consider adding an AR term to the model.\n\nThe lag at which the PACF cuts off from the confidence limits in the software is the indicated number of AR terms."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-11",
    "href": "Module3/Autocorrelation.slides.html#section-11",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Following part 1 of Rule 1, we conclude that an autoregressive term of order 2 will be sufficient to capture the relationships between the elements of the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#stochastic-terms-moving-averages-ma",
    "href": "Module3/Autocorrelation.slides.html#stochastic-terms-moving-averages-ma",
    "title": "Autocorrelation Models",
    "section": "3. Stochastic Terms (Moving Averages, MA)",
    "text": "3. Stochastic Terms (Moving Averages, MA)\nInstead of using past values of the response variable, a moving average model uses stochastic terms to predict the current response. The model has different versions depending on the number of errors used to predict the response. For example:\n\nMA of order 1: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1}\\);\nMA of order 2: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1} + \\theta_2 a_{t-2}\\),\n\nwhere \\(\\theta_0\\) is a constant and \\(a_t\\) are terms from a white noise series (i.e., random terms)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#how-do-i-choose-the-order-of-the-mas",
    "href": "Module3/Autocorrelation.slides.html#how-do-i-choose-the-order-of-the-mas",
    "title": "Autocorrelation Models",
    "section": "How do I choose the order of the MAs?",
    "text": "How do I choose the order of the MAs?\n\nRule 4: MA models have:\n\nCorrelations other than 0 in the ACF. The lags at which this occurs indicate the terms to include in the MA model.\nCorrelations in the PACF that gradually decrease to zero in some way."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-12",
    "href": "Module3/Autocorrelation.slides.html#section-12",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "This means that to determine the order, we focus primarily on the autocorrelation function. Remember, it’s the autocorrelation function of the series after it’s been differentiated.\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-13",
    "href": "Module3/Autocorrelation.slides.html#section-13",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Since there is no significant correlation for any lag above 0, we do not need any MA elements to model the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima",
    "href": "Module3/Autocorrelation.slides.html#arima",
    "title": "Autocorrelation Models",
    "section": "ARIMA",
    "text": "ARIMA\n\n\n\nDefine the response differentiation level and create \\(Z_t\\).\nDefine the order of the AR model (e.g., order 2).\nDefine the order of the MA model (e.g., order 1).\n\n\n\n\n\\[\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2} + \\theta_1 a_{t-1}\\]\n\n\n\nThe ARIMA model coefficients are estimated using an advanced method that takes into account the dependencies between the time series responses."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#arima-in-pyhon",
    "href": "Module3/Autocorrelation.slides.html#arima-in-pyhon",
    "title": "Autocorrelation Models",
    "section": "ARIMA in Pyhon",
    "text": "ARIMA in Pyhon\n\nTo fit an ARIMA model, we use the ARIMA() function from statsmodels.\nThe function has an important argument called order, which equals (p,d,q), where\n\np is the order of the autoregressive model.\nd is the level of the integration or differencing operator.\nq is the number of elements in the moving average."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-14",
    "href": "Module3/Autocorrelation.slides.html#section-14",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "From our previous analysis of the training data for the Canadian workhours example, we conclude that:\n\nWe must use a level-2 differencing operator to remove the quadratic trend from the series. Therefore, d = 2.\nOne autoregressive term should be sufficient to capture the patterns in the time series. Therefore, p = 2.\nIt is not necessary to have moving average terms. Therefore, q = 0."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-15",
    "href": "Module3/Autocorrelation.slides.html#section-15",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Once this is defined, we can train an ARIMA model using the training data with the following code:\n\nArima_Canadian = ARIMA(Canadian_train['Hours per Week'], \n                       order=(2, 2, 0))\nresults_ARIMA_Canadian = Arima_Canadian.fit()\n\nTechnically, ARIMA() defines the model and .fit() fits the model to the data using maximum likelihood estimation."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-16",
    "href": "Module3/Autocorrelation.slides.html#section-16",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "After fitting, we can get a summary of the model fit using the following code.\n\nprint(results_ARIMA_Canadian.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:         Hours per Week   No. Observations:                   28\nModel:                 ARIMA(2, 2, 0)   Log Likelihood                  -2.537\nDate:                Thu, 07 Aug 2025   AIC                             11.074\nTime:                        11:11:45   BIC                             14.849\nSample:                             0   HQIC                            12.161\n                                 - 28                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.3236      0.403     -0.803      0.422      -1.113       0.466\nar.L2         -0.4401      0.250     -1.760      0.078      -0.930       0.050\nsigma2         0.0699      0.021      3.356      0.001       0.029       0.111\n===================================================================================\nLjung-Box (L1) (Q):                   0.26   Jarque-Bera (JB):                 3.02\nProb(Q):                              0.61   Prob(JB):                         0.22\nHeteroskedasticity (H):               1.48   Skew:                             0.74\nProb(H) (two-sided):                  0.57   Kurtosis:                         3.75\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-17",
    "href": "Module3/Autocorrelation.slides.html#section-17",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The next step in evaluating an ARIMA model is to study the model’s residuals to ensure there is nothing else to explain in the model.\n\nWe can obtain the residuals using the following code.\n\nARIMA_residuals = results_ARIMA_Canadian.resid\nARIMA_residuals = ARIMA_residuals.drop(0)"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#time-series-of-residuals",
    "href": "Module3/Autocorrelation.slides.html#time-series-of-residuals",
    "title": "Autocorrelation Models",
    "section": "Time series of residuals",
    "text": "Time series of residuals\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=ARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#correlation-plots",
    "href": "Module3/Autocorrelation.slides.html#correlation-plots",
    "title": "Autocorrelation Models",
    "section": "Correlation plots",
    "text": "Correlation plots"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-18",
    "href": "Module3/Autocorrelation.slides.html#section-18",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The three graphs show no obvious patterns or significant correlations between the residuals. Therefore, we say the model is correct."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#forecast",
    "href": "Module3/Autocorrelation.slides.html#forecast",
    "title": "Autocorrelation Models",
    "section": "Forecast",
    "text": "Forecast\nOnce the model is validated, we make predictions for elements in the time series.\n\nTo predict the average number of hours worked in the next, say, 3 years, we use the .forecast() function. The steps argument indicates the number of steps in the future to make the predictions.\n\nresults_ARIMA_Canadian.forecast(steps = 3)\n\n28    36.102917\n29    36.205233\n30    36.382827\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#model-evaluation-using-mse",
    "href": "Module3/Autocorrelation.slides.html#model-evaluation-using-mse",
    "title": "Autocorrelation Models",
    "section": "Model evaluation using MSE",
    "text": "Model evaluation using MSE\n\nInstead of evaluating the ARIMA model using graphical analyses of the residuals, we can take a more data-driven approach and evaluate the model using the mean squared error (MSE) or root MSE.\nTo this end, we simply use the mean_squared_error() function with the validation responses and our predictions."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#validation-data",
    "href": "Module3/Autocorrelation.slides.html#validation-data",
    "title": "Autocorrelation Models",
    "section": "Validation data",
    "text": "Validation data\n\nCanadian_validation\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n28\n1994\n36.0\n\n\n29\n1995\n35.7\n\n\n30\n1996\n35.7\n\n\n31\n1997\n35.5\n\n\n32\n1998\n35.6\n\n\n33\n1999\n36.3\n\n\n34\n2000\n36.5"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-19",
    "href": "Module3/Autocorrelation.slides.html#section-19",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "The validation data has 7 time periods that can be determined using the command below.\n\nlen(Canadian_validation)\n\n7\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Canadian = results_ARIMA_Canadian.forecast(steps = 7)\nforecast_Canadian\n\n28    36.102917\n29    36.205233\n30    36.382827\n31    36.580331\n32    36.738265\n33    36.900243\n34    37.078325\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-20",
    "href": "Module3/Autocorrelation.slides.html#section-20",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Using the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(mse**(1/2), 2))\n\n0.75\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(rtwo, 2))\n\n-3.52\n\n\n\nA negative signifies that the model’s predictions are worse than simply predicting the average of the response"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonality",
    "href": "Module3/Autocorrelation.slides.html#seasonality",
    "title": "Autocorrelation Models",
    "section": "Seasonality",
    "text": "Seasonality\n\n\nSeasonality consists of repetitive or cyclical behavior that occurs with a constant frequency.\nIt can be identified from the series graph or using the ACF and PACF.\nTo do this, we must have removed the trend."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#example-3",
    "href": "Module3/Autocorrelation.slides.html#example-3",
    "title": "Autocorrelation Models",
    "section": "Example 3",
    "text": "Example 3\nWe use the Airline data containing the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#create-training-and-validation-data",
    "href": "Module3/Autocorrelation.slides.html#create-training-and-validation-data",
    "title": "Autocorrelation Models",
    "section": "Create training and validation data",
    "text": "Create training and validation data\n\nWe use 80% of the time series for training and the rest for validation.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Airline_data) * split_ratio)\n\n# Split the data\nAirline_train = Airline_data[:split_point]\nAirline_validation = Airline_data[split_point:]"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#training-data-1",
    "href": "Module3/Autocorrelation.slides.html#training-data-1",
    "title": "Autocorrelation Models",
    "section": "Training data",
    "text": "Training data\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_train)\nplt.xlabel('T')\nplt.ylabel('Number of passengers')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-22",
    "href": "Module3/Autocorrelation.slides.html#section-22",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "First, let’s use a level-1 operator.\n\nZ_series_one = diff(Airline_train['Number of passengers'], k_diff = 1)\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x=Airline_train['Number of passengers'], y=Z_series_one)\nplt.xlabel('T')\nplt.ylabel('Difference Level 1')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#autocorrelation-plots",
    "href": "Module3/Autocorrelation.slides.html#autocorrelation-plots",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#sarima-model",
    "href": "Module3/Autocorrelation.slides.html#sarima-model",
    "title": "Autocorrelation Models",
    "section": "SARIMA model",
    "text": "SARIMA model\nThe SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an extension of the ARIMA model for modeling seasonality patterns.\nThe SARIMA model has three additional elements for modeling seasonality in time series.\n\nDifferenced or integrated operators (integrated) for seasonality.\nAutoregressive terms (autoregressive) for seasonality.\nStochastic terms or moving averages (moving average) for seasonality."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#notation",
    "href": "Module3/Autocorrelation.slides.html#notation",
    "title": "Autocorrelation Models",
    "section": "Notation",
    "text": "Notation\n\nSeasonality in a time series is a regular pattern of change that repeats over \\(S\\) time periods, where \\(S\\) defines the number of time periods until the pattern repeats again.\nFor example, there is seasonality in monthly data, where high values always tend to occur in some particular months and low values always tend to occur in other particular months.\nIn this case, \\(S=12\\) (months per year) is the length of periodic seasonal behavior. For quarterly data, \\(S=4\\) time periods per year."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonal-differentiation",
    "href": "Module3/Autocorrelation.slides.html#seasonal-differentiation",
    "title": "Autocorrelation Models",
    "section": "Seasonal differentiation",
    "text": "Seasonal differentiation\n\nThis is the difference between a response and a response with a lag that is a multiple of \\(S\\).\nFor example, with monthly data \\(S=12\\),\n\nA level 1 seasonal difference is \\(Y_{t} - Y_{t-12}\\).\nA level 2 seasonal difference is \\((Y_{t-12}) - (Y_{t-12} - Y_{t-24})\\).\n\nSeasonal differencing eliminates the seasonal trend and can also eliminate a type of nonstationarity caused by a seasonal random walk."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#seasonal-ar-and-ma-terms",
    "href": "Module3/Autocorrelation.slides.html#seasonal-ar-and-ma-terms",
    "title": "Autocorrelation Models",
    "section": "Seasonal AR and MA Terms",
    "text": "Seasonal AR and MA Terms\nIn SARIMA, the seasonal AR and MA component terms predict the current response (\\(Y_t\\)) using responses and errors at times with lags that are multiples of \\(S\\).\nFor example, with monthly data \\(S = 12\\),\n\n\nThe first-order seasonal AR model would use \\(Y_{t-12}\\) to predict \\(Y_{t}\\).\nThe second-order seasonal AR model would use \\(Y_{t-12}\\) and \\(Y_{t-24}\\) to predict \\(Y_{t}\\).\nThe first-order seasonal MA model would use the stochastic term \\(a_{t-12}\\) as a predictor.\nThe second-order seasonal MA model would use the stochastic terms \\(a_{t-12}\\) and \\(a_{t-24}\\) as predictors."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-23",
    "href": "Module3/Autocorrelation.slides.html#section-23",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "To fit the SARIMA model, we use the ARIMA() function from statsmodels, but with an additional argument, seasonal_order=(0, 0, 0, 0).\n\nThis is the order (P, D, Q, s) of the seasonal component of the model for the autoregressive parameters, differencing operator levels, moving average parameters, and periodicity.\n\nRecall that the function has the argument order = (p, d, q) where p is the order of the autoregressive model, d is the differencing operator level, and q is the number of elements in the moving average.\nThese arguments capture the detailed information of the time series, while seasonal_order captures the patterns given by seasonality."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-24",
    "href": "Module3/Autocorrelation.slides.html#section-24",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Let’s fit a SARIMA model.\n\nSARIMA_model = ARIMA(Airline_train['Number of passengers'], order=(1, 2, 1), \n                      seasonal_order=(1, 1, 0, 12))\nSARIMA_Airline = SARIMA_model.fit()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#summary-of-fit",
    "href": "Module3/Autocorrelation.slides.html#summary-of-fit",
    "title": "Autocorrelation Models",
    "section": "Summary of fit",
    "text": "Summary of fit\n\nprint(SARIMA_Airline.summary())\n\n                                     SARIMAX Results                                     \n=========================================================================================\nDep. Variable:              Number of passengers   No. Observations:                  115\nModel:             ARIMA(1, 2, 1)x(1, 1, [], 12)   Log Likelihood                -374.241\nDate:                           Thu, 07 Aug 2025   AIC                            756.482\nTime:                                   11:11:47   BIC                            766.943\nSample:                                        0   HQIC                           760.717\n                                           - 115                                         \nCovariance Type:                             opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.1729      0.094     -1.833      0.067      -0.358       0.012\nma.L1         -0.9999     14.439     -0.069      0.945     -29.300      27.300\nar.S.L12      -0.1303      0.084     -1.543      0.123      -0.296       0.035\nsigma2        91.7900   1326.009      0.069      0.945   -2507.139    2690.719\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 3.18\nProb(Q):                              0.98   Prob(JB):                         0.20\nHeteroskedasticity (H):               1.13   Skew:                             0.39\nProb(H) (two-sided):                  0.73   Kurtosis:                         2.64\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#residual-analysis",
    "href": "Module3/Autocorrelation.slides.html#residual-analysis",
    "title": "Autocorrelation Models",
    "section": "Residual analysis",
    "text": "Residual analysis\nWe can have a graphical evaluation of the model’s performance using a residual analysis.\n\nSARIMA_residuals = SARIMA_Airline.resid\nSARIMA_residuals = SARIMA_residuals.drop(0)\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\nsns.lineplot(x=Airline_train['T'], y=SARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-25",
    "href": "Module3/Autocorrelation.slides.html#section-25",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#validation",
    "href": "Module3/Autocorrelation.slides.html#validation",
    "title": "Autocorrelation Models",
    "section": "Validation",
    "text": "Validation\nThe validation data has 29 time periods that can be determined using the command below.\n\nlen(Airline_validation)\n\n29\n\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Airline = SARIMA_Airline.forecast(steps = 29)\nforecast_Airline.head(3)\n\n115    489.635426\n116    428.961078\n117    373.169684\nName: predicted_mean, dtype: float64"
  },
  {
    "objectID": "Module3/Autocorrelation.slides.html#section-26",
    "href": "Module3/Autocorrelation.slides.html#section-26",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Using the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Airline_validation[\"Number of passengers\"], \n                        forecast_Airline)  \nprint(round(mse**(1/2), 2))\n\n26.39\n\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Airline_validation[\"Number of passengers\"], forecast_Airline)  \nprint(round(rtwo, 2))\n\n0.89"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#agenda",
    "href": "Module3/PredictiveModels.slides.html#agenda",
    "title": "Predictive Models and Time Series",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nTime Series\nLinear Regression Model for Time Series"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#load-the-libraries",
    "href": "Module3/PredictiveModels.slides.html#load-the-libraries",
    "title": "Predictive Models and Time Series",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#main-data-science-problems",
    "href": "Module3/PredictiveModels.slides.html#main-data-science-problems",
    "title": "Predictive Models and Time Series",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#main-data-science-problems-1",
    "href": "Module3/PredictiveModels.slides.html#main-data-science-problems-1",
    "title": "Predictive Models and Time Series",
    "section": "Main data science problems",
    "text": "Main data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#regression-problem",
    "href": "Module3/PredictiveModels.slides.html#regression-problem",
    "title": "Predictive Models and Time Series",
    "section": "Regression problem",
    "text": "Regression problem\n\nGoal: Find the best function \\(f(\\mathbf{X})\\) of the predictors \\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) that describes the response \\(Y\\).\nIn mathematical terms, we want to establish the following relationship:\n\\[Y = f(\\mathbf{X}) + \\epsilon\\]\n\nWhere \\(\\epsilon\\) is a natural (random) error."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx",
    "href": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx",
    "title": "Predictive Models and Time Series",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx-1",
    "href": "Module3/PredictiveModels.slides.html#how-to-find-the-shape-of-fx-1",
    "title": "Predictive Models and Time Series",
    "section": "How to find the shape of \\(f(X)\\)?",
    "text": "How to find the shape of \\(f(X)\\)?\n\nUsing training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "href": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx",
    "title": "Predictive Models and Time Series",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "href": "Module3/PredictiveModels.slides.html#how-to-evaluate-the-quality-of-the-candidate-function-hatfx-1",
    "title": "Predictive Models and Time Series",
    "section": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?",
    "text": "How to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#moreover",
    "href": "Module3/PredictiveModels.slides.html#moreover",
    "title": "Predictive Models and Time Series",
    "section": "Moreover…",
    "text": "Moreover…\n\n\n\n\nWe can use test data for a final evaluation of the model.\nTest data is data obtained from the process that generated the training data.\nTest data is independent of the training data."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model",
    "title": "Predictive Models and Time Series",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nA common candidate function for predicting a response is the linear regression model. It has the mathematical form:\n\\[\\hat{Y}_i = \\hat{f}(X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i.\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) associated with a predictor value equal to \\(X_i\\).\nThe values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are called the coefficients of the model."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section",
    "href": "Module3/PredictiveModels.slides.html#section",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "The values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the test data set and the least squares method.\nThis method finds the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the error made by the model \\(\\hat{f}(X_i)\\) when trying to predict the responses of the training set.\n\nTechnically, the method minimizes the following expression\n\n\\[(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_t} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_t} ))^2 \\]\n\nFor the \\(n_t\\) the training data!"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#the-idea-in-two-dimensions",
    "href": "Module3/PredictiveModels.slides.html#the-idea-in-two-dimensions",
    "title": "Predictive Models and Time Series",
    "section": "The idea in two dimensions",
    "text": "The idea in two dimensions"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-1",
    "href": "Module3/PredictiveModels.slides.html#example-1",
    "title": "Predictive Models and Time Series",
    "section": "Example 1",
    "text": "Example 1\n\nWe used the dataset called “Advertising.xlsx” in Canvas.\n\nTV: Money spent on TV ads for a product ($).\nSales: Sales generated from the product ($).\n200 markets\n\n\n# Load the data into Python\nAds_data = pd.read_excel('Advertising.xlsx')"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-1",
    "href": "Module3/PredictiveModels.slides.html#section-1",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Ads_data.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-2",
    "href": "Module3/PredictiveModels.slides.html#section-2",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Now, let’s choose our predictor and response. In the definition of X_full, the double bracket in [] is important because it allows us to have a pandas DataFrame as output. This makes it easier to fit the linear regression model with scikit-learn.\n\n# Chose the predictor.\nX_full = Ads_data.filter(['TV'])\n\n# Set the response.\nY_full = Ads_data.filter(['Sales'])"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#create-training-and-validation-data",
    "href": "Module3/PredictiveModels.slides.html#create-training-and-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Create training and validation data",
    "text": "Create training and validation data\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into a training dataset and a validation dataset. To do this, we use the scikit-learn train_test_split() function.\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.25,\n                                                      random_state = 301655)\n\nWe use 75% of the data for training and the rest for validation."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#fit-a-linear-regression-model-in-python",
    "href": "Module3/PredictiveModels.slides.html#fit-a-linear-regression-model-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Fit a linear regression model in Python",
    "text": "Fit a linear regression model in Python\n\nIn Python, we use the LinearRegression() and fit() functions from the scikit-learn to fit a linear regression model.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-3",
    "href": "Module3/PredictiveModels.slides.html#section-3",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "The following commands allow you to show the estimated coefficients of the model.\n\nprint(\"Coefficients:\", LRmodel.coef_)\n\nCoefficients: [[0.05185463]]\n\n\nWe can also show the estimated intercept.\n\nprint(\"Intercept:\", LRmodel.intercept_)\n\nIntercept: [6.69303889]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 6.69 + 0.051 X_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model-assumptions",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model-assumptions",
    "title": "Predictive Models and Time Series",
    "section": "Linear regression model assumptions",
    "text": "Linear regression model assumptions\n\nTo use the regression model, the model errors \\(e_i = Y_i - \\hat{Y}_i\\) obtained on the training data must meet three conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nThese assumptions are evaluated using a graphical analysis of residuals (model errors)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python",
    "href": "Module3/PredictiveModels.slides.html#in-python",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nTo conduct a residual analysis, we need to obtain the predicted values and residuals of the model first (on the training data).\n\n# Fitted values.\nfitted = LRmodel.predict(X_train)\n\n# Residuals\nresiduals = Y_train - fitted\n\nFor plotting, we put everything together in a pandas dataframe.\n\n# Create a DataFrame for plotting\n#results_df = pd.DataFrame()\n#results_df['predicted'] = list(fitted)\n#results_df['actual'] = list(Y_train)\n#results_df['residual'] = results_df['predicted'] - results_df['actual']\n#results_df = results_df.sort_values(by='residual').reset_index(drop=True)\n#results_df.describe()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-4",
    "href": "Module3/PredictiveModels.slides.html#section-4",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Code\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = fitted, y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Fitted\")\n#plt.xlabel(\"Fitted values\")\n#plt.ylabel(\"Residuals\")\n#plt.show()\n\n\n\n\nCode\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = range(len(residuals)), y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Row Order\")\n#plt.xlabel(\"Observation Order\")\n#plt.ylabel(\"Residuals\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#prediction-error",
    "href": "Module3/PredictiveModels.slides.html#prediction-error",
    "title": "Predictive Models and Time Series",
    "section": "Prediction error",
    "text": "Prediction error\nAfter estimating and validating the linear regression model, we can check the quality of its predictions on unobserved data. That is, on the data in the validation set.\nOne metric for this is the mean prediction error (MSE\\(_v\\)):\n\n\\[\\text{MSE}_v = \\frac{(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_v} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_v} ))^2}{n_v}\\]\n\n\nFor \\(n_v\\), the validation data!\n\nThe smaller \\(\\text{MSE}_v\\), the better the predictions."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-5",
    "href": "Module3/PredictiveModels.slides.html#section-5",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "In practice, the square root of the mean prediction error is used:\n\\[\\text{RMSE}_v = \\sqrt{\\text{MSE}_v}.\\]\nThe advantage of \\(\\text{RMSE}_v\\) is that it can be interpreted as:\n\nThe average variability of a model prediction.\n\nFor example, if \\(\\text{RMSE}_v = 1\\), then a prediction of \\(\\hat{Y} = 5\\) will have an (average) error rate of \\(\\pm 1\\)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-1",
    "href": "Module3/PredictiveModels.slides.html#in-python-1",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\n\nTo evaluate the model’s performance, we use the validation dataset. Specifically, we use the predictor matrix stored in X_valid.\n\nIn Python, we make the prediction using the pre-trained LRmodel.\n\nY_pred = LRmodel.predict(X_valid)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-6",
    "href": "Module3/PredictiveModels.slides.html#section-6",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "To evaluate the model, we use the mean squared error in the Python mse() function. Recall that the responses from the validation dataset are in Y_valid, and the model predictions are in Y_pred.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse, 2))\n\n15.49\n\n\n\nTo obtain the root mean squared error (RMSE), we simply take the square root of the MSE.\n\nprint(round(mse**(1/2), 2))\n\n3.94"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#another-metric-r2",
    "href": "Module3/PredictiveModels.slides.html#another-metric-r2",
    "title": "Predictive Models and Time Series",
    "section": "Another Metric: \\(R^2\\)",
    "text": "Another Metric: \\(R^2\\)\n\nIn the context of Data Science, \\(R^2\\) can be interpreted as the squared correlation between the actual responses and those predicted by the model.\nThe higher the correlation, the better the agreement between the predicted and actual responses.\n\n\nWe compute \\(R^2\\) in Python as follows:\n\nrtwo_sc = r2_score(Y_valid, Y_pred)  # Rsquared\nprint(round(rtwo_sc, 2))\n\n0.33"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode",
    "href": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode",
    "title": "Predictive Models and Time Series",
    "section": "Mini-Activity (cooperative mode)",
    "text": "Mini-Activity (cooperative mode)\n\n\nConsider the Advertising.xlsx dataset in Canvas.\nUse a model to predict Sales that includes the Radio predictor (money spent on radio ads for a product ($)). What is the \\(\\text{RMSE}_v\\)?\nNow, use a model to predict Sales that includes two predictors: TV and Radio. What is the \\(\\text{RMSE}_v\\)?\nWhich model do you prefer?"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#other-candidate-functions",
    "href": "Module3/PredictiveModels.slides.html#other-candidate-functions",
    "title": "Predictive Models and Time Series",
    "section": "Other candidate functions",
    "text": "Other candidate functions\nThe linear regression model is one of the most common models for predicting a response. It is simple and easy to calculate and interpret.\nHowever, it can be limited for very complex problems.\nFor this purpose, there are other, more advanced candidate functions \\(\\hat{f}(X)\\), such as:\n\nK nearest neighbors.\nDecision or regression trees.\nEnsamble methods (bagging and random forest)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#what-is-a-time-series",
    "href": "Module3/PredictiveModels.slides.html#what-is-a-time-series",
    "title": "Predictive Models and Time Series",
    "section": "What is a time series?",
    "text": "What is a time series?\n\n\nIt is a sequence of observations collected at successive time intervals.\nTime series data is commonly used in fields such as finance, economics, weather forecasting, signal processing, and many others.\nAnalyzing time series data helps us understand patterns, trends, and behaviors over time, enabling prediction, anomaly detection, and decision-making."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-2",
    "href": "Module3/PredictiveModels.slides.html#example-2",
    "title": "Predictive Models and Time Series",
    "section": "Example 2",
    "text": "Example 2"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-7",
    "href": "Module3/PredictiveModels.slides.html#section-7",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Technically, a time series is a set of observations about a (discrete) predictor \\(T\\) and a response \\(Y\\).\nObservations of \\(Y\\) are recorded at the moments or times given by the predictor \\(T\\).\nThe special feature of the time series is that the observations of \\(Y\\) are not independent!\n\n\n\n\n\nDay\nT\nTemperature (Y)\n\n\n\n\nMonday\n1\n10\n\n\nTuesday\n2\n12\n\n\nWednesday\n3\n15\n\n\nThursday\n4\n14\n\n\nFriday\n5\n18"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-3-amtrak-data",
    "href": "Module3/PredictiveModels.slides.html#example-3-amtrak-data",
    "title": "Predictive Models and Time Series",
    "section": "Example 3: Amtrak data",
    "text": "Example 3: Amtrak data\n\n\nThe Amtrak train company in the USA collects data on the number of passengers traveling on its trains.\nRecords are available from January 1991 to March 2004.\nThe data is available in “Amtrak.xlsx” on Canvas.\n\n\nAmtrak_data = pd.read_excel('Amtrak.xlsx')"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-8",
    "href": "Module3/PredictiveModels.slides.html#section-8",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Amtrak_data.head()\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#time-series-plot-in-python",
    "href": "Module3/PredictiveModels.slides.html#time-series-plot-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Time series plot in Python",
    "text": "Time series plot in Python\nWe can create a line graph to visualize the evolution of Amtrak train ridership over time using lineplot from seaborn.\n\n\nCode\nplt.figure(figsize=(6, 4))\nsns.lineplot(x='Month', y='Ridership (in 000s)', data = Amtrak_data)\nplt.xlabel('Month')\nplt.ylabel('Ridership')\nplt.title('Amtrak Ridership Over Time')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#informative-series",
    "href": "Module3/PredictiveModels.slides.html#informative-series",
    "title": "Predictive Models and Time Series",
    "section": "Informative Series",
    "text": "Informative Series\n\nAn informative time series is a series that contains patterns that we can use to predict future values of the series.\nThe three possible patterns are:\n\n\nTrend: the series has an increasing/decreasing behavior.\nSeasonality: the series has a repeating cyclical pattern in its values.\nAutocorrelation: the series follows a pattern that can be described by previous values of the series."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-4-airline-data",
    "href": "Module3/PredictiveModels.slides.html#example-4-airline-data",
    "title": "Predictive Models and Time Series",
    "section": "Example 4: Airline data",
    "text": "Example 4: Airline data\n\n\n\n\n\nThis series has an upper trend.\nThis series has cyclical patterns in its values.\nAlthough not immediately visible, we can use the previous values of the series to describe the future ones."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#non-informative-series-white-noise",
    "href": "Module3/PredictiveModels.slides.html#non-informative-series-white-noise",
    "title": "Predictive Models and Time Series",
    "section": "Non-informative series: White noise",
    "text": "Non-informative series: White noise\n\n\n\n\n\nWhite noise is a series whose values, on average, are 0 and have a constant variation.\nIts values are also independent of each other.\nIt is used to describe random or natural error."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#linear-regression-model-1",
    "href": "Module3/PredictiveModels.slides.html#linear-regression-model-1",
    "title": "Predictive Models and Time Series",
    "section": "Linear regression model",
    "text": "Linear regression model\n\nThe linear regression model is useful for capturing patterns in a time series. In this context, the model takes the form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) at time \\(T_i\\)."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#trend",
    "href": "Module3/PredictiveModels.slides.html#trend",
    "title": "Predictive Models and Time Series",
    "section": "Trend",
    "text": "Trend\n\nThe trend of the time series is captured by the value of \\(\\hat{\\beta}_1\\) at\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nIf \\(\\hat{\\beta}_1\\) is positive, the series has an upward trend.\nIf \\(\\hat{\\beta}_1\\) is negative, the series has a downward trend.\n\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the least squares method."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#model-evaluation",
    "href": "Module3/PredictiveModels.slides.html#model-evaluation",
    "title": "Predictive Models and Time Series",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nRemember that the errors of the linear regression model (\\(e_i = Y_i - \\hat{Y}_i\\)) must meet two conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nIn the context of time series, this means that the model errors \\(e_i\\) must behave like white noise that contains no patterns."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#example-3-amtrak-data-cont.",
    "href": "Module3/PredictiveModels.slides.html#example-3-amtrak-data-cont.",
    "title": "Predictive Models and Time Series",
    "section": "Example 3: Amtrak data (cont.)",
    "text": "Example 3: Amtrak data (cont.)\n\nLet’s fit a linear regression model to the ridership data from Amtrak."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#creating-a-train-and-a-validation-data",
    "href": "Module3/PredictiveModels.slides.html#creating-a-train-and-a-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Creating a train and a validation data",
    "text": "Creating a train and a validation data\nIn time series, the order of the data matters because each observation is tied to a specific point in time.\nUnlike typical datasets where observations are independent of one another, time series data follows a sequence where past values influence future ones.\nBecause of this, we cannot randomly split the data using a function like train_test_split(). Doing so might result in a situation where the model learns from future values to predict past ones—which doesn’t make sense and would lead to overly optimistic performance."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-9",
    "href": "Module3/PredictiveModels.slides.html#section-9",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Instead, we want to mimic real-world forecasting: train the model on earlier time periods and test it on later ones.\nTo help us do this properly, we use the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Amtrak_data) * split_ratio)\n\n# Split the data\nAmtrak_train = Amtrak_data[:split_point]\nAmtrak_validation = Amtrak_data[split_point:]\n\nThis code ensures that the training data always comes before the validation data in time, preserving the temporal order. The proportion of data that goes to training is set using split_ratio."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#fit-linear-regression-model",
    "href": "Module3/PredictiveModels.slides.html#fit-linear-regression-model",
    "title": "Predictive Models and Time Series",
    "section": "Fit linear regression model",
    "text": "Fit linear regression model\nWe first set the predictor and response.\n\n# Set predictor.\nX_train = Amtrak_train.filter(['t'])\n\n# Set response.\nY_train = Amtrak_train.filter(['Ridership (in 000s)'])\n\nNext, we fit the model using LinearRegression() and fit() from scikit-learn.\n\n# 1. Create linear regression model.\nLRmodelAmtrak = LinearRegression()\n\n# 2. Fit the model to the training data.\nLRmodelAmtrak.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-11",
    "href": "Module3/PredictiveModels.slides.html#section-11",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s inspect the estimated coefficient for the predictor (time).\n\nprint(LRmodelAmtrak.coef_)\n\n[[-1.2818326]]\n\n\nAnd the intercept.\n\nprint(LRmodelAmtrak.intercept_)\n\n[1810.77686661]\n\n\nThe estimated model then is:\n\\[\\hat{Y}_i = 1810.777 - 1.281 T_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\nWe can validate the model using a residual analysis on the training data. To this end, we first compute the predicted values and residuals of the model.\n\nY_pred = LRmodelAmtrak.predict(X_train)\nresiduals = Y_train - Y_pred"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-12",
    "href": "Module3/PredictiveModels.slides.html#section-12",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Code\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=X_train, y=residuals)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#the-model-is-more-flexible-than-that",
    "href": "Module3/PredictiveModels.slides.html#the-model-is-more-flexible-than-that",
    "title": "Predictive Models and Time Series",
    "section": "The model is more flexible than that",
    "text": "The model is more flexible than that\n\nIf necessary, the linear regression model can be extended to capture quadratic relationships. For this, the model takes the following form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i + \\hat{\\beta}_2 T^{2}_i \\]\n\nWhere \\(T^{2}_i\\) is the squared value of the time index.\n\\(\\hat{\\beta}_2\\) is a term that captures possible curvature in the time series."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-2",
    "href": "Module3/PredictiveModels.slides.html#in-python-2",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nTo include a quadratic term, we must augment our predictor matrix with an additional column. The following code shows how to augment X_full by the square of the Amtrak_data['t'] column. This is done using the pandas .concat() function. The resulting matrix is stored in X_quad.\n\nX_quad = pd.concat([X_train, Amtrak_train['t']**2], axis = 1)\n\nNext, we follow the same steps to fit this model.\n\n# 1. Create linear regression model\nQuadmodelAmtrak = LinearRegression()\n\n# 2. Fit linear regression model\nQuadmodelAmtrak.fit(X_quad, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-13",
    "href": "Module3/PredictiveModels.slides.html#section-13",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "We show the estimated coefficients in Python.\n\nprint(\"Intercept = \", QuadmodelAmtrak.intercept_)\nprint(\"Coefficients = \", QuadmodelAmtrak.coef_)\n\nIntercept =  [1866.84019635]\nCoefficients =  [[-4.64563238  0.03397778]]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 1866.84 - 4.65 T_i + 0.03 T^2_i.\\]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-1",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-1",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\n\n\n\nCode\n# Recuerda usar la misma matriz `X_quad`\nY_pred_quad = QuadmodelAmtrak.predict(X_quad)\nresiduals_quad = Y_train - Y_pred_quad\n\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred_quad, y=residuals_quad, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=Amtrak_train['t'], y=residuals_quad)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#model-evaluation-using-validation-data",
    "href": "Module3/PredictiveModels.slides.html#model-evaluation-using-validation-data",
    "title": "Predictive Models and Time Series",
    "section": "Model evaluation using validation data",
    "text": "Model evaluation using validation data\n\nRemember that another way to evaluate the performance of a model is using the \\(\\text{MSE}_v\\) or \\(\\text{RMSE}_v\\) on the validation data. To this end, we need some Python objects.\n\n# Set predictor.\nX_valid = Amtrak_validation.filter(['t'])\n\n# Set response.\nY_valid = Amtrak_validation.filter(['Ridership (in 000s)'])"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-14",
    "href": "Module3/PredictiveModels.slides.html#section-14",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s compute the \\(\\text{MSE}_v\\) for the linear regression model.\n\nY_val_pred_lin = LRmodelAmtrak.predict(X_valid)\nmse = mean_squared_error(Y_valid, Y_val_pred_lin)  \nprint(round(mse, 2))\n\n62517.47\n\n\nLet’s do the same for the the linear regression model with a quadratic term.\n\nX_quad_valid = pd.concat([X_valid, Amtrak_validation['t']**2], axis = 1)\nY_val_pred_quad = QuadmodelAmtrak.predict(X_quad_valid)\nmse_quad = mean_squared_error(Y_valid, Y_val_pred_quad)  # Mean Squared Error (MSE)\nprint(round(mse_quad, 2))\n\n30271.15\n\n\nWe conclude that the linear model with a quadratic term is better than the linear regression model because the \\(\\text{MSE}_v\\) of the former is smaller than for the latter."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode-1",
    "href": "Module3/PredictiveModels.slides.html#mini-activity-cooperative-mode-1",
    "title": "Predictive Models and Time Series",
    "section": "Mini-Activity (cooperative mode)",
    "text": "Mini-Activity (cooperative mode)\n\nConsider the dataset CanadianWorkHours.xlsx in Canvas.\nSplit the data into training and validation\nVisualize the series in Python. The response variable is Working Hours and the predictor is Year.\nUsing Python, answer the question: Which of the following models best fits the series?\n\n\nLinear trend regression model.\nQuadratic trend regression model.\nExponential trend regression model."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#identifying-heteroskedasticity",
    "href": "Module3/PredictiveModels.slides.html#identifying-heteroskedasticity",
    "title": "Predictive Models and Time Series",
    "section": "Identifying Heteroskedasticity",
    "text": "Identifying Heteroskedasticity\n\nHeteroskedasticity arises when the dispersion of model errors is not constant over time.\nTo see it, let’s go back to the Airline data, which contains the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-15",
    "href": "Module3/PredictiveModels.slides.html#section-15",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Airline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-16",
    "href": "Module3/PredictiveModels.slides.html#section-16",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "For illustrative purposes, we will not split the time series into training and validation datasets.\n\n\nCode\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_data)\nplt.xlabel('Time')\nplt.ylabel('Number of passengers')\nplt.title('Number of passengers across time')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-17",
    "href": "Module3/PredictiveModels.slides.html#section-17",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Let’s fit a linear regression model.\n\n# Set predictor.\nX_full = Airline_data.filter(['T'])\n\n# Set response.\nY_full = Airline_data.filter(['Number of passengers'])\n\n# 1. Create linear regression\nLRmodelAirline = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirline.fit(X_full, Y_full)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-2",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-2",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\nHeteroskedasticity: Dispersion of the residuals increases with the predicted value.\n\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#solution",
    "href": "Module3/PredictiveModels.slides.html#solution",
    "title": "Predictive Models and Time Series",
    "section": "Solution",
    "text": "Solution\n\nIf we identify heteroskedasticity in the regression model errors, we have several transformation options for our original series.\n\nA common transformations to the time series \\(Y_i\\) is the Natural Logarithm\nIf the original time series contains negative values, it can be lagged by adding the negative of its minimum value."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#in-python-3",
    "href": "Module3/PredictiveModels.slides.html#in-python-3",
    "title": "Predictive Models and Time Series",
    "section": "In Python",
    "text": "In Python\nThe easiest way to apply the logarithm in Python is to use the log() function from the numpy library\n\nlog_Y_full = np.log( Y_full )\n\nNow, the response to use is in log_Y_full.\n\nThe steps to fit a linear regression model are similar.\n\n# 1. Create linear regression\nLRmodelAirlineTransformed = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirlineTransformed.fit(X_full, log_Y_full)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-3",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-3",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\n\nWith transformation\n\n\nCode\nY_pred_log = LRmodelAirlineTransformed.predict(X_full)\nresiduals_log = log_Y_full - Y_pred_log\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred_log, y=residuals_log, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWithout transformation\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#what-do-i-do-if-the-transformation-doesnt-work",
    "href": "Module3/PredictiveModels.slides.html#what-do-i-do-if-the-transformation-doesnt-work",
    "title": "Predictive Models and Time Series",
    "section": "What do I do if the transformation doesn’t work?",
    "text": "What do I do if the transformation doesn’t work?\n\n\nIf the log transformation doesn’t significantly reduce heteroskedasticity, there are models for modeling variance called GARCH.\nYou can find literature on these models and their software implementations in a time series textbook such as Time Series Analysis with Applications in R by Cryer and Chan."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#seasonality",
    "href": "Module3/PredictiveModels.slides.html#seasonality",
    "title": "Predictive Models and Time Series",
    "section": "Seasonality",
    "text": "Seasonality\nSeasonality refers to repetitive or cyclical behavior that occurs with a constant frequency.\n\n\n\nExamples:\n\nDemand for winter clothing\nDemand for tourist travel\nAmount of rainfall throughout the year."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#capturing-seasonality",
    "href": "Module3/PredictiveModels.slides.html#capturing-seasonality",
    "title": "Predictive Models and Time Series",
    "section": "Capturing seasonality",
    "text": "Capturing seasonality\n\n\n\nThe linear regression model can be extended to capture seasonal patterns in the time series.\nTo do this, an additional categorical predictor is created that indicates the season to which each data item belongs.\nBehind the scenes, the additional categorical predictor is transformed into several auxiliary numerical predictors."
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#analyzing-seasonal-series-in-python",
    "href": "Module3/PredictiveModels.slides.html#analyzing-seasonal-series-in-python",
    "title": "Predictive Models and Time Series",
    "section": "Analyzing seasonal series in Python",
    "text": "Analyzing seasonal series in Python\nConsider the data in Amtrak_train with the additional predictor of Season to model seasonality.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-18",
    "href": "Module3/PredictiveModels.slides.html#section-18",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "To fit a linear regression model with a categorical variable like Season, we must transform the text categories into numbers. To do this, we use dummy variables constructed using the following commands.\n\ndummy_data = pd.get_dummies(Amtrak_train['Season'], dtype = 'int')\ndummy_data.head(4)\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-19",
    "href": "Module3/PredictiveModels.slides.html#section-19",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Simply put, the matrix above contains one column for each month. Each column indicates the observations that belong to the month of the column. For example, the column Apr has the values 0 and 1. The value 1 indicates that the corresponding observation belongs to the month of April. A 0 indicates otherwise.\n\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-20",
    "href": "Module3/PredictiveModels.slides.html#section-20",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Unfortunately, we cannot use the matrix as is in the linear regression model. This is due to multicollinearity issues. Technically, this happens because if you add all the columns, the resulting column is a column of 1s, which is already used by the intercept. Therefore, you cannot fit a model with the intercept and all the columns of the dummy variables.\n\nTo solve this problem, we arbitrarily remove a column from the matrix above. For example, let’s remove Dec.\n\ndummy_data = dummy_data.drop([\"Dec\"], axis = 1)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#section-21",
    "href": "Module3/PredictiveModels.slides.html#section-21",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Now, let’s build the complete matrix of predictors, including the column for time, time squared, and the dummy variables.\n\nX_quad_season = pd.concat([Amtrak_train['t'], Amtrak_train['t']**2, dummy_data], \n                          axis = 1)\n\n\nNext, we fit the model with all the terms in the matrix above.\n\n# 0. Asegurarnos que tenemos la respuesta del problema en `Y_full`.\nY_train = Amtrak_train['Ridership (in 000s)']\n\n# 1. Crear modelo de regresión lineal\nSeasonmodelAmtrak = LinearRegression()\n\n# 2. Ajustar el modelo usando los datos de entrenamiento\nSeasonmodelAmtrak.fit(X_quad_season, Y_train)"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#estimated-model-coefficients",
    "href": "Module3/PredictiveModels.slides.html#estimated-model-coefficients",
    "title": "Predictive Models and Time Series",
    "section": "Estimated model coefficients",
    "text": "Estimated model coefficients\n\n\nprint(\"Intercept = \", SeasonmodelAmtrak.intercept_)\nprint(\"Coefficients = \", SeasonmodelAmtrak.coef_)\n\nIntercept =  1924.3205889003025\nCoefficients =  [-6.12171259e+00  4.96085283e-02  1.22309969e+01  1.77720172e+02\n -2.83088245e+02 -2.25659158e+02  1.15685204e+02 -7.31735621e+00\n  1.59876604e+01  5.17037414e+01 -4.77796001e+01 -3.19949172e+01\n -1.45366826e+02]"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#residual-analysis-4",
    "href": "Module3/PredictiveModels.slides.html#residual-analysis-4",
    "title": "Predictive Models and Time Series",
    "section": "Residual analysis",
    "text": "Residual analysis\n\n\nCode\nY_pred = SeasonmodelAmtrak.predict(X_quad_season)\nresiduals = Y_train - Y_pred\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=Amtrak_data['t'], y=residuals)\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#predictions-on-the-validation-dataset",
    "href": "Module3/PredictiveModels.slides.html#predictions-on-the-validation-dataset",
    "title": "Predictive Models and Time Series",
    "section": "Predictions on the validation dataset",
    "text": "Predictions on the validation dataset\nPrepare the validation data using dummy variables.\n\nY_valid = Amtrak_validation['Ridership (in 000s)']\n\ndummy_valid = pd.get_dummies(Amtrak_validation['Season'], dtype = 'int')\ndummy_valid = dummy_valid.drop([\"Dec\"], axis = 1)\n\nX_qs_valid = pd.concat([Amtrak_validation['t'], Amtrak_validation['t']**2, \n                        dummy_valid], axis = 1)\n\n\nNow, we compute the validation \\(\\text{MSE}_v\\).\n\nY_pred_valid = SeasonmodelAmtrak.predict(X_qs_valid)\n\nmse_season = mean_squared_error(Y_valid, Y_pred_valid) \nprint(round(mse_season, 2))\n\n4890.5"
  },
  {
    "objectID": "Module3/PredictiveModels.slides.html#disadvantages-of-linear-regression-models",
    "href": "Module3/PredictiveModels.slides.html#disadvantages-of-linear-regression-models",
    "title": "Predictive Models and Time Series",
    "section": "Disadvantages of linear regression models",
    "text": "Disadvantages of linear regression models\n\nDespite their simplicity and versatility, linear regression models are not the best for describing a time series.\nThis is because they do not assume a dependency between consecutive values in the time series. That is, they do not use the fact that, for example, \\(Y_1\\) can help us predict \\(Y_2\\), and \\(Y_2\\) can help us predict \\(Y_3\\), etc.\nModels that help us use past observations to predict future values of the response variable \\(Y\\) are autoregressive models."
  },
  {
    "objectID": "Module4/Clustering.slides.html#agenda",
    "href": "Module4/Clustering.slides.html#agenda",
    "title": "Clustering Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nUnsupervised Learning\nClustering Methods\nK-Means Method\nHierarchical Clustering"
  },
  {
    "objectID": "Module4/Clustering.slides.html#load-the-libraries",
    "href": "Module4/Clustering.slides.html#load-the-libraries",
    "title": "Clustering Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn, and scipy libraries in Python."
  },
  {
    "objectID": "Module4/Clustering.slides.html#types-of-learning",
    "href": "Module4/Clustering.slides.html#types-of-learning",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/Clustering.slides.html#types-of-learning-1",
    "href": "Module4/Clustering.slides.html#types-of-learning-1",
    "title": "Clustering Methods",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-1",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nGoal: organize or group data to gain insights. It answers questions like these\n\nIs there an informative way to visualize the data?\nCan we discover subgroups among variables or observations?\n\n\nUnsupervised learning is more challenging than supervised learning because it is subjective and there is no simple objective for the analysis, such as predicting a response.\n\n\nIt is also known as exploratory data analysis."
  },
  {
    "objectID": "Module4/Clustering.slides.html#examples-of-unsupervised-learning",
    "href": "Module4/Clustering.slides.html#examples-of-unsupervised-learning",
    "title": "Clustering Methods",
    "section": "Examples of Unsupervised Learning",
    "text": "Examples of Unsupervised Learning\n\n\nMarketing. Identify a segment of customers with a high tendency to purchase a specific product.\nRetail. Group customers based on their preferences, style, clothing choices, and store preferences.\nMedical Science. Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.\nSociology. Classify people based on their demographics, lifestyle, socioeconomic status, etc."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-methods",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-methods",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.slides.html#unsupervised-learning-methods-1",
    "href": "Module4/Clustering.slides.html#unsupervised-learning-methods-1",
    "title": "Clustering Methods",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.slides.html#clustering-methods-1",
    "href": "Module4/Clustering.slides.html#clustering-methods-1",
    "title": "Clustering Methods",
    "section": "Clustering methods",
    "text": "Clustering methods\nThey group data in different ways to discover groups with common traits."
  },
  {
    "objectID": "Module4/Clustering.slides.html#clustering-methods-2",
    "href": "Module4/Clustering.slides.html#clustering-methods-2",
    "title": "Clustering Methods",
    "section": "Clustering methods",
    "text": "Clustering methods\n\nTwo classic clustering methods are:\n\nK-Means Method. We seek to divide the observations into K groups.\nHierarchical Clustering. We divide the n observations into 1 group, 2 groups, 3 groups, …, up to n groups. We visualize the divisions using a graph called a dendrogram."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-1",
    "href": "Module4/Clustering.slides.html#example-1",
    "title": "Clustering Methods",
    "section": "Example 1",
    "text": "Example 1\nThe “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:\n\n\n\n\nBill length in millimeters.\nBill depth in millimeters.\nFlipper length in millimeters.\nBody mass in grams."
  },
  {
    "objectID": "Module4/Clustering.slides.html#data",
    "href": "Module4/Clustering.slides.html#data",
    "title": "Clustering Methods",
    "section": "Data",
    "text": "Data\n\npenguins_data = pd.read_excel(\"penguins.xlsx\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "Module4/Clustering.slides.html#data-visualization",
    "href": "Module4/Clustering.slides.html#data-visualization",
    "title": "Clustering Methods",
    "section": "Data visualization",
    "text": "Data visualization\nCan we group penguins based on their characteristics?\n\n\nCode\nplt.figure(figsize=(8, 5)) # Set figure size.\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\") # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.slides.html#the-k-means-method",
    "href": "Module4/Clustering.slides.html#the-k-means-method",
    "title": "Clustering Methods",
    "section": "The K-Means method",
    "text": "The K-Means method\n\nGoal: Find K groups of observations such that each observation is in a different group."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section",
    "href": "Module4/Clustering.slides.html#section",
    "title": "Clustering Methods",
    "section": "",
    "text": "For this, the method requires two elements:\n\n\nA measure of “closeness” between observations.\nAn algorithm that groups observations that are close to each other.\n\n\n\nGood clustering is one in which observations within a group are close together and observations in different groups are far apart."
  },
  {
    "objectID": "Module4/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "href": "Module4/Clustering.slides.html#how-do-we-measure-the-distance-between-observations",
    "title": "Clustering Methods",
    "section": "How do we measure the distance between observations?",
    "text": "How do we measure the distance between observations?\nFor quantitative predictors, we use the Euclidean distance.\nFor example, if we have two predictors \\(X_1\\) and \\(X_2\\) with observations given in the table:\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#euclidean-distance",
    "href": "Module4/Clustering.slides.html#euclidean-distance",
    "title": "Clustering Methods",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\n\n\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }\\]"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-1",
    "href": "Module4/Clustering.slides.html#section-1",
    "title": "Clustering Methods",
    "section": "",
    "text": "We can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, with 3 predictors we have\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\\(X_{1,3}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\\(X_{2,3}\\)\n\n\n\n\nWhere the Euclidean distance is\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }\\]"
  },
  {
    "objectID": "Module4/Clustering.slides.html#problem-with-euclidean-distance",
    "href": "Module4/Clustering.slides.html#problem-with-euclidean-distance",
    "title": "Clustering Methods",
    "section": "Problem with Euclidean distance",
    "text": "Problem with Euclidean distance\n\n\nThe Euclidean distance depends on the units of measurement of the predictors!\nPredictors with certain units have greater importance in calculating the distance.\nThis is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.\nThe solution is to standardize the units of the predictors."
  },
  {
    "objectID": "Module4/Clustering.slides.html#k-means-algorithm",
    "href": "Module4/Clustering.slides.html#k-means-algorithm",
    "title": "Clustering Methods",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\n\n\n\nChoose a value for K, the number of groups.\n\nRandomly assign observations to one of the K groups.\nFind the centroids (average points) of each group.\nReassign observations to the group with the closest centroid.\nRepeat steps 3 and 4 until there are no more changes."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-1-cont.",
    "href": "Module4/Clustering.slides.html#example-1-cont.",
    "title": "Clustering Methods",
    "section": "Example 1 (cont.)",
    "text": "Example 1 (cont.)\nLet’s apply the algorithm to the predictors bill_depth_mm and bill_length_mm of the penguins dataset.\n\n\nCode\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm'])\nX_penguins.head()\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\n\n\n\n\n0\n18.7\n39.1\n\n\n1\n17.4\n39.5\n\n\n2\n18.0\n40.3\n\n\n3\n19.3\n36.7\n\n\n4\n20.6\n39.3"
  },
  {
    "objectID": "Module4/Clustering.slides.html#standarization",
    "href": "Module4/Clustering.slides.html#standarization",
    "title": "Clustering Methods",
    "section": "Standarization",
    "text": "Standarization\nSince the K-means algorithm works with Euclidean distance, we must standardize the predictors before we start. In this way, all of them will be equally informative in the process.\n\n\nCode\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-2",
    "href": "Module4/Clustering.slides.html#section-2",
    "title": "Clustering Methods",
    "section": "",
    "text": "In Python, we use the KMeans() function of sklearn to apply K-means clustering. KMeans() tells Python we want to train a K-means clustering algorithm and .fit_predict() actually trains it using the data.\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\nThe argument n_clusters sets the desired number of clusters and random_state allows us to reproduce the analysis."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-3",
    "href": "Module4/Clustering.slides.html#section-3",
    "title": "Clustering Methods",
    "section": "",
    "text": "The clusters created are contained in the clusters object.\n\n\nCode\nclusters\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n       0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2], dtype=int32)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-4",
    "href": "Module4/Clustering.slides.html#section-4",
    "title": "Clustering Methods",
    "section": "",
    "text": "To visualize the clusters, we augment the original dataset X_penguins (without standarization) with the clusters object. usign the code below.\n\nclustered_penguins = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\nclustered_penguins.head()\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\nCluster\n\n\n\n\n0\n18.7\n39.1\n1\n\n\n1\n17.4\n39.5\n1\n\n\n2\n18.0\n40.3\n1\n\n\n3\n19.3\n36.7\n1\n\n\n4\n20.6\n39.3\n1"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-5",
    "href": "Module4/Clustering.slides.html#section-5",
    "title": "Clustering Methods",
    "section": "",
    "text": "Code\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data = clustered_penguins, x = 'bill_length_mm', y = 'bill_depth_mm', \n                hue = 'Cluster', palette = 'Set1')\nplt.title('K-means Clustering of Penguins')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "href": "Module4/Clustering.slides.html#the-truth-3-groups-of-penguins",
    "title": "Clustering Methods",
    "section": "The truth: 3 groups of penguins",
    "text": "The truth: 3 groups of penguins\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\",\n                hue=\"species\", palette = 'Set1') # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.slides.html#lets-try-using-more-predictors",
    "href": "Module4/Clustering.slides.html#lets-try-using-more-predictors",
    "title": "Clustering Methods",
    "section": "Let’s try using more predictors",
    "text": "Let’s try using more predictors\n\n\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', \n                          'flipper_length_mm', 'body_mass_g'])\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\n# Save new clusters into the original data\nclustered_X = (X_penguins\n              .assign(Cluster = clusters)\n              )"
  },
  {
    "objectID": "Module4/Clustering.slides.html#these-are-the-three-species",
    "href": "Module4/Clustering.slides.html#these-are-the-three-species",
    "title": "Clustering Methods",
    "section": "These are the three species",
    "text": "These are the three species\n\n\n\nAdelie\n\n\nGentoo\n\n\nChinstrap"
  },
  {
    "objectID": "Module4/Clustering.slides.html#determining-the-number-of-clusters",
    "href": "Module4/Clustering.slides.html#determining-the-number-of-clusters",
    "title": "Clustering Methods",
    "section": "Determining the number of clusters",
    "text": "Determining the number of clusters\n\nA simple way to determine the number of clusters is recording the quality of clustering for different numbers of clusters.\nIn sklearn, we can record the inertia of a partition into clusters. Technically, the inertia is the sum of squared distances of observations to their closest cluster center.\nThe lower the intertia the better because this means that all observations are close to their cluster centers overall."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-8",
    "href": "Module4/Clustering.slides.html#section-8",
    "title": "Clustering Methods",
    "section": "",
    "text": "To record the intertias for different numbers of clusters, we use the code below.\n\ninertias = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(Xs_penguins)\n    inertias.append(kmeans.inertia_)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-9",
    "href": "Module4/Clustering.slides.html#section-9",
    "title": "Clustering Methods",
    "section": "",
    "text": "Next, we plot the intertias and look for the elbow in the plot.\nThe elbow represents a number of clusters for which there is no significant improvement in the quality of the clustering.\nIn this case, the number of clusters recommended by this elbow method is 3.\n\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#comments",
    "href": "Module4/Clustering.slides.html#comments",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\nSelecting the number of clusters K is more of an art than a science. You’d better get K right, or you’ll be detecting patterns where none really exist.\nWe need to standardize all predictors.\nThe performance of K-means clustering is affected by the presence of outliers.\nThe algorithm’s solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported."
  },
  {
    "objectID": "Module4/Clustering.slides.html#hierarchical-clustering-1",
    "href": "Module4/Clustering.slides.html#hierarchical-clustering-1",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\n\n\n\n\n\nStart with each observation standing alone in its own group.\nThen, gradually merge the groups that are close together.\nContinue this process until all the observations are in one large group.\nFinally, step back and see which grouping works best."
  },
  {
    "objectID": "Module4/Clustering.slides.html#essential-elements",
    "href": "Module4/Clustering.slides.html#essential-elements",
    "title": "Clustering Methods",
    "section": "Essential elements",
    "text": "Essential elements\n\n\n\nDistance between two observations.\n\nWe use Euclidean distance.\nWe must standardize the predictors!\n\nDistance between two groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#distance-between-two-groups",
    "href": "Module4/Clustering.slides.html#distance-between-two-groups",
    "title": "Clustering Methods",
    "section": "Distance between two groups",
    "text": "Distance between two groups\n\n\n\n\nThe distance between two groups of observations is called linkage.\nThere are several types of linking. The most commonly used are:\n\nComplete linkage\nAverage linkage"
  },
  {
    "objectID": "Module4/Clustering.slides.html#complete-linkage",
    "href": "Module4/Clustering.slides.html#complete-linkage",
    "title": "Clustering Methods",
    "section": "Complete linkage",
    "text": "Complete linkage\nThe distance between groups is measured using the largest distance between observations."
  },
  {
    "objectID": "Module4/Clustering.slides.html#average-linkage",
    "href": "Module4/Clustering.slides.html#average-linkage",
    "title": "Clustering Methods",
    "section": "Average linkage",
    "text": "Average linkage\nThe distance between groups is the average of all the distances between observations."
  },
  {
    "objectID": "Module4/Clustering.slides.html#hierarchical-clustering-algorithm",
    "href": "Module4/Clustering.slides.html#hierarchical-clustering-algorithm",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering algorithm",
    "text": "Hierarchical clustering algorithm\n\nThe steps of the algorithm are as follows:\n\n\nAssign each observation to a cluster.\nMeasure the linkage between all clusters.\nMerge the two most similar clusters.\nThen, merge the next two most similar clusters.\nContinue until all clusters have been merged."
  },
  {
    "objectID": "Module4/Clustering.slides.html#example-2",
    "href": "Module4/Clustering.slides.html#example-2",
    "title": "Clustering Methods",
    "section": "Example 2",
    "text": "Example 2\n\nLet’s consider a dataset called “Cereals.xlsx.” The data includes nutritional information for 77 cereals, among other data.\n\ncereal_data = pd.read_excel(\"cereals.xlsx\")"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-10",
    "href": "Module4/Clustering.slides.html#section-10",
    "title": "Clustering Methods",
    "section": "",
    "text": "Here, we will restrict to 7 numeric predictors\n\nX_cereal = cereal_data.filter(['calories', 'protein', 'fat', 'sodium', 'fiber',\n                              'carbo', 'sugars', 'potass', 'vitamins'])\nX_cereal.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25"
  },
  {
    "objectID": "Module4/Clustering.slides.html#do-not-forget-to-standardize",
    "href": "Module4/Clustering.slides.html#do-not-forget-to-standardize",
    "title": "Clustering Methods",
    "section": "Do not forget to standardize",
    "text": "Do not forget to standardize\n\nSince the hierarchical clustering algorithm also works with distances, we must standardize the predictors to have an accurate analysis.\n\nscaler = StandardScaler()\nXs_cereal = scaler.fit_transform(X_cereal)"
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-11",
    "href": "Module4/Clustering.slides.html#section-11",
    "title": "Clustering Methods",
    "section": "",
    "text": "Unfortunately, the Agglomerative() function in sklearn is not as user friendly compared to other available functions in Python. In particular, the scipy library has a function called linkage() for hierarchical clustering that works as follows.\n\nClust_Cereal = linkage(Xs_cereal, method = 'complete')\n\nThe argument method sets the type of linkage to be used."
  },
  {
    "objectID": "Module4/Clustering.slides.html#results-dendrogram",
    "href": "Module4/Clustering.slides.html#results-dendrogram",
    "title": "Clustering Methods",
    "section": "Results: Dendrogram",
    "text": "Results: Dendrogram\n\n\n\n\n\n\nA dendrogram is a tree diagram that summarizes and visualizes the clustering process.\nObservations are on the horizontal axis and at the bottom of the diagram.\nThe vertical axis shows the distance between groups.\nIt is read from top to bottom."
  },
  {
    "objectID": "Module4/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "href": "Module4/Clustering.slides.html#what-to-do-with-a-dendrogram",
    "title": "Clustering Methods",
    "section": "What to do with a dendrogram?",
    "text": "What to do with a dendrogram?\n\n\n\n\nWe draw a horizontal line at a specific height to define the groups.\nThis line defines three groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-12",
    "href": "Module4/Clustering.slides.html#section-12",
    "title": "Clustering Methods",
    "section": "",
    "text": "This line defines 5 groups."
  },
  {
    "objectID": "Module4/Clustering.slides.html#dendrogram-in-python",
    "href": "Module4/Clustering.slides.html#dendrogram-in-python",
    "title": "Clustering Methods",
    "section": "Dendrogram in Python",
    "text": "Dendrogram in Python\nTo produce a nice dendrogram in Python, we use the function dendrogram from scipy.\n\n\nCode\nplt.figure(figsize=(8, 4))\ndendrogram(Clust_Cereal, color_threshold=None)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/Clustering.slides.html#comments-1",
    "href": "Module4/Clustering.slides.html#comments-1",
    "title": "Clustering Methods",
    "section": "Comments",
    "text": "Comments\n\n\nRemember to standardize the predictors!\nIt’s not easy to choose the correct number of clusters using the dendrogram.\nThe results depend on the linkage measure used.\n\nComplete linkage results in narrower clusters.\nAverage linkage strikes a balance between narrow and thinner clusters.\n\nHierarchical clustering is useful for detecting outliers."
  },
  {
    "objectID": "Module4/Clustering.slides.html#section-13",
    "href": "Module4/Clustering.slides.html#section-13",
    "title": "Clustering Methods",
    "section": "",
    "text": "With these methods, there is no single correct answer; any solution that exposes some interesting aspect of the data should be considered.\n\nJames et al. (2017)"
  },
  {
    "objectID": "Module4/PCA.slides.html#agenda",
    "href": "Module4/PCA.slides.html#agenda",
    "title": "Principal Component Analysis",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nDispersion in one or more dimensions\nPrincipal component analysis"
  },
  {
    "objectID": "Module4/PCA.slides.html#types-of-learning",
    "href": "Module4/PCA.slides.html#types-of-learning",
    "title": "Principal Component Analysis",
    "section": "Types of learning",
    "text": "Types of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data."
  },
  {
    "objectID": "Module4/PCA.slides.html#unsupervised-learning-methods",
    "href": "Module4/PCA.slides.html#unsupervised-learning-methods",
    "title": "Principal Component Analysis",
    "section": "Unsupervised learning methods",
    "text": "Unsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/PCA.slides.html#dispersion-in-one-dimension",
    "href": "Module4/PCA.slides.html#dispersion-in-one-dimension",
    "title": "Principal Component Analysis",
    "section": "Dispersion in one dimension",
    "text": "Dispersion in one dimension\n\nThe concept of principal components requires an understanding of the dispersion or variability of the data.\nSuppose we have data for a single predictor."
  },
  {
    "objectID": "Module4/PCA.slides.html#dispersion-in-two-dimensions",
    "href": "Module4/PCA.slides.html#dispersion-in-two-dimensions",
    "title": "Principal Component Analysis",
    "section": "Dispersion in two dimensions",
    "text": "Dispersion in two dimensions"
  },
  {
    "objectID": "Module4/PCA.slides.html#capturing-dispersion",
    "href": "Module4/PCA.slides.html#capturing-dispersion",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension."
  },
  {
    "objectID": "Module4/PCA.slides.html#capturing-dispersion-1",
    "href": "Module4/PCA.slides.html#capturing-dispersion-1",
    "title": "Principal Component Analysis",
    "section": "Capturing dispersion",
    "text": "Capturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\nA single predictor \\(X_2\\) captures much of the spread in the data."
  },
  {
    "objectID": "Module4/PCA.slides.html#lets-see-another-example",
    "href": "Module4/PCA.slides.html#lets-see-another-example",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example"
  },
  {
    "objectID": "Module4/PCA.slides.html#lets-see-another-example-1",
    "href": "Module4/PCA.slides.html#lets-see-another-example-1",
    "title": "Principal Component Analysis",
    "section": "Let’s see another example",
    "text": "Let’s see another example\n\n\n\n\n\n\n\n\n\n\nA single predictor captures much of the dispersion in the data. In this case, the new predictor has the form \\(Z_1 = a X_1 + b X_2 + c.\\)"
  },
  {
    "objectID": "Module4/PCA.slides.html#section",
    "href": "Module4/PCA.slides.html#section",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Alternatively, we can use two alternative dimensions to capture the dispersion."
  },
  {
    "objectID": "Module4/PCA.slides.html#a-new-coordinate-system",
    "href": "Module4/PCA.slides.html#a-new-coordinate-system",
    "title": "Principal Component Analysis",
    "section": "A new coordinate system",
    "text": "A new coordinate system\n\n\n\n\n\nThe new coordinate axis is given by two new predictors, \\(Z_1\\) and \\(Z_2\\). Both are given by linear equations of the new predictors.\nThe first axis, \\(Z_1\\), captures a large portion of the dispersion, while \\(Z_2\\) captures a small portion from another angle.\nThe new axes, \\(Z_1\\) and \\(Z_2\\), are called principal components."
  },
  {
    "objectID": "Module4/PCA.slides.html#dimension-reduction",
    "href": "Module4/PCA.slides.html#dimension-reduction",
    "title": "Principal Component Analysis",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nPrincipal Components Analysis (PCA) helps us reduce the dimension of the data.\n\n\nIt creates a new coordinate axis in two (or more) dimensions.\nTechnically, it creates new predictors by combining highly correlated predictors. The new predictors are uncorrelated."
  },
  {
    "objectID": "Module4/PCA.slides.html#setup",
    "href": "Module4/PCA.slides.html#setup",
    "title": "Principal Component Analysis",
    "section": "Setup",
    "text": "Setup\n\nStep 1. We start with a database with \\(n\\) observations and \\(p\\) predictors.\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n15\n14\n5\n\n\n2\n1\n6\n\n\n10\n3\n17\n\n\n8\n18\n9\n\n\n12\n16\n11"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-1",
    "href": "Module4/PCA.slides.html#section-1",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. We standardize each predictor individually.\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}\\]\n\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n\n1.15\n0.46\n-0.96\n\n\n\n-1.52\n-1.20\n-0.75\n\n\n\n0.12\n-0.95\n1.55\n\n\n\n-0.29\n0.97\n-0.13\n\n\n\n0.53\n0.72\n0.29\n\n\nSum\n0\n0\n0\n\n\nVariance\n1\n1\n1"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-2",
    "href": "Module4/PCA.slides.html#section-2",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. We assume that the standardized database is an \\(n\\times p\\) matrix \\(\\mathbf{X}\\).\n\\[\\mathbf{X} = \\begin{pmatrix}\n1.15    &   0.46    &   -0.96   \\\\\n-1.52   &   -1.20   &   -0.75   \\\\\n0.12    &   -0.95   &   1.55    \\\\\n-0.29   &   0.97    &   -0.13   \\\\\n0.53    &   0.72    &   0.29    \\\\\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "Module4/PCA.slides.html#algorithm",
    "href": "Module4/PCA.slides.html#algorithm",
    "title": "Principal Component Analysis",
    "section": "Algorithm",
    "text": "Algorithm\n\nThe PCA algorithm has its origins in linear algebra.\n\nIts basic idea is:\n\nCreate a matrix \\(\\mathbf{C}\\) with the correlations between the predictors of the matrix \\(\\mathbf{X}\\).\nSplit the matrix \\(\\mathbf{C}\\) into three parts, which give us the new coordinate axis and the importance of each axis."
  },
  {
    "objectID": "Module4/PCA.slides.html#correlation-matrix",
    "href": "Module4/PCA.slides.html#correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nContinuing with our example, the correlation matrix contains the correlations between two columns of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "Module4/PCA.slides.html#partitioning-the-correlation-matrix",
    "href": "Module4/PCA.slides.html#partitioning-the-correlation-matrix",
    "title": "Principal Component Analysis",
    "section": "Partitioning the correlation matrix",
    "text": "Partitioning the correlation matrix\n\nThe \\(\\mathbf{C}\\) matrix is partitioned using the eigenvalue and eigenvector decomposition method."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-3",
    "href": "Module4/PCA.slides.html#section-3",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The columns of \\(\\mathbf{B}\\) define the axes of the new coordinate system. These axes are called principal components.\nThe diagonal values in \\(\\mathbf{A}\\) define the individual importance of each principal component (axis)."
  },
  {
    "objectID": "Module4/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "href": "Module4/PCA.slides.html#proportion-of-the-dispersion-explained-by-the-component",
    "title": "Principal Component Analysis",
    "section": "Proportion of the dispersion explained by the component",
    "text": "Proportion of the dispersion explained by the component\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion of the dispersion in the data that is captured by the first component is \\(\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53\\)."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-4",
    "href": "Module4/PCA.slides.html#section-4",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion captured by the second component is \\(\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36\\).\nThe proportion captured by the third component is \\(\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11.\\)"
  },
  {
    "objectID": "Module4/PCA.slides.html#comments",
    "href": "Module4/PCA.slides.html#comments",
    "title": "Principal Component Analysis",
    "section": "Comments",
    "text": "Comments\n\nPrincipal components can be used to approximate a matrix.\nFor example, we can approximate the matrix \\(\\mathbf{C}\\) by setting the third component equal to zero.\n\n\\[\\begin{pmatrix}\n-0.68   &   0.35    &   0.00    \\\\\n-0.72   &   -0.13   &   0.00    \\\\\n0.16    &   0.93    &   0.00\\\\\n\\end{pmatrix} \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.00    \\\\\n\\end{pmatrix} \\begin{pmatrix}\n-0.68   &   -0.72   &   0.16    \\\\\n0.35    &   -0.13   &   0.93    \\\\\n0.00    &   0.00    &   0.00    \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.86    &   0.73    &   0.18    \\\\\n0.73    &   0.85    &   -0.30   \\\\\n0.18    &   -0.30   &   0.96    \\\\\n\\end{pmatrix}\\]\n\n\n\\[\\approx \\begin{pmatrix}\n1.00    &   0.58    &   0.11    \\\\\n0.58    &   1.00    &   -0.23   \\\\\n0.11    &   -0.23   &   1.00    \\\\\n\\end{pmatrix} = \\mathbf{C}\\]"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-5",
    "href": "Module4/PCA.slides.html#section-5",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Approximations are useful for storing large matrices.\nThis is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\nThis is the idea behind image compression."
  },
  {
    "objectID": "Module4/PCA.slides.html#example-1",
    "href": "Module4/PCA.slides.html#example-1",
    "title": "Principal Component Analysis",
    "section": "Example 1",
    "text": "Example 1\n\nConsider a database of the 100 most popular songs on TikTok. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements.\nEnergy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\nThe overall volume of a track in decibels (dB). Loudness values are averaged across the entire track."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-6",
    "href": "Module4/PCA.slides.html#section-6",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Other predictors are:\n\nSpeech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\nA confidence measure from 0 to 1 about whether the track is acoustic.\nDetects the presence of an audience in the recording.\nA measure from 0 to 1 that describes the musical positivity a track conveys."
  },
  {
    "objectID": "Module4/PCA.slides.html#the-data",
    "href": "Module4/PCA.slides.html#the-data",
    "title": "Principal Component Analysis",
    "section": "The data",
    "text": "The data\n\ntiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\ntiktok_data.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nalbum\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\n0\nSay So\nDoja Cat\nHot Pink\n0.787\n0.673\n-4.583\n0.1590\n0.26400\n0.0904\n0.779\n110.962\n\n\n1\nBlinding Lights\nThe Weeknd\nAfter Hours\n0.514\n0.730\n-5.934\n0.0598\n0.00146\n0.0897\n0.334\n171.005\n\n\n2\nSupalonely (feat. Gus Dapperton)\nBENEE\nHey u x\n0.862\n0.631\n-4.746\n0.0515\n0.29100\n0.1230\n0.841\n128.978\n\n\n3\nSavage\nMegan Thee Stallion\nSuga\n0.843\n0.741\n-5.609\n0.3340\n0.02520\n0.0960\n0.680\n168.983\n\n\n4\nMoral of the Story\nAshe\nMoral of the Story\n0.572\n0.406\n-8.624\n0.0427\n0.58700\n0.1020\n0.265\n119.812"
  },
  {
    "objectID": "Module4/PCA.slides.html#standardize-the-data",
    "href": "Module4/PCA.slides.html#standardize-the-data",
    "title": "Principal Component Analysis",
    "section": "Standardize the data",
    "text": "Standardize the data\n\nRemember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n\n# Select the predictors\nfeatures = ['danceability', 'energy', 'loudness', 'speechiness',\n            'acousticness', 'liveness', 'valence', 'tempo']\nX_tiktok = tiktok_data.filter(features)  \n\n# Standardize the data\nscaler = StandardScaler()\nXs_tiktok = scaler.fit_transform(X_tiktok)"
  },
  {
    "objectID": "Module4/PCA.slides.html#pca-in-python",
    "href": "Module4/PCA.slides.html#pca-in-python",
    "title": "Principal Component Analysis",
    "section": "PCA in Python",
    "text": "PCA in Python\n\nWe tell Python that we want to apply PCA using the function PCA() from sklearn. Next, we run the algorithm using .fit_transform()\n\npca = PCA()\nPCA_tiktok = pca.fit_transform(Xs_tiktok)"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-7",
    "href": "Module4/PCA.slides.html#section-7",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The Screen or Summary Plot tells you the variability captured by each component. This variability is given by the Eigenvalue. From 1 to 8 components.\nThe first component covers most of the data dispersion.\nThis graph is used to define the total number of components to use."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-8",
    "href": "Module4/PCA.slides.html#section-8",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate a scree plot is below.\n\nexplained_var = pca.explained_variance_ratio_\n\nplt.figure(figsize=(5, 5))\nplt.plot(range(1, len(explained_var) + 1), explained_var, \n         marker='o', linestyle='-')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.xticks(range(1, len(explained_var) + 1))\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#biplot",
    "href": "Module4/PCA.slides.html#biplot",
    "title": "Principal Component Analysis",
    "section": "Biplot",
    "text": "Biplot\n\n\n\n\nDisplays the graphical observations on the new coordinate axis given by the first two components.\nHelps visualize data for three or more predictors using a two-dimensional scatter plot.\nA red line indicates the growth direction of the labeled variable."
  },
  {
    "objectID": "Module4/PCA.slides.html#section-9",
    "href": "Module4/PCA.slides.html#section-9",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The code to generate the biplot is lenghty but it can be broken into three steps.\nStep 1. Create a DataFrame with the PCA results\n\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\n0\n1.103065\n0.558086\n-0.800688\n0.446496\n0.605944\n-0.044089\n0.287325\n-0.413604\n\n\n1\n0.805080\n-0.766973\n1.580513\n-2.215856\n0.359655\n0.708123\n-0.882761\n0.113058\n\n\n2\n1.330433\n0.728161\n-0.288982\n0.376298\n0.786185\n-1.134308\n0.178388\n-0.242497\n\n\n3\n1.496277\n2.095014\n1.351398\n-0.621691\n0.390949\n0.494101\n0.024648\n-0.080720\n\n\n4\n-1.973362\n-0.966108\n-0.302071\n-1.266269\n0.414639\n-0.335677\n-0.076711\n-0.140126"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-10",
    "href": "Module4/PCA.slides.html#section-10",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 2. Create biplot of first two principal components\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-11",
    "href": "Module4/PCA.slides.html#section-11",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Step 3. Add more information to the biplot.\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.slides.html#section-12",
    "href": "Module4/PCA.slides.html#section-12",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "With some extra lines of code, we label the points in the plot.\n\n\nCode\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df = (pca_df\n          .assign(songs = tiktok_data['track_name'])\n          )\n\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add labels for each song\nfor i in range(pca_df.shape[0]):\n    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n             pca_df['songs'][i], fontsize=8, alpha=0.7)\n\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module3/Nonparametric.slides.html#agenda",
    "href": "Module3/Nonparametric.slides.html#agenda",
    "title": "Nonparametric Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nLogistic regression\nEstimating a logistic regression model\nClassification performance"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#agenda",
    "href": "Module2/EnsembleMethods.slides.html#agenda",
    "title": "Ensamble Methods",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "href": "Module2/EnsembleMethods.slides.html#load-the-libraries",
    "title": "Ensamble Methods",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#decision-trees",
    "title": "Ensamble Methods",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "href": "Module2/EnsembleMethods.slides.html#limitations-of-decision-trees",
    "title": "Ensamble Methods",
    "section": "Limitations of decision trees",
    "text": "Limitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, decision trees can be combined to build effective algorithms for these problems."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "href": "Module2/EnsembleMethods.slides.html#ensamble-methods-1",
    "title": "Ensamble Methods",
    "section": "Ensamble methods",
    "text": "Ensamble methods\n\nEnsemble methods refer to frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "href": "Module2/EnsembleMethods.slides.html#bootstrap-samples",
    "title": "Ensamble Methods",
    "section": "Bootstrap samples",
    "text": "Bootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-1",
    "href": "Module2/EnsembleMethods.slides.html#bagging-1",
    "title": "Ensamble Methods",
    "section": "Bagging",
    "text": "Bagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions",
    "href": "Module2/EnsembleMethods.slides.html#predictions",
    "title": "Ensamble Methods",
    "section": "Predictions",
    "text": "Predictions\n\nLet \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values. For classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\mathbf{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section",
    "href": "Module2/EnsembleMethods.slides.html#section",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Using the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-1",
    "href": "Module2/EnsembleMethods.slides.html#section-1",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Compute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).\\]"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-2",
    "href": "Module2/EnsembleMethods.slides.html#section-2",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Classify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\)."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#implementation",
    "href": "Module2/EnsembleMethods.slides.html#implementation",
    "title": "Ensamble Methods",
    "section": "Implementation",
    "text": "Implementation\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#example-1",
    "href": "Module2/EnsembleMethods.slides.html#example-1",
    "title": "Ensamble Methods",
    "section": "Example 1",
    "text": "Example 1\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "href": "Module2/EnsembleMethods.slides.html#read-the-dataset",
    "title": "Ensamble Methods",
    "section": "Read the dataset",
    "text": "Read the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "href": "Module2/EnsembleMethods.slides.html#selected-predictors.",
    "title": "Ensamble Methods",
    "section": "Selected predictors.",
    "text": "Selected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nAdult_Subset = Adult_data.filter(['age', 'education.num', 'capital.gain', \n                                  'hours.per.week', 'income'])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "href": "Module2/EnsembleMethods.slides.html#set-the-target-class",
    "title": "Ensamble Methods",
    "section": "Set the target class",
    "text": "Set the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_Subset['income']\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nlarge\nsmall\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-3",
    "href": "Module2/EnsembleMethods.slides.html#section-3",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Here we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: large, dtype: int64"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "href": "Module2/EnsembleMethods.slides.html#training-and-validation-datasets",
    "title": "Ensamble Methods",
    "section": "Training and validation datasets",
    "text": "Training and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Create predictor matrix\nX_full = Adult_Subset.drop(columns = ['income'])\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2)\n\nWe use 80% of the dataset for training and the rest for validation."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "href": "Module2/EnsembleMethods.slides.html#bagging-in-python",
    "title": "Ensamble Methods",
    "section": "Bagging in Python",
    "text": "Bagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#predictions-1",
    "href": "Module2/EnsembleMethods.slides.html#predictions-1",
    "title": "Ensamble Methods",
    "section": "Predictions",
    "text": "Predictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 0, ..., 0, 0, 0])"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix",
    "title": "Ensamble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy",
    "href": "Module2/EnsembleMethods.slides.html#accuracy",
    "title": "Ensamble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.81"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "href": "Module2/EnsembleMethods.slides.html#a-single-deep-tree",
    "title": "Ensamble Methods",
    "section": "A single deep tree",
    "text": "A single deep tree\nTo compare the bagging, let’s use a single tree with cost complexity pruning.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.8"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#advantages",
    "href": "Module2/EnsembleMethods.slides.html#advantages",
    "title": "Ensamble Methods",
    "section": "Advantages",
    "text": "Advantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#limitations",
    "href": "Module2/EnsembleMethods.slides.html#limitations",
    "title": "Ensamble Methods",
    "section": "Limitations",
    "text": "Limitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#other-issues",
    "href": "Module2/EnsembleMethods.slides.html#other-issues",
    "title": "Ensamble Methods",
    "section": "Other issues",
    "text": "Other issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#section-4",
    "href": "Module2/EnsembleMethods.slides.html#section-4",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Bagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-1",
    "title": "Ensamble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "href": "Module2/EnsembleMethods.slides.html#why-does-it-work",
    "title": "Ensamble Methods",
    "section": "Why does it work?",
    "text": "Why does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "href": "Module2/EnsembleMethods.slides.html#tuning-parameter",
    "title": "Ensamble Methods",
    "section": "Tuning parameter",
    "text": "Tuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "href": "Module2/EnsembleMethods.slides.html#the-final-product-is-a-black-box",
    "title": "Ensamble Methods",
    "section": "The final product is a black box",
    "text": "The final product is a black box\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you."
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "href": "Module2/EnsembleMethods.slides.html#random-forest-in-python",
    "title": "Ensamble Methods",
    "section": "Random Forest in Python",
    "text": "Random Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "href": "Module2/EnsembleMethods.slides.html#confusion-matrix-1",
    "title": "Ensamble Methods",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "href": "Module2/EnsembleMethods.slides.html#accuracy-1",
    "title": "Ensamble Methods",
    "section": "Accuracy",
    "text": "Accuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.81"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#agenda",
    "href": "Module2/IntrotoDataScience.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nData Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "href": "Module2/IntrotoDataScience.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data Science is …",
    "text": "Data Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#other-similar-concepts",
    "href": "Module2/IntrotoDataScience.slides.html#other-similar-concepts",
    "title": "Introduction to Data Science",
    "section": "Other Similar Concepts",
    "text": "Other Similar Concepts\n\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#in-2004",
    "href": "Module2/IntrotoDataScience.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004…",
    "text": "In 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section",
    "href": "Module2/IntrotoDataScience.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#the-result",
    "href": "Module2/IntrotoDataScience.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#cross-industry-standard-process-crisp-for-data-science",
    "title": "Introduction to Data Science",
    "section": "Cross-Industry Standard Process (CRISP) for Data Science",
    "text": "Cross-Industry Standard Process (CRISP) for Data Science"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "href": "Module2/IntrotoDataScience.slides.html#crisp-model",
    "title": "Introduction to Data Science",
    "section": "CRISP Model",
    "text": "CRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nImplementation: How do stakeholders access the results?"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "href": "Module2/IntrotoDataScience.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business Understanding",
    "text": "Business Understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "href": "Module2/IntrotoDataScience.slides.html#problems-in-data-science",
    "title": "Introduction to Data Science",
    "section": "Problems in Data Science",
    "text": "Problems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-1",
    "href": "Module2/IntrotoDataScience.slides.html#section-1",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Clustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#discussion",
    "href": "Module2/IntrotoDataScience.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data Understanding I",
    "text": "Data Understanding I\n\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "href": "Module2/IntrotoDataScience.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data Understanding II",
    "text": "Data Understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#example",
    "href": "Module2/IntrotoDataScience.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\nIn the 1980s, credit cards were essentially priced uniformly because companies didn’t have adequate information systems to deal with differential pricing on a massive scale.\n\nAround 1990, Richard Fairbanks and Nigel Morris realized that information technology was powerful enough to enable more sophisticated predictive models and offer different terms (today: pricing, credit limits, low introductory rate balance transfers, cash back, and loyalty points)."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-2",
    "href": "Module2/IntrotoDataScience.slides.html#section-2",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Signet Bank’s management was convinced that modeling profitability, not just the probability of default, was the right strategy.\nThey knew that a small proportion of customers actually account for more than 100% of a bank’s profit from credit card transactions (because the rest are either breaking even or losing money).\nIf they could model profitability, they could make better offers to the best customers and “skim the cream” of the big banks’ clientele."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-3",
    "href": "Module2/IntrotoDataScience.slides.html#section-3",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "But Signet Bank had a really big problem implementing this strategy.\nThey didn’t have the right data to model profitability for offering different terms to different customers!\nSince the bank offered credit with a specific set of terms and a specific default model, they had the data to model profitability (1) for the terms they actually offered in the past, and (2) for the type of customer actually offered credit."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#section-4",
    "href": "Module2/IntrotoDataScience.slides.html#section-4",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "What could Signet Bank do? They put into play a fundamental data science strategy: acquire the necessary data at a cost!\nIn this case, data on customer profitability with different credit terms could be generated by conducting experiments. Different terms were randomly offered to different customers.\nThis might seem silly outside the context of data analytics thinking: you’re likely to lose money!\nThis is true. In this case, the losses are the cost of data acquisition."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#what-happened",
    "href": "Module2/IntrotoDataScience.slides.html#what-happened",
    "title": "Introduction to Data Science",
    "section": "What happened?",
    "text": "What happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "href": "Module2/IntrotoDataScience.slides.html#richard-fairbanks-and-nigel-morris",
    "title": "Introduction to Data Science",
    "section": "Richard Fairbanks and Nigel Morris",
    "text": "Richard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#most-used-data-science-tools",
    "href": "Module2/IntrotoDataScience.slides.html#most-used-data-science-tools",
    "title": "Introduction to Data Science",
    "section": "Most Used Data Science Tools",
    "text": "Most Used Data Science Tools\n\nPython\nR\nSAS\nExcel\nPower BI\nTableau\nApache Spark\n\nhttps://hackr.io/blog/top-data-analytics-tools"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#other-tools-used",
    "href": "Module2/IntrotoDataScience.slides.html#other-tools-used",
    "title": "Introduction to Data Science",
    "section": "Other Tools Used",
    "text": "Other Tools Used\n\nRapidMiner (https://rapidminer.com/products/studio/)\nJMP (https://www.jmp.com/es_mx/home.html)\nMinitab (https://www.minitab.com/es-mx/products/minitab/)\nTrifacta (https://www.trifacta.com/)\nBigML (https://bigml.com/)\nMLBase (http://www.mlbase.org/)\nGoogle Cloud AutoML (https://cloud.google.com/automl/)"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#terminology",
    "href": "Module2/IntrotoDataScience.slides.html#terminology",
    "title": "Introduction to Data Science",
    "section": "Terminology",
    "text": "Terminology\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict.\n\n\nWe want to establish the following relationship\n\\[\nY = f(X_1, X_2, \\ldots, X_p) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "href": "Module2/IntrotoDataScience.slides.html#types-of-learning",
    "title": "Introduction to Data Science",
    "section": "Types of Learning",
    "text": "Types of Learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#supervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised Learning…",
    "text": "Supervised Learning…\nIncludes algorithms that learn by example. The user provides the supervised algorithm with a known data set that includes the corresponding known inputs and outputs. The algorithm must find a method to determine how to reach those inputs and outputs.\nWhile the user knows the correct answers to the problem, the algorithm identifies patterns in the data, learns from observations, and makes predictions.\nThe algorithm makes predictions that can be corrected by the user, and this process continues until the algorithm reaches a high level of accuracy and performance."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-supervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Supervised Algorithms",
    "text": "Popular Supervised Algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "href": "Module2/IntrotoDataScience.slides.html#unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Unsupervised Learning…",
    "text": "Unsupervised Learning…\n\nstudies data to identify patterns. There is no answer key or human operator to provide instruction. The machine determines correlations and relationships by analyzing the available data.\nIn this process, the unsupervised algorithm is left to interpret large data sets. The algorithm attempts to organize that data in some way to describe its structure.\nAs it evaluates more data, its ability to make decisions about it gradually improves and becomes more refined."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "href": "Module2/IntrotoDataScience.slides.html#popular-unsupervised-algorithms",
    "title": "Introduction to Data Science",
    "section": "Popular Unsupervised Algorithms",
    "text": "Popular Unsupervised Algorithms"
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "href": "Module2/IntrotoDataScience.slides.html#two-data-sets",
    "title": "Introduction to Data Science",
    "section": "Two Data Sets",
    "text": "Two Data Sets\n\nIn supervised learning, there are several types of data.\nTraining data is the data used to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is the data that was NOT used in the fitting process, but is used to test the model’s performance on unanalyzed data."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#yogi-berra",
    "href": "Module2/IntrotoDataScience.slides.html#yogi-berra",
    "title": "Introduction to Data Science",
    "section": "Yogi Berra",
    "text": "Yogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "Module2/IntrotoDataScience.slides.html#lets-play",
    "href": "Module2/IntrotoDataScience.slides.html#lets-play",
    "title": "Introduction to Data Science",
    "section": "Let’s Play",
    "text": "Let’s Play\n\nLet’s play with supervised models.\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "index.slides.html#course-topics",
    "href": "index.slides.html#course-topics",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "Course topics",
    "text": "Course topics\nModule 1\n\nIndicators (slides)\nIntroduction to Data Bases\n\nModule 2\n\nIntroduction to Data Science (slides)\nDecision trees for classification\nEnsambles of trees\nK-nearest neighbours\n\nModule 3\n\nPredictive Models\nAutocorrelation Models\nTree-based methods\n\nModule 4\n\nPrincipal Component Analysis\nClustering Methods"
  },
  {
    "objectID": "index.slides.html#about-the-author",
    "href": "index.slides.html#about-the-author",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "About the author",
    "text": "About the author\nAlan R. Vazquez (website) is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\nLicense"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IN2004B Generation of Value with Data Analytics",
    "section": "",
    "text": "Course topics\n\nModule 1\n\nIndicators (slides)\nIntroduction to Data Bases (slides)\n\n\n\nModule 2\n\nIntroduction to Data Science (slides)\nDecision trees for classification (slides)\nEnsamble methods (slides)\nK-nearest neighbours (slides)\n\n\n\nModule 3\n\nPredictive Models (slides)\nAutocorrelation Models (slides)\nEnsamble Methods for Time Series\n\n\n\nModule 4\n\nClustering Methods (slides)\nPrincipal Component Analysis (slides)\n\n\n\n\nAbout the author\nAlan R. Vazquez (website) is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\n\nLicense",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Module1/DataBases.slides.html#agenda",
    "href": "Module1/DataBases.slides.html#agenda",
    "title": "Introduction to Data Bases",
    "section": "Agenda",
    "text": "Agenda\n\n\nDashboards para Visualizar Indicadores\nBases de Datos para Almacenar Indicadores"
  },
  {
    "objectID": "Module1/DataBases.slides.html#el-tablero-de-control-integral",
    "href": "Module1/DataBases.slides.html#el-tablero-de-control-integral",
    "title": "Introduction to Data Bases",
    "section": "El tablero de control integral",
    "text": "El tablero de control integral\nUna vez que se definene los indicadores usando el Balance Scorecard y el formato de documentación, se procede a diseñar los tableros de control:\n\npantallas,\nestilos de gráficos y tablas,\nniveles de agregación,\nreportes predeterminados,\nrequerimientos de drill-down.\n\nEsto es tanto para el nivel general de la organización, como para los BSC funcionales."
  },
  {
    "objectID": "Module1/DataBases.slides.html#despliegue-del-tablero-de-indicadores",
    "href": "Module1/DataBases.slides.html#despliegue-del-tablero-de-indicadores",
    "title": "Introduction to Data Bases",
    "section": "Despliegue del tablero de indicadores",
    "text": "Despliegue del tablero de indicadores\nPara la implementación de los tableros de indicadores se pueden seguir diversas estrategias, por ejemplo:\n\nDesarrollo en una plataforma especializada para sistemas de indicadores (https://www.predictiveanalyticstoday.com/open-source-balanced-scorecard-software/)\nDesarrollo con herramientas genéricas OLAP On-line Analytical Processing (https://www.softwaretestinghelp.com/best-olap-tools/)\nImplementaciones aisladas de corto alcance en hoja de cálculo."
  },
  {
    "objectID": "Module1/DataBases.slides.html#ejemplo-de-tablero-de-indicadores-de-tableu",
    "href": "Module1/DataBases.slides.html#ejemplo-de-tablero-de-indicadores-de-tableu",
    "title": "Introduction to Data Bases",
    "section": "Ejemplo de tablero de indicadores de Tableu",
    "text": "Ejemplo de tablero de indicadores de Tableu\n\nhttps://www.tableau.com/es-mx"
  },
  {
    "objectID": "Module1/DataBases.slides.html#otro-ejemplo",
    "href": "Module1/DataBases.slides.html#otro-ejemplo",
    "title": "Introduction to Data Bases",
    "section": "Otro Ejemplo",
    "text": "Otro Ejemplo"
  },
  {
    "objectID": "Module1/DataBases.slides.html#contexto",
    "href": "Module1/DataBases.slides.html#contexto",
    "title": "Introduction to Data Bases",
    "section": "Contexto",
    "text": "Contexto\nAunque los datos para el cálculo de algunos indicadores tienen su origen fuera de la organización, la gran mayoría de los datos provienen de las bases de datos internas del negocio.\nExiste mucha diversidad de empresas, unas que tienen aplicaciones aisladas con bases de datos dispersas y con archivos en hojas de cálculo, hasta empresas muy organizadas con un sistema de bases centralizado en un servidor de datos y aplicaciones vinculadas."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section",
    "href": "Module1/DataBases.slides.html#section",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "Para obtener los datos necesarios para el cálculo de indicadores, muchas veces será necesario integrar datos de diversas fuentes en la empresa. Para esto, se usan herramientas de ETL (extract-transform-load) y data warehousing."
  },
  {
    "objectID": "Module1/DataBases.slides.html#qué-es-una-base-de-datos",
    "href": "Module1/DataBases.slides.html#qué-es-una-base-de-datos",
    "title": "Introduction to Data Bases",
    "section": "¿Qué es una Base de Datos?",
    "text": "¿Qué es una Base de Datos?\nBásicamente, una tabla de datos donde los renglones representan un conjunto de ocurrencias de una entidad (clientes, productos, pacientes, pedidos) y las columnas representan atributos o características que describen a la entidad (cliente: ID + nombre + domicilio + e-mail + saldo ) En el ámbito de TI el concepto de “base de datos” no se refiere a una tabla, sino a un conjunto de tablas relacionadas."
  },
  {
    "objectID": "Module1/DataBases.slides.html#terminología-de-bd",
    "href": "Module1/DataBases.slides.html#terminología-de-bd",
    "title": "Introduction to Data Bases",
    "section": "Terminología de BD",
    "text": "Terminología de BD\nLas tablas contienen datos que se refieren a:\n\nalguna entidad acerca de la cual la organización necesita mantener información\nrelaciones entre entidades.\n\nA los renglones de la tabla se les denomina registros.\nA las columnas de la tabla se les denomina campos."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-1",
    "href": "Module1/DataBases.slides.html#section-1",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "Los registros son ocurrencias diferentes de la entidad correspondiente.\nLos campos son atributos que describen la entidad.\nCada registro tiene uno o varios campos que identifican de manera única cada registro, esos campos se denominan “llave”."
  },
  {
    "objectID": "Module1/DataBases.slides.html#bases-de-datos-relacionales",
    "href": "Module1/DataBases.slides.html#bases-de-datos-relacionales",
    "title": "Introduction to Data Bases",
    "section": "Bases de Datos Relacionales",
    "text": "Bases de Datos Relacionales\n\nActualmente el modelo de bases más utilizado en el mundo es el de bases de datos relacionales, ver por ejemplo:\nhttps://db-engines.com/en/ranking\nhttps://www.dataversity.net/database-management-trends-in-2020/\nAnte el surgimiento de aplicaciones de big data hace que estas bases de datos sean la principal fuente de datos para indicadores en las empresas."
  },
  {
    "objectID": "Module1/DataBases.slides.html#ejemplo-de-base-de-datos-relacional",
    "href": "Module1/DataBases.slides.html#ejemplo-de-base-de-datos-relacional",
    "title": "Introduction to Data Bases",
    "section": "Ejemplo de Base de Datos Relacional",
    "text": "Ejemplo de Base de Datos Relacional"
  },
  {
    "objectID": "Module1/DataBases.slides.html#ejemplo",
    "href": "Module1/DataBases.slides.html#ejemplo",
    "title": "Introduction to Data Bases",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\nLa base de datos tiene 4 tablas: Pacientes, Medicinas, Recetas y Detalle de las recetas.\nLa tabla que contiene datos de la entidad “paciente” tiene los siguientes campos: identificador único, nombre, domicilio, fecha de nacimiento, teléfono, y foto."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-2",
    "href": "Module1/DataBases.slides.html#section-2",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "La tabla de recetas contiene los datos generales de cada receta expedida: número de receta (es la llave), fecha, hora de consulta, un indicador de si contiene medicamentos controlados o no, y el identificador del paciente, este campo permite relacionar los datos de la receta con los datos del paciente."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-3",
    "href": "Module1/DataBases.slides.html#section-3",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "La tabla de medicinas tiene los campos: identificador de la medicina, descripción genérica, agente activo, presentación más común, y contra-indicaciones."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-4",
    "href": "Module1/DataBases.slides.html#section-4",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "La tabla de detalle de la receta contiene los renglones de cada receta. Como hay recetas que pueden tener un solo medicamento, puede haber algunas con 2, 3 o más medicamentos, en el modelo relacional se guardan los renglones de todas las recetas en una sola tabla, todos los renglones de una receta tienen el mismo “número de receta” pero diferente “id del medicamento”…"
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-5",
    "href": "Module1/DataBases.slides.html#section-5",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "En la tabla Detalle de Receta puede haber múltiples registros con el mismo Número de receta, porque la receta puede amparar varios medicamentos. También puede haber múltiples registros con el mismo ID de medicamento, porque un medicamento puede aparecer en muchas recetas."
  },
  {
    "objectID": "Module1/DataBases.slides.html#section-6",
    "href": "Module1/DataBases.slides.html#section-6",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "En este caso, para identificar de manera única un renglón de una receta en particular se requieren los dos identificadores, el de la receta y el del medicamento, esto constituye una llave compuesta."
  },
  {
    "objectID": "Module1/DataBases.slides.html#consultas-queries-a-la-bd",
    "href": "Module1/DataBases.slides.html#consultas-queries-a-la-bd",
    "title": "Introduction to Data Bases",
    "section": "Consultas (queries) a la BD",
    "text": "Consultas (queries) a la BD\nUna vez que se tiene la base de datos, es posible contestar preguntas como:\n\n¿En cuáles colonias viven los pacientes a quienes se les ha recetado turbocicloxina?\n¿A cuántos pacientes se les ha recetado turbocicloxina en el último mes?\n¿Cuáles agentes activos se les han administrado a los pacientes que viven en Prados #520 en el último año?"
  },
  {
    "objectID": "Module1/DataBases.slides.html#sql",
    "href": "Module1/DataBases.slides.html#sql",
    "title": "Introduction to Data Bases",
    "section": "SQL",
    "text": "SQL\n\nLas herramientas y plataformas de Business Intelligence con las que contamos actualmente, nos permiten hacer fácilmente consultas como las del ejemplo anterior, simplemente “arrastrando” campos y aplicando filtros en un lienzo de diseño.\nInternamente las herramientas procesan las consultas mediante un lenguaje de manejo de bases de datos que se llama SQL Structured Query Language.\nPara hacer consultas más complejas puede ser necesario que se haga la consulta escribiendo directamente el código de SQL.\nEn este módulo no estudiaremos SQL."
  },
  {
    "objectID": "Module1/Indicators.html",
    "href": "Module1/Indicators.html",
    "title": "Indicators",
    "section": "",
    "text": "Conceptos Básicos de Indicadores\nModelos para Definir Indicadores\nDocumentación de Indicadores"
  },
  {
    "objectID": "Module1/Indicators.html#conceptos-básicos-de-indicadores",
    "href": "Module1/Indicators.html#conceptos-básicos-de-indicadores",
    "title": "Indicators",
    "section": "Conceptos Básicos de Indicadores",
    "text": "Conceptos Básicos de Indicadores\n\nLa administración\n\n“Administración es el proceso mediante el cual se diseña y mantiene un ambiente en el que individuos que trabajan en grupos cumplen metas específicas de manera eficaz”.\n\nKoontz, Harold, “Administración”, 14 Edición, Mc Graw Hill, México, 2012\n\n\nFunciones de la Administración\nEl proceso administrativo se desglosa en cinco funciones gerenciales:\n\nPlanear: elegir misiones y objetivos, y las acciones para lograrlos\nOrganizar: establecer una estructura intencional de funciones que las personas desempeñen en una organización\nIntegrar personal: Cubrir y mantener cubiertos los puestos en la estructura organizacional\nDirigir: Influir en las personas para que contribuyan a las metas organizacionales y de grupo\nControlar: Medir y corregir el desempeño individual y organizacional para asegurar que los hechos se conformen a los planes.\n\n\n\nIntegración de la Planeación y el Control\n\n\n\nPlaneación\nComo resultado del proceso de planeación pueden generarse algunos de estos tipos de planes:\n\nObjetivos o metas. Son resultados específicos y medibles.\nEstrategias. Enfoques o planes para lograr objetivos.\nProcedimientos. Detallan la ejecución de una actividad.\nProgramas. Conjuntos organizados de actividades para alcazar objetivos.\nPresupuestos. Planes financieros detallados para facilitar la toma de decisiones económicas.\n\nAlta necesidad de medir progreso\n\n\nDefinición de Objetivos SMART\nSpecific: Un objetivo debe ser claro y específico, evitando la ambigüedad. Debe responder a las preguntas qué, quién, cuándo, dónde y por qué.\nMeasurable: Un objetivo debe ser cuantificable o al menos evaluable para determinar el progreso y el éxito. Debe ser posible medirlo con indicadores o criterios tangibles.\nAchievable: Un objetivo debe ser realista y alcanzable, teniendo en cuenta los recursos disponibles, el tiempo y las habilidades necesarias.\nRelevant: Un objetivo debe ser relevante y estar alineado con los objetivos más amplios de la organización o del individuo.\nTime-bound: Un objetivo debe tener un plazo o fecha límite claramente definidos.\n\n\nEjemplo 1\nObjetivo: Cumplir con capacitación de personal\nSMART: Cumplir con al menos el 90% del programa de capacitación 2014 para todo el personal operativo de la empresa para el 30 de noviembre del presente año.\n\n\nEjemplo 2\nObjetivo: Aumentar las ventas un 20%.\nSMART: Lograr un incremento de ventas del producto X de al menos 17% al termino del primer semestre del 2015, manteniendo una rentabilidad para la empresa de al menos 5%.\n\n\nNecesidad de medir\n\n\n\nPara poder llevar a cabo los procesos de Planeación, Instrumentación y Control, es necesario contar con un sistema de información que permita evaluar si se están logrando los fines que se planearon, y si las acciones instrumentadas se están llevando también de acuerdo con los planes.\nLa información es necesaria para corregir los planes o su instrumentación, y para producir nuevos planes.\n\n\n\n\n\n\n\n¿Qué es un indicador?\n\n“Es el resultado de una medición cuantitativa o cualitativa, o algún otro criterio, mediante el cual se puede evaluar el desempeño, la eficiencia, el logro, etc., de una persona u organización, frecuentemente comparándolo con un estándar o con una meta”.\n\nCollins English Dictionary.\n\n“The qualitative and/or quantitative information on an examined phenomenon (or a process, or a result), which makes it possible to analyze its evolution and to check whether quality targets are met, driving actions and decisions”.\n\nFranceschini, Fiorenzo & Galetto, Maurizio & Maisano, Domenico. (2007). Management by Measurement: Designing Key Indicators and Performance Measurement Systems. 10.1007/978-3-540-73212-9.\n\n\nEjemplos de indicadores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaracterísticas de un indicador\nFundamentales:\n\nValidez: el indicador debe mostrar fielmente el comportamiento real del fenómeno, variable, resultado, etc., que se desea medir.\nEstabilidad: el indicador debe definirse, calcularse e interpretarse de la misma manera a través del tiempo (permite comparaciones y observar tendencias).\n\n\n\n\nIdeales:\n\nSimple y fácil de interpretar.\nCapaz de indicar tendencias a través del tiempo.\nSensible a cambios dentro y fuera de la organización.\nFácil recolección y procesamiento de los datos.\nFácil y rápida actualización.\n\n\n\nUtilidad de los indicadores\n\n\n\nDimensiones de análisis\n\nLos indicadores y los datos que conducen a ellos, muchas veces se estratifican con respecto a otras variables.\nA las variables que se utilizan como criterios de estratificación se les denomina “Dimensiones de análisis” (son dimensiones desde el punto de vista de hipercubos de datos).\n\nEjemplo: En un proceso de ventas, las ventas mensuales de pueden estratificar por: canal de distribución, región del país, familia de productos, etc., para efectos de su análisis y visualización.\n\n\nPor ejemplo\n\n\n\nElección de indicadores\nLa elección de los indicadores es un factor crítico para que una organización se acerque al cumplimiento de su misión y haga realidad sus estrategias. Los indicadores y las estrategias están inevitablemente vinculados.\n\nUna estrategia sin indicadores es inútil, los indicadores sin una estrategia son irrelevantes!\n\nExisten dos tipos principales de indicadores\n\nAdelantados\nRetrasados\n\n\n\nIndicadores adelantados y retrasados (leading – lagging)\nUn indicador retrasado (lagging) mide el resultado del desempeño al final de un periodo, tiene una orientación hacia el pasado porque nos muestra las consecuencias de lo que ya se hizo. También se conocen como indicadores de resultado.\nUn indicador adelantado (leading) mide el desempeño de los factores que son críticos ahora, para obtener un resultado deseado en el futuro. También se conocen como indicadores de actuación.\n\n\nIndicador Retrasado\nPropósito: Mide el resultado del desempeño al final de un periodo\nEjemplo: Ventas anuales, market share, ROI\n\nVentaja: Son objetivos\nDesventaja: Reflejan el efecto de acciones del pasado.\n\n\n\nIndicador Adelantado\nPropósito: Mide procesos, actividades, comportamientos\nEjemplo: # de clientes visitados, # de cursos ofrecidos\n\nVentaja: Son predictivos, permiten corregir la estrategia\nDesventaja: Basados en hipótesis causa-efecto.\n\n\n\nOtros Ejemplos\nEn el contexto de un YouTuber:\n\nIndicador adelantado: Número de vistas de un video de YouTube en las primeras 24 horas.\nIndicador retrasado: Ingreso mensual generado por la monetización.\n\nEn el contexto del departamento de ventas de Netflix:\n\nIndicador adelantado: Número de usuarios que comienzan una prueba gratuita en un mes dado.\nIndicador retrasado: Ingreso mensuales por suscripciones.\n\n\n\nOtros Ejemplos\nEn el contexto de una empresa de GenAI:\n\nIndicador adelantado: Número de programadores contratados con expertise en IA.\nIndicador retrasado: Numero de licensias vendidas al año.\n\nEn el contexto de una empresa de mantenimiento de minisplits:\n\nIndicador adelantado: El tiempo promedio de respuesta del servicio técnico a solicitudes de soporte.\nIndicador retrasado: Numero de reseñas positivas en Google Maps en un mes.\n\n\n\n¿Cómo defino un indicador?\nCriterios de selección:\n\nRelación directa con el objetivo a medir\nFacilidad de comunicación enfocada a la estrategia\nRepetibilidad y confiabilidad\nFrecuencia de actualización\nUtilidad en la fijación de metas\nUtilidad para asignar responsabilidades\nUtilidad para el despliegue hacia abajo\n\n\n\nIndicadores Básicos y Derivados\nUn indicador básico se obtiene de la medición directa de un fenómeno o hecho. Por ejemplo: Número de pedidos entregados completos y a tiempo en la semana.\nUn indicador derivado combina la información de dos o más indicadores básicos o derivados. Por ejemplo: Porcentaje de pedidos entregados completos y a tiempo en la semana.\nEjemplo: en Análisis de modos de falla y efectos\n\nIndicadores básicos: Severidad, Ocurencia, Detección\nIndicador derivado: Risk Priority Number = SOD\n\n\n\nEjemplo 3\nEn el análisis de modos de falla y efectos, tenemos los siguientes indicadores básicos:\n\nSeveridad (S): Mide el impacto o la gravedad de la falla en caso de que ocurra. Se mide en una escala del 1 al 10, donde 1 indica un efecto insignificante y 10 representa un efecto catastrófico para el usuario o el sistema.\nOcurrencia (O): Evalúa la probabilidad de que la falla ocurra. Se evalua en una escala de 1 al 10, donde 1 indica que la ocurrencia es muy rara y 10 que es altamente probable o frecuente.\nDetección (D): Representa la capacidad del sistema para detectar la falla antes de que llegue al cliente o al usuario final. Se califica en una escala del 1 al 10, donde 1 significa que la detección es casi segura y 10 que es muy difícil o imposible de detectar antes de que ocurra un problema.\n\nIndicador derivado: Número de Prioridad de Riesgo = S\\(\\times\\)O\\(\\times\\)D\n\n\nEl Formato de un Indicador\nUn indicador se debe medir númericamente usando:\nNúmeros absolutos: Resultantes de un proceso de medición o conteo (volumen producido, precio de la acción, número de empleados, costos fijos…)\nTasas: Relación entre dos variables con diferentes unidades (número de unidades / número de operarios, consumo energético / litros producidos…)\nÍndices: Cantidad adimensional que resulta de dividir el valor actual de una variable entre un valor base de referencia de esa variable (índice de precios al consumidor)\n\n\n\nProporciones: Relaciones entre dos variables que se miden en las mismas unidades (hombres vs mujeres, admitidos vs rechazados)\nPorcentajes de crecimiento o decrecimiento: (Valor actual – Valor anterior)*100/Valor anterior.\nEvaluaciones: Evaluaciones de una variable cualitativa en una escala ordinal tipo Likert (bajo medio alto, pésimo malo regular bueno excelente).\n\n\nActividad 1.1 (cooperative mode)\nJúntate en equipos de 3 y pregúntale a ChatGPT! Para los siguientes conceptos sugieran al menos un indicador cuantitativo adelantado (leading) y un indicador retrasado (lagging):\n\nProductividad mensual de una línea de producción de muebles\nRotación anual de personal en una empresa de manufactura\nNivel de servicio al cliente de una empresa que fabrica envases de plástico y entrega regionalmente\nRentabilidad del negocio para una empresa mediana mayorista de abarrotes.\nDesempeño del proceso de recaudación de fondos de una asociación de apoyo a niños en situación de calle.\n\n\n\nRecuerda que …\n\n\nLos propósitos de un indicador son:\n\nEstablecer metas cuantitativas\nMotivación organizacional, inducción de conductas deseables\nEvaluación de la estrategia y aprendizaje estratégico."
  },
  {
    "objectID": "Module1/Indicators.html#modelos-para-definir-indicadores",
    "href": "Module1/Indicators.html#modelos-para-definir-indicadores",
    "title": "Indicators",
    "section": "Modelos para Definir Indicadores",
    "text": "Modelos para Definir Indicadores\n\n¿Cuáles indicadores debemos utilizar?\n\nUna vez que conocemos qué son los indicadores, su papel dentro del proceso administrativo, y los tipos de indicadores existen, nos preguntamos ¿Cuántos indicadores debemos tener? ¿Cuál es un conjunto apropiado de indicadores? ¿Cómo los documentamos y compartimos?\nAunque hay algunos indicadores que pudieran aplicarse de manera general a cualquier empresa, cada empresa tiene su propia estrategia, sus propias prioridades, su entorno competitivo particular, por lo tanto, el conjunto más conveniente de indicadores depende de cada organización.\n\nAquí describimos modelos para encontrar el número y conjunto apropiado de indicadores.\n\n\nEstrategia y Ejecución\n\n\nLa estructura de planeación y control de la organización provee un marco de trabajo para identificar y estructurar el sistema de indicadores.\nYa que uno de los retos primordiales de cualquier organización consiste en alinear la ejecución con la estrategia, o la estrategia con la ejecución.\nLos sistemas de indicadores para la medición del desempeño deben apoyar a mantener el vínculo entre la estrategia y la ejecución.\n\n\n\n\nhttps://nationalpost.com/news/wal-marts-epic-strategy-fail\n\n\n\n\n\n\nModelos para Definir Indicadores\nLos modelos para definir indicadores dependen del tipo de planeación estratégica.\nTres marcos de planeación estratégica son:\n\nAdministración por Objetivos (APO; Peter Drucker, 1954)\nHoshin Kanri\nBalanced Score Card (BSC; Kaplan & Norton, 1992, 1996)\n\n\n\nBalance Score Card (BSC)\nEs un modelo que ayuda a las organizaciones a traducir la estrategia en objetivos operacionales (medibles), que resulta en acciones, conductas y desempeño.\nBSC incluye todos los factores críticos de éxito en un sistema de medición, que brinda a las organizaciones una mejor posibilidad de alcanzar sus metas.\n\n\n\n\nhttps://www.youtube.com/watch?v=QCi09LlI7Hs\n\n\nObjetivos del BSC\n\nTraducir la estrategia a objetivos medibles.\nAlinear los componentes de la estrategia: objetivos, indicadores e iniciativas.\n\n3.Comunicar la estrategia a la organización.\n\nCrear la base para una administración estratégica.\n\n\n\n\nBSC convierte la estrategia en un sistema integrado definido a través de 4 perspectivas:\n\n\n\nFinanciera\nClientes\nProcesos Internos\nAprendizaje y crecimiento\n\n\n\n\n\n\n\nLas 4 perspectivas\nBSC convierte la estrategia en un sistema integrado definido a través de 4 perspectivas:\n\n\n\nFinanciera. Incluye objetivos relacionados con la rentabilidad, productividad, utilidades, precio de las acciones, etc., son los objetivos que debe lograr la organización desde la perspectiva de los accionistas.\n\n\n\n\n\n\n\n\n\n\n\nClientes. Incluye objetivos relacionados con la propuesta de valor de la empresa, están orientados al mercado y se establecen desde la perspectiva de los clientes. Incluye objetivos de percepción de los clientes en cuanto al servicio, tiempo de entrega, calidad, valor/precio.\n\n\n\n\n\n\n\n\n\n\n\nProcesos Internos. Incluye objetivos relacionados con el desempeño de los procesos que son críticos para cumplir con los objetivos de la perspectiva de clientes. Objetivos de desempeño de la Cadena de Valor primaria del negocio.\n\n\n\n\n\n\n\n\n\n\n\nAprendizaje y crecimiento. Son los objetivos relacionados con los habilitadores para lograr los objetivos de las otras perspectivas. Son objetivos de desarrollo de competencias, ambiente laboral, ambiente físico, infraestructura tecnológica, etc."
  },
  {
    "objectID": "Module1/Indicators.html#ejemplo-planear-tu-fiesta-de-cumpleaños",
    "href": "Module1/Indicators.html#ejemplo-planear-tu-fiesta-de-cumpleaños",
    "title": "Indicators",
    "section": "Ejemplo: Planear tu Fiesta de Cumpleaños",
    "text": "Ejemplo: Planear tu Fiesta de Cumpleaños\n\n\n\n\n\n\nComponentes del Modelo\nPara cada perspectiva se debe definir:\n\nUn conjunto pequeño de objetivos estratégicos\nPara cada objetivo una (o más si es indispensable) métrica como indicador de desempeño\nPara cada indicador establecer metas de largo y corto plazo\nIniciativas (programas, proyectos, acciones) para cerrar las brechas entre el desempeño actual y el deseado de acuerdo con las metas.\n\n\n\nMapa Estratégico\n\nEl mapa estratégico muestra los objetivos estratégicos dentro de cada perspectiva usando una matriz.\nSe muestran también posibles relaciones causales entre los objetivos mediante flechas.\nSi se revisan los estatutos estratégicos de la organización (Visión y Misión) se pueden incluir Temas Estratégicos que mostrarán si la organización está atendiendo dichos temas de manera explícita en la planeación."
  },
  {
    "objectID": "Module1/Indicators.html#estructura-del-mapa-estratégico",
    "href": "Module1/Indicators.html#estructura-del-mapa-estratégico",
    "title": "Indicators",
    "section": "Estructura del Mapa Estratégico",
    "text": "Estructura del Mapa Estratégico\n\n\n\nhttps://www.youtube.com/watch?v=Brv3w1MZpRg&ab_channel=EstrategiaenAcci%C3%B3nconIv%C3%A1nMart%C3%ADnezLima\n\n\n\n\n\n\n\n\n\n\nActividad 1.2 (cooperative mode)\n\nJúntate en equipos de 3.\nConstruye un mapa estratégico como parte de la aplicación del modelo Balanced Scorecard.\n\nLee la introducción del mini-caso Muebles Finos MF en CANVAS\n\n\n\nLa actividad consiste en asignar los objetivos de la empresa a las perspectivas del modelo BSC y a las líneas estratégicas del negocio. Una vez que hayan colocado los objetivos en el mapa, vincula los objetivos entre sí, estableciendo relaciones causa-efecto entre los objetivos. Se te pide también seleccionar 3 de los objetivos y proponer al menos un métrico sugerido para cada uno de ellos. Finalmente, escribirás la justificación de las relaciones causales establecidas\n\n\nComentarios Finales\n\nUna vez que se ha diseñado el modelo de BSC para la empresa, se procede a desarrollar modelos específicos por áreas funcionales.\nEn las áreas funcionales se trabaja a partir de los objetivos estratégicos, identificando objetivos particulares del área funcional.\nEs más frecuente utilizar indicadores de actuación que de impacto a nivel departamental.\nNo necesariamente se mantienen todos los temas estratégicos en las áreas funcionales, ni necesariamente todas las áreas tienen objetivos en todas las perspectivas del BSC.\n\n\n\n\n\nEs indispensable que los gerentes funcionales entiendan perfectamente el BSC de la empresa y la contribución de su área a los objetivos globales.\nEl responsable del despliegue de BSC debe asegurar la congruencia y alineación de los BSC funcionales.\nAlgunos indicadores a nivel organizacional son simplemente agregaciones de los indicadores departamentales."
  },
  {
    "objectID": "Module1/Indicators.html#documentación-de-indicadores",
    "href": "Module1/Indicators.html#documentación-de-indicadores",
    "title": "Indicators",
    "section": "Documentación de Indicadores",
    "text": "Documentación de Indicadores\n\nDocumentación de Indicadores\nCuando se están seleccionado y/o diseñando los indicadores, es necesario documentar formalmente la definición de cada uno de los indicadores.\nLa documentación es muy útil porque:\n\nAyuda a clarificar el significado de los indicadores.\nFacilita la comunicación entre los usuarios y creadores de los indicadores.\nSirve como referencia futura cuando se revise el sistema.\n\n\n\nFormato Básico\n\n\n\nReturn to main page"
  },
  {
    "objectID": "Module2/KNearestNeighbours.html",
    "href": "Module2/KNearestNeighbours.html",
    "title": "K nearest neighbors",
    "section": "",
    "text": "Before we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python."
  },
  {
    "objectID": "Module2/KNearestNeighbours.html#return-to-main-page",
    "href": "Module2/KNearestNeighbours.html#return-to-main-page",
    "title": "K nearest neighbors",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/Classification.html",
    "href": "Module2/Classification.html",
    "title": "Classification Trees",
    "section": "",
    "text": "Introduction\nClassification Trees\nClassification Algorithm Metrics"
  },
  {
    "objectID": "Module2/Classification.html#introduction",
    "href": "Module2/Classification.html#introduction",
    "title": "Classification Trees",
    "section": "Introduction",
    "text": "Introduction\n\nLoad the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical.\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical.\n\n\nTerminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors.\n\n\n\n\n\nResponse:\n\n\n\\(Y\\) is a categorical variable that takes 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or no spam.\nWhen classes are strings, they are usually encoded as 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\).\n\n\n\n\n\nClassification algorithms\n\nClassification algorithms use predictor values to predict the class of the response (target or reference).\n\nThat is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n\nTechnically, they predict the probability that the record belongs to the target class.\n\n\n\n\nGoal: Develop a function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n. . .\nTo achieve this goal, most algorithms consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n. . .\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification.\n\n\nExample 1\nConsider a spam filter where \\(Y\\) is the email type.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n. . .\n\n\n\n\n\n. . .\nBoth emails would be classified as spam. However, we would have greater confidence in our classification for the second email.\n\n\n\n\nTechnically, \\(C(\\boldsymbol{X})\\) works with the conditional probability:\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n. . .\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]\n\n\nBayes classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\).\n\n\n\n\nHOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n. . .\nTo overcome this issue, we several standard solutions:\n\n\nLogistic Regression: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). This was covered in IN1002B.\nClassification Trees: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. What we will cover today.\nEnsemble methods and K-Nearest Neighbours: Estimate \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\) directly. (If time permits).\n\n\n\n\nTwo datasets\n\nThe application of data science algorithms needs two data sets:\n\n\nTraining data is data that we use to train or construct the estimated function \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only.\n\n\n\n\n\n\n\n\n\n\nA random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\nValidation dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\n\n\n\n. . .\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset.\n\n\nExample 2: Identifying Counterfeit Banknotes\n\n\n\n\nDataset\nThe data is located in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7\n\n\n\n\n\n\n\n\n\nHow do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\nThe function has three main inputs:\n\nA pandas dataframe with the predictor columns only.\nA pandas dataframe with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set.\n\n\n\nCreate the predictor matrix\nWe use the function .drop() from pandas. This function drops one or more columns from a data frame. Let’s drop the response column Status and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status']) \nX_full.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\n131.0\n131.1\n9.0\n9.7\n\n\n1\n129.7\n129.7\n8.1\n9.5\n\n\n2\n129.7\n129.7\n8.7\n9.6\n\n\n3\n129.7\n129.6\n7.5\n10.4\n\n\n\n\n\n\n\n\n\nCreate the response column\nWe use the function .filter() from pandas to extract the column Status from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = bank_data.filter(['Status'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nStatus\n\n\n\n\n0\ngenuine\n\n\n1\ngenuine\n\n\n2\ngenuine\n\n\n3\ngenuine\n\n\n\n\n\n\n\n\n\nSet the target category\nTo set the target category in the response we use the get_dummies() function.\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_full, dtype = 'int')\n\n# Select target variable.\nY_target_full = Y_dummies['Status_counterfeit']\n\n# Show target variable.\nY_target_full.head() \n\n0    0\n1    0\n2    0\n3    0\n4    0\nName: Status_counterfeit, dtype: int64\n\n\n\n\nLet’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%.\n\n\n\n\nThe predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n167\n130.4\n130.1\n9.6\n11.2\n1\n\n\n163\n130.1\n130.2\n11.6\n10.9\n1\n\n\n35\n130.2\n130.2\n9.4\n9.7\n0\n\n\n179\n130.2\n130.4\n8.2\n11.8\n1\n\n\n\n\n\n\n\n\n\n\nEquivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head()\n\n\n\n\n\n\n\n\nLeft\nRight\nBottom\nTop\nStatus_counterfeit\n\n\n\n\n19\n130.2\n129.9\n8.6\n10.0\n0\n\n\n86\n129.9\n129.7\n8.7\n9.5\n0\n\n\n156\n129.7\n129.6\n10.3\n11.4\n1\n\n\n87\n129.6\n129.2\n8.4\n10.2\n0\n\n\n149\n129.9\n130.0\n9.9\n12.3\n1\n\n\n\n\n\n\n\n\n\nWork on your training dataset\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nModel development.\n\nWe will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\nNote that all preprocessing techniques will also be applied to the validation and test datasets to prepare it for your model!"
  },
  {
    "objectID": "Module2/Classification.html#classification-trees",
    "href": "Module2/Classification.html#classification-trees",
    "title": "Classification Trees",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nDecision tree\nIt is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n\nMain characteristics:\n\nSimple and useful for interpretation.\nCan handle numerical and categorical predictors and responses.\nComputationally efficient.\nNonparametric technique.\n\n\n\nBasic idea of a decision tree\nStratify or segment the prediction space into several simpler regions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you build a decision tree?\n\nBuilding decision trees involves two main procedures:\n\nGrow a large tree.\nPrune the tree to prevent overfitting.\n\nAfter building a “good” tree, we can predict new observations that are not in the data set we used to build it.\n\n\nHow do we grow a tree?\n\n. . .\nUsing the CART algorithm!\n\n\nThe algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\nBasically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that minimizes the so-called impurity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe repeat the partitioning process until the terminal nodes have no less than, say, 5 observations.\n\n\n\n\n\n\n\n\n\n\n\nWhat is impurity?\nNode impurity refers to the homogeneity of the response classes at that node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CART algorithm minimizes impurity between tree nodes.\n\n\nHow do we measure impurity?\n\n\n\nThere are three different metrics for impurity:\n\nRisk of misclassification.\nCross entropy.\nGini impurity index.\n\n\n \n\nProportion of elements in a class\n\n\n\n\n\n\nPruning the tree\nTo avoid overfitting, we pruned some of the tree’s branches. More specifically, we collapsed two internal (non-terminal) nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo prune a tree, we use an advanced algorithm to measure the contribution of the tree’s branches.\nThe algorithm has a tuning parameter called \\(\\alpha\\), which places greater weight on the number of tree nodes (or size):\n\nLarge values of \\(\\alpha\\) result in small trees with few nodes.\nSmall values of \\(\\alpha\\) result in large trees with many nodes.\n\n\n\nIn Python\n\nIn Python, we can use the DecisionTreeClassifier() and fit() functions from scikit-learn to train a decision tree.\n\n# We tell Python we want a classification tree\nclf = DecisionTreeClassifier(max_depth=5, random_state=507134)\n\n# We train the classification tree using the training data.\nclf.fit(X_train, Y_train)\n\nThe parameters max_depth of DecisionTreeClassifier() controls the depth of the tree. The parameter random_state allows you to reproduce the same tree in different runs of the Python code.\n\n\nPlotting the tree\n\nTo see the decision tree, we use the plot_tree function from scikit-learn and some commands from matplotlib. Specifically, we use the plt.figure() functions to define the size of the figure and plt.show() to display the figure.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf, filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation details\n\nCategorical predictors with unordered levels \\(\\{A, B, C\\}\\). We order the levels in a specific way (works for binary and regression problems).\nPredictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new “NA” level.\nTertiary or quartary splits. There is not much improvement.\nDiagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability.\n\n\n\nApply penalty for large trees\nNow, let’s illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of the \\(\\alpha\\) parameter, which weighs the contribution of the tree branches.\n\nIn Python, this is achieved using the cost_complexity_pruning_path() function with the following syntax.\n\npath = clf.cost_complexity_pruning_path(X_train, Y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n\n\n\nThe ccp_alphas and impurities objects contain the different values of the \\(\\alpha\\) parameter used, as well as the impurity performance of the generated trees.\nTo train a decision tree using different alpha values, we use the following code that iterates over the values contained in ccp_alphas.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=507134, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, Y_train)\n    clfs.append(clf)\n\nIn the next section, we will evaluate the performance of these decision trees."
  },
  {
    "objectID": "Module2/Classification.html#classification-metrics",
    "href": "Module2/Classification.html#classification-metrics",
    "title": "Classification Trees",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nEvaluation\n\nWe evaluate a classification tree by classifying observations that were not used for training.\nThat is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\nIn Python, we use the commands:\n\n# Predict classes.\npredicted_class = clf.predict(X_valid)\n\n\n\n\n\nThe predict() function generates actual classes to which each observation was assigned.\n\npredicted_class\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\n\n\nThe predict_proba() function generates the probabilities used by the algorithm to assign the classes.\n\n# Predict probabilities.\npredicted_probability = clf.predict_proba(X_valid)\npredicted_probability\n\narray([[0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571],\n       [0.48571429, 0.51428571]])\n\n\n\n\nConfusion matrix\n\nTable to evaluate the performance of a classifier.\nCompares actual values with the predicted values of a classifier.\nUseful for binary and multiclass classification problems.\n\n\n\n\n\n\n\n\nIn Python\n\nWe calculate the confusion matrix using the homonymous function scikit-learn.\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Show confusion matrix.\nprint(cm)\n\n[[ 0 32]\n [ 0 28]]\n\n\n\n\n\n\nWe can display the confusion matrix using the ConfusionMatrixDisplay() function.\n\nConfusionMatrixDisplay(cm).plot()\n\n\n\n\n\n\n\n\n\n\nAccuracy\nA simple metric for summarizing the information in the confusion matrix is accuracy. It is the proportion of correct classifications for both classes, out of the total classifications performed.\nIn Python, we calculate accuracy using the scikit-learn accuracy_score() function.\n\naccuracy = accuracy_score(Y_valid, predicted_class)\nprint( round(accuracy, 2) )\n\n0.47\n\n\nThe higher the accuracy, the better the performance of the classifier.\n\n\nLet’s get back to the penalized trees\n\nNow, for each of those trees contained in clfs, we evaluate the performance in terms of accuracy for the training and validation data.\n\ntrain_scores = [clf.score(X_train, Y_train) for clf in clfs]\nvalidation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]\n\n\n\n\nWe visualize the results using the following plot.\n\n\nCode\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and validation data\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nChoosing the best tree\nWe can see that the best \\(\\alpha\\) value for the validation dataset is 0.01.\nTo train our new reduced tree, we use the DecisionTreeClassifier() function again with the ccp_alpha parameter set to the best \\(\\alpha\\) value. The object containing this new tree is clf_simple.\n\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.01)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\n\n\nOnce this is done, we can visualize the small tree using the plot_tree() function.\n\nplt.figure(figsize=(6, 6))\nplot_tree(clf_simple, filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEvaluation\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.93\n\n\n\n\nComments\n\n\nAccuracy is easy to calculate and interpret.\nIt works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\nHowever, there are situations in which identifying the target class is more important than the reference class.\nFor example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading.\n\n\n\nAn example\n\nLet’s say we want to create a classifier that tells us whether a mobile phone company’s customer will churn next month.\nCustomers who churn significantly decrease the company’s revenue. That’s why it’s important to retain these customers.\nTo retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\nIdeally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay.\n\n\n\n\n\n\nIn other words, we want to avoid making wrong decisions about customers who will churn.\nWrong decisions about loyal customers aren’t as relevant.\nBecause if we classify a loyal customer as one who will churn, the customer will get a good deal. They’ll probably pay less but stay anyway.\n\n\n\nAnother example\n\nAnother example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\nThe classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\nA healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease.\n\n\n\nClassification-specific metrics\n\nTo overcome this limitation of accuracy and error rate, there are several class-specific metrics. The most popular are:\n\nSensitivity or recall\nPrecision\nType I error\n\nThese metrics are calculated from the confusion matrix.\n\n\n\n\n\n\n\n\nSensitivity or recall = OO/(OO + OR) “How many records of the target class did we predict correctly?”\n\n\n\n\n\n\n\n\nPrecision = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?\n\n\n\n\nIn Python, we compute sensitivity and precision using the following commands:\n\nrecall = recall_score(Y_valid, predicted_class)\nprint( round(recall, 2) )\n\n1.0\n\n\n\n\nprecision = precision_score(Y_valid, predicted_class)\nprint( round(precision, 2) )\n\n0.47\n\n\n\n\n\n\n\n\n\n\nType I error = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”\n\n\n\n\nUnfortunately, there is no simple command to calculate the type-I error in sklearn. To overcome this issue, we must calculate it manually.\n\n# Confusion matrix to compute Type-I error\ntn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n\n# Type-I error rate = False Positive Rate = FP / (FP + TN)\ntype_I_error = fp / (fp + tn)\nprint( round(type_I_error, 2) )\n\n1.0\n\n\n\n\nDiscussion\n\n\nThere is generally a trade-off between sensitivity and Type I error.\nIntuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\nPossible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error.\n\n\n\nActivity 2.1: Classification and Metrics (solo mode)\nUsing the data in the “weight-height.csv” table, apply the CART procedure to build a decision tree useful for predicting a person’s sex based on their weight and height.\nIn this example, the predictor variables are continuous, and the predictor variable is binary.\nInterpret the Precision, Accuracy, Sensitivity, and Type 1 Error values for the validation set. If the software doesn’t report them, perform the calculations using the confusion matrix. Use “Female” as the target class. Discuss the effectiveness of the resulting model.\n\n\nDisadvantages of decision trees\n\nDecision trees have high variance. A small change in the training data can result in a very different tree.\nIt has trouble identifying simple data structures."
  },
  {
    "objectID": "Module2/Classification.html#return-to-main-page",
    "href": "Module2/Classification.html#return-to-main-page",
    "title": "Classification Trees",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/Autocorrelation.html",
    "href": "Module3/Autocorrelation.html",
    "title": "Autocorrelation Models",
    "section": "",
    "text": "Autocorrelation\nThe ARIMA model\nThe SARIMA model"
  },
  {
    "objectID": "Module3/Autocorrelation.html#autocorrelation",
    "href": "Module3/Autocorrelation.html#autocorrelation",
    "title": "Autocorrelation Models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nProblem with linear regression models\n\nLinear regression models do not incorporate the dependence between consecutive values in a time series.\nThis is unfortunate because responses recorded over close time periods tend to be correlated. This correlation is called the autocorrelation of the time series.\nAutocorrelation helps us develop a model that can make better predictions of future responses.\n\n\nWhat is correlation?\n\n\n\n\n\nIt is a measure of the strength and direction of the linear relationship between two numerical variables.\nSpecifically, it is used to assess the relationship between two sets of observations.\nCorrelation is between \\(-1\\) and 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we measure autocorrelation?\n\nThere are two formal tools for measuring the correlation between observations in a time series:\n\n\nThe autocorrelation function.\nThe partial autocorrelation function.\n\n\n\n\nThe autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before.\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation function\n\nMeasures the correlation between responses separated by \\(j\\) periods.\n\nFor example, consider the autocorrelation between the current temperature and the temperature recorded the day before. This would be the correlation between these two columns\n\n\n\n\n\n\n\nExample 1\nLet’s consider again the dataset in the file “Amtrak.xlsx.” The file contains records of Amtrak passenger numbers from January 1991 to March 2004.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation function\n\n\nThe autocorrelation function measures the correlation between responses that are separated by a specific number of periods.\nThe autocorrelation function is commonly visualized using a bar chart.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the correlations between observations at different lags.\n\n\n\nAutocorrelation plot\n\nIn Python, we use the plot_acf function from the statsmodels library.\n\nplt.figure(figsize=(10, 6))\nplot_acf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()\n\nThe lags parameter controls the number of periods for which to compute the autocorrelation function.\n\n\nThe resulting plot\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation plot shows that the responses and those from zero periods ago have a correlation of 1.\nThe autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45.\nThe autocorrelation plot shows that the responses and those from 24 periods ago have a correlation of around 0.5.\n\n\n\n\n\n\n&lt;Figure size 384x384 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation patterns\n\n\nA strong autocorrelation (positive or negative) with a lag \\(j\\) greater than 1 and its multiples (\\(2k, 3k, \\ldots\\)) typically reflects a cyclical pattern or seasonality.\nPositive lag-1 autocorrelation describes a series in which consecutive values generally move in the same direction.\nNegative lag-1 autocorrelation reflects oscillations in the series, where high values (generally) are immediately followed by low values and vice versa.\n\n\n\nMore about the autocorrelation function\nConsider the problem of predicting the average price of a kilo of avocado this month.\nFor this, we have the average price from last month and the month before that.\n\n\n\n\n\n\n\n\n\nThe autocorrelation function for \\(Y_t\\) and \\(Y_{t-2}\\) includes the direct and indirect effect of \\(Y_{t-2}\\) on \\(Y_t\\).\n\n\n\n\n\n\n\nPartial autocorrelation function\n\nMeasures the correlation between responses that are separated by \\(j\\) periods, excluding correlation due to responses separated by intervening periods.\n\n\n\n\n\n\n\n\n\nIn technical terms, the partial autocorrelation function fits the following linear regression model\n\\[\\hat{Y}_t = \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}\\] Where:\n\n\\(\\hat{Y}_{t}\\) is the predicted response at the current time (\\(t\\)).\n\\(\\hat{\\beta}_1\\) is the direct effect of \\(Y_{t-1}\\) on predicting \\(Y_{t}\\).\n\\(\\hat{\\beta}_2\\) is the direct effect of \\(Y_{t-2}\\) on predicting \\(Y_{t}\\).\n\nThe partial autocorrelation between \\(Y_t\\) and \\(Y_{t-2}\\) is equal to \\(\\hat{\\beta}_2\\).\n\n\n\n\nThe partial autocorrelation function is visualized using a graph similar to that for autocorrelation.\nThe vertical axis shows the differences (or lags) between the periods considered, and the horizontal axis shows the partial correlations between observations at different lags.\n\nIn Python, we use the plot_pacf function from statsmodels.\n\nplt.figure(figsize=(10, 6))\nplot_pacf(Amtrak_data['Ridership (in 000s)'], lags = 25)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Correlation\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe partial autocorrelation plot shows that the responses and those from one period ago have a correlation of around 0.45. This is the same for the autocorrelation plot.\nThe partial autocorrelation plot shows that the responses and those from two periods ago have a correlation near 0.\n\n\n\n\n\n&lt;Figure size 960x576 with 0 Axes&gt;"
  },
  {
    "objectID": "Module3/Autocorrelation.html#the-arima-model",
    "href": "Module3/Autocorrelation.html#the-arima-model",
    "title": "Autocorrelation Models",
    "section": "The ARIMA Model",
    "text": "The ARIMA Model\n\nAutoregressive models\nAutoregressive models are a type of linear regression model that directly incorporate the autocorrelation of the time series to predict the current response.\nTheir main characteristic is that the predictors of the current value of the series are its past values.\n\nAn autoregressive model of order 2 has the mathematical form: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2}.\\)\nAn order 3 model looks like this: \\(\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Y_{t-1} + \\hat{\\beta}_2 Y_{t-2} + \\hat{\\beta}_3 Y_{t-3}.\\)\n\n\n\nARIMA models\n\nA special class of autoregressive models are ARIMA (Autoregressive Integrated Moving Average).\n. . .\nAn ARIMA model consists of three elements:\n\n\nIntegrated operators (integrated).\nAutoregressive terms (autoregressive).\nStochastic terms (moving average).\n\n\n\n\n1. Integrated or differentiatedoperators (I)\n\nThey create a new variable \\(Z_t\\), which equals the difference between the current response and the delayed response by a number of periods or lags.\nThere are three common levels of differentiation:\n\nLevel 0: \\(Z_t = Y_t\\).\nLevel 1: \\(Z_t = Y_t - Y_{t-1}\\).\nLevel 2: \\(Z_t = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})\\).\n\n\n\nExample 2\nWe consider the time series “CanadianWorkHours.xlsx” that contains the average hours worked by a certain group of workers over a certain range of years.\n\nCanadianWorkHours = pd.read_excel('CanadianWorkHours.xlsx')\nCanadianWorkHours.head(4)\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n0\n1966\n37.2\n\n\n1\n1967\n37.0\n\n\n2\n1968\n37.4\n\n\n3\n1969\n37.5\n\n\n\n\n\n\n\n\n\nCreating a train and a validation data\nRecall that we would like to train the model on earlier time periods and test it on later ones. To this end, we make the split using the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(CanadianWorkHours) * split_ratio)\n\n# Split the data\nCanadian_train = CanadianWorkHours[:split_point]\nCanadian_validation = CanadianWorkHours[split_point:]\n\nWe use 80% of the time series for training and the rest for validation.\n\n\nTraining data\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x='Year', y='Hours per Week', data = Canadian_train)\nplt.xlabel('Year')\nplt.ylabel('Hours per Week')\nplt.title('Hours per Week Over Time')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn statsmodels, we apply the integration operator using the pre-loaded diff() function. The function’s k_diff argument specifies the order or level of the operator.\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Canadian_train['Hours per Week'], k_diff = 1)\n\n\n\n\nThe time series with a level-1 operator looks like this.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_one)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 1')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nA level-2 operator would work like this.\n\nZ_series_two = diff(Canadian_train['Hours per Week'], k_diff = 2)\n\n\n\nCode\nplt.figure(figsize=(9, 5))\nsns.lineplot(x=Canadian_train['Year'], y=Z_series_two)\nplt.xlabel('Year')\nplt.ylabel('Difference Level 2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the level 2 operator is more successful in removing the trend from the original time series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\n\nThe differentiation operator removes or de-trends the time series.\n\nThe level 0 differentiation operator leaves the time series intact.\nThe level 1 differentiation operator removes its linear trend.\nThe level 2 differentiation operator removes its quadratic trend.\n\n\n\nHow do we determine the operator level?\n\nVisualizing the time series and determining whether there is a linear or quadratic trend.\nIf level 1 and level 2 operators yield similar results, we choose level 1 because it is simpler.\nOnce this is done, we set our transformed variable \\(Z_t\\) as the new response variable.\n\n\n2. Autoregressive (AR) terms\n\nHere we use autoregressive models, but with the new response variable \\(Z_t\\).\nWe can have different levels of order (or number of terms) in the autoregression model. For example:\n\nOrder 1 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1}\\).\nOrder 2 model: \\(\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2}\\).\n\nIf necessary, we can exclude the constant coefficient \\(\\hat{\\beta}_0\\) from the model.\n\n\nHow do we determine the number of terms?\n\nUsing the correlation functions (ACF) and partial correlation functions (PACF) of the differenced series.\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nTo achieve this, we have some rules.\n\n\nRule 1\n\nA first-order autoregressive model has:\n\nAn ACF with a single peak at the first period difference (lag).\nAn ACF with exponentially decreasing correlations.\n\n\n\nRule 2\n\nA second-order autoregressive model has:\n\nA PACF with two peaks at the first period differences (lags).\nAn ACF with correlations that decrease positively and negatively but approach zero.\n\n\n\nRule 3\n\nIf the PACF of the differenced series \\(Z_t\\) shows a higher partial correlation than the others and/or the lag-1 autocorrelation is positive, then consider adding an AR term to the model.\n\nThe lag at which the PACF cuts off from the confidence limits in the software is the indicated number of AR terms.\n\n\n\nFollowing part 1 of Rule 1, we conclude that an autoregressive term of order 2 will be sufficient to capture the relationships between the elements of the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n3. Stochastic Terms (Moving Averages, MA)\nInstead of using past values of the response variable, a moving average model uses stochastic terms to predict the current response. The model has different versions depending on the number of errors used to predict the response. For example:\n\nMA of order 1: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1}\\);\nMA of order 2: \\(Z_t = \\theta_0 + \\theta_1 a_{t-1} + \\theta_2 a_{t-2}\\),\n\nwhere \\(\\theta_0\\) is a constant and \\(a_t\\) are terms from a white noise series (i.e., random terms).\n\n\nHow do I choose the order of the MAs?\n\nRule 4: MA models have:\n\nCorrelations other than 0 in the ACF. The lags at which this occurs indicate the terms to include in the MA model.\nCorrelations in the PACF that gradually decrease to zero in some way.\n\n\n\n\n\nThis means that to determine the order, we focus primarily on the autocorrelation function. Remember, it’s the autocorrelation function of the series after it’s been differentiated.\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nSince there is no significant correlation for any lag above 0, we do not need any MA elements to model the series.\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nARIMA\n\n\n\nDefine the response differentiation level and create \\(Z_t\\).\nDefine the order of the AR model (e.g., order 2).\nDefine the order of the MA model (e.g., order 1).\n\n\n. . .\n\n\\[\\hat{Z}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 Z_{t-1} + \\hat{\\beta}_2 Z_{t-2} + \\theta_1 a_{t-1}\\]\n\n. . .\nThe ARIMA model coefficients are estimated using an advanced method that takes into account the dependencies between the time series responses.\n\n\nARIMA in Pyhon\n\nTo fit an ARIMA model, we use the ARIMA() function from statsmodels.\nThe function has an important argument called order, which equals (p,d,q), where\n\np is the order of the autoregressive model.\nd is the level of the integration or differencing operator.\nq is the number of elements in the moving average.\n\n\n\n\n\nFrom our previous analysis of the training data for the Canadian workhours example, we conclude that:\n\nWe must use a level-2 differencing operator to remove the quadratic trend from the series. Therefore, d = 2.\nOne autoregressive term should be sufficient to capture the patterns in the time series. Therefore, p = 2.\nIt is not necessary to have moving average terms. Therefore, q = 0.\n\n\n\n\n\nOnce this is defined, we can train an ARIMA model using the training data with the following code:\n\nArima_Canadian = ARIMA(Canadian_train['Hours per Week'], \n                       order=(2, 2, 0))\nresults_ARIMA_Canadian = Arima_Canadian.fit()\n\nTechnically, ARIMA() defines the model and .fit() fits the model to the data using maximum likelihood estimation.\n\n\n\nAfter fitting, we can get a summary of the model fit using the following code.\n\nprint(results_ARIMA_Canadian.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:         Hours per Week   No. Observations:                   28\nModel:                 ARIMA(2, 2, 0)   Log Likelihood                  -2.537\nDate:                Thu, 07 Aug 2025   AIC                             11.074\nTime:                        11:11:45   BIC                             14.849\nSample:                             0   HQIC                            12.161\n                                 - 28                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.3236      0.403     -0.803      0.422      -1.113       0.466\nar.L2         -0.4401      0.250     -1.760      0.078      -0.930       0.050\nsigma2         0.0699      0.021      3.356      0.001       0.029       0.111\n===================================================================================\nLjung-Box (L1) (Q):                   0.26   Jarque-Bera (JB):                 3.02\nProb(Q):                              0.61   Prob(JB):                         0.22\nHeteroskedasticity (H):               1.48   Skew:                             0.74\nProb(H) (two-sided):                  0.57   Kurtosis:                         3.75\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\n\n\nThe next step in evaluating an ARIMA model is to study the model’s residuals to ensure there is nothing else to explain in the model.\n\nWe can obtain the residuals using the following code.\n\nARIMA_residuals = results_ARIMA_Canadian.resid\nARIMA_residuals = ARIMA_residuals.drop(0)\n\n\n\nTime series of residuals\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=Canadian_train['Year'], y=ARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCorrelation plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe three graphs show no obvious patterns or significant correlations between the residuals. Therefore, we say the model is correct.\n\n\nForecast\nOnce the model is validated, we make predictions for elements in the time series.\n\nTo predict the average number of hours worked in the next, say, 3 years, we use the .forecast() function. The steps argument indicates the number of steps in the future to make the predictions.\n\nresults_ARIMA_Canadian.forecast(steps = 3)\n\n28    36.102917\n29    36.205233\n30    36.382827\nName: predicted_mean, dtype: float64\n\n\n\n\nModel evaluation using MSE\n\nInstead of evaluating the ARIMA model using graphical analyses of the residuals, we can take a more data-driven approach and evaluate the model using the mean squared error (MSE) or root MSE.\nTo this end, we simply use the mean_squared_error() function with the validation responses and our predictions.\n\n\nValidation data\n\nCanadian_validation\n\n\n\n\n\n\n\n\nYear\nHours per Week\n\n\n\n\n28\n1994\n36.0\n\n\n29\n1995\n35.7\n\n\n30\n1996\n35.7\n\n\n31\n1997\n35.5\n\n\n32\n1998\n35.6\n\n\n33\n1999\n36.3\n\n\n34\n2000\n36.5\n\n\n\n\n\n\n\n\n\n\n\nThe validation data has 7 time periods that can be determined using the command below.\n\nlen(Canadian_validation)\n\n7\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Canadian = results_ARIMA_Canadian.forecast(steps = 7)\nforecast_Canadian\n\n28    36.102917\n29    36.205233\n30    36.382827\n31    36.580331\n32    36.738265\n33    36.900243\n34    37.078325\nName: predicted_mean, dtype: float64\n\n\n\n\n\n\nUsing the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(mse**(1/2), 2))\n\n0.75\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Canadian_validation[\"Hours per Week\"], forecast_Canadian)  \nprint(round(rtwo, 2))\n\n-3.52\n\n\n\nA negative signifies that the model’s predictions are worse than simply predicting the average of the response"
  },
  {
    "objectID": "Module3/Autocorrelation.html#the-sarima-model",
    "href": "Module3/Autocorrelation.html#the-sarima-model",
    "title": "Autocorrelation Models",
    "section": "The SARIMA Model",
    "text": "The SARIMA Model\n\nSeasonality\n\n\nSeasonality consists of repetitive or cyclical behavior that occurs with a constant frequency.\nIt can be identified from the series graph or using the ACF and PACF.\nTo do this, we must have removed the trend.\n\n\n\nExample 3\nWe use the Airline data containing the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121\n\n\n\n\n\n\n\n\n\nCreate training and validation data\n\nWe use 80% of the time series for training and the rest for validation.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Airline_data) * split_ratio)\n\n# Split the data\nAirline_train = Airline_data[:split_point]\nAirline_validation = Airline_data[split_point:]\n\n\n\nTraining data\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_train)\nplt.xlabel('T')\nplt.ylabel('Number of passengers')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, let’s use a level-1 operator.\n\nZ_series_one = diff(Airline_train['Number of passengers'], k_diff = 1)\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.lineplot(x=Airline_train['Number of passengers'], y=Z_series_one)\nplt.xlabel('T')\nplt.ylabel('Difference Level 1')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation plots\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 288x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSARIMA model\nThe SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an extension of the ARIMA model for modeling seasonality patterns.\nThe SARIMA model has three additional elements for modeling seasonality in time series.\n\nDifferenced or integrated operators (integrated) for seasonality.\nAutoregressive terms (autoregressive) for seasonality.\nStochastic terms or moving averages (moving average) for seasonality.\n\n\n\nNotation\n\nSeasonality in a time series is a regular pattern of change that repeats over \\(S\\) time periods, where \\(S\\) defines the number of time periods until the pattern repeats again.\nFor example, there is seasonality in monthly data, where high values always tend to occur in some particular months and low values always tend to occur in other particular months.\nIn this case, \\(S=12\\) (months per year) is the length of periodic seasonal behavior. For quarterly data, \\(S=4\\) time periods per year.\n\n\nSeasonal differentiation\n\nThis is the difference between a response and a response with a lag that is a multiple of \\(S\\).\nFor example, with monthly data \\(S=12\\),\n\nA level 1 seasonal difference is \\(Y_{t} - Y_{t-12}\\).\nA level 2 seasonal difference is \\((Y_{t-12}) - (Y_{t-12} - Y_{t-24})\\).\n\nSeasonal differencing eliminates the seasonal trend and can also eliminate a type of nonstationarity caused by a seasonal random walk.\n\n\nSeasonal AR and MA Terms\nIn SARIMA, the seasonal AR and MA component terms predict the current response (\\(Y_t\\)) using responses and errors at times with lags that are multiples of \\(S\\).\nFor example, with monthly data \\(S = 12\\),\n\n\nThe first-order seasonal AR model would use \\(Y_{t-12}\\) to predict \\(Y_{t}\\).\nThe second-order seasonal AR model would use \\(Y_{t-12}\\) and \\(Y_{t-24}\\) to predict \\(Y_{t}\\).\nThe first-order seasonal MA model would use the stochastic term \\(a_{t-12}\\) as a predictor.\nThe second-order seasonal MA model would use the stochastic terms \\(a_{t-12}\\) and \\(a_{t-24}\\) as predictors.\n\n\n\n\n\n\nTo fit the SARIMA model, we use the ARIMA() function from statsmodels, but with an additional argument, seasonal_order=(0, 0, 0, 0).\n\nThis is the order (P, D, Q, s) of the seasonal component of the model for the autoregressive parameters, differencing operator levels, moving average parameters, and periodicity.\n\nRecall that the function has the argument order = (p, d, q) where p is the order of the autoregressive model, d is the differencing operator level, and q is the number of elements in the moving average.\nThese arguments capture the detailed information of the time series, while seasonal_order captures the patterns given by seasonality.\n\n\n\n\n\nLet’s fit a SARIMA model.\n\nSARIMA_model = ARIMA(Airline_train['Number of passengers'], order=(1, 2, 1), \n                      seasonal_order=(1, 1, 0, 12))\nSARIMA_Airline = SARIMA_model.fit()\n\n\n\nSummary of fit\n\nprint(SARIMA_Airline.summary())\n\n                                     SARIMAX Results                                     \n=========================================================================================\nDep. Variable:              Number of passengers   No. Observations:                  115\nModel:             ARIMA(1, 2, 1)x(1, 1, [], 12)   Log Likelihood                -374.241\nDate:                           Thu, 07 Aug 2025   AIC                            756.482\nTime:                                   11:11:47   BIC                            766.943\nSample:                                        0   HQIC                           760.717\n                                           - 115                                         \nCovariance Type:                             opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.1729      0.094     -1.833      0.067      -0.358       0.012\nma.L1         -0.9999     14.439     -0.069      0.945     -29.300      27.300\nar.S.L12      -0.1303      0.084     -1.543      0.123      -0.296       0.035\nsigma2        91.7900   1326.009      0.069      0.945   -2507.139    2690.719\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 3.18\nProb(Q):                              0.98   Prob(JB):                         0.20\nHeteroskedasticity (H):               1.13   Skew:                             0.39\nProb(H) (two-sided):                  0.73   Kurtosis:                         2.64\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n\nResidual analysis\nWe can have a graphical evaluation of the model’s performance using a residual analysis.\n\nSARIMA_residuals = SARIMA_Airline.resid\nSARIMA_residuals = SARIMA_residuals.drop(0)\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\nsns.lineplot(x=Airline_train['T'], y=SARIMA_residuals)\nplt.xlabel('Year')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 576x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidation\nThe validation data has 29 time periods that can be determined using the command below.\n\nlen(Airline_validation)\n\n29\n\n\n\nSo, we must forecast 7 periods ahead using our ARIMA model.\n\nforecast_Airline = SARIMA_Airline.forecast(steps = 29)\nforecast_Airline.head(3)\n\n115    489.635426\n116    428.961078\n117    373.169684\nName: predicted_mean, dtype: float64\n\n\n\n\n\n\nUsing the forecast and validation data, we compute the RMSE.\n\nmse = mean_squared_error(Airline_validation[\"Number of passengers\"], \n                        forecast_Airline)  \nprint(round(mse**(1/2), 2))\n\n26.39\n\n\n\nWe can also compute the \\(R^2\\) score.\n\nrtwo = r2_score(Airline_validation[\"Number of passengers\"], forecast_Airline)  \nprint(round(rtwo, 2))\n\n0.89"
  },
  {
    "objectID": "Module3/Autocorrelation.html#return-to-main-page",
    "href": "Module3/Autocorrelation.html#return-to-main-page",
    "title": "Autocorrelation Models",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/PredictiveModels.html",
    "href": "Module3/PredictiveModels.html",
    "title": "Predictive Models and Time Series",
    "section": "",
    "text": "Introduction\nTime Series\nLinear Regression Model for Time Series"
  },
  {
    "objectID": "Module3/PredictiveModels.html#introduction",
    "href": "Module3/PredictiveModels.html#introduction",
    "title": "Predictive Models and Time Series",
    "section": "Introduction",
    "text": "Introduction\n\nLoad the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical.\n\n\nMain data science problems\n\nRegression Problems. The response is numerical. For example, a person’s income, the value of a house, or a patient’s blood pressure.\nClassification Problems. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be numerical or categorical.\n\n\nRegression problem\n\nGoal: Find the best function \\(f(\\mathbf{X})\\) of the predictors \\(\\mathbf{X} = (X_1, \\ldots, X_p)\\) that describes the response \\(Y\\).\nIn mathematical terms, we want to establish the following relationship:\n\\[Y = f(\\mathbf{X}) + \\epsilon\\]\n\nWhere \\(\\epsilon\\) is a natural (random) error.\n\n\n\nHow to find the shape of \\(f(X)\\)?\n\nUsing training data. \n\n\nHow to find the shape of \\(f(X)\\)?\n\nUsing training data.\n\n\n\n\n\n\n\nHow to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data.\n\n\n\n\n\n\n\n\n\n\n\nHow to evaluate the quality of the candidate function \\(\\hat{f}(X)\\)?\n\n\n\nUsing validation data.\n\n\n\n\n\n\n\n\n\n\n\nMoreover…\n\n\n\n\nWe can use test data for a final evaluation of the model.\nTest data is data obtained from the process that generated the training data.\nTest data is independent of the training data.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression Model\nA common candidate function for predicting a response is the linear regression model. It has the mathematical form:\n\\[\\hat{Y}_i = \\hat{f}(X_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i.\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) associated with a predictor value equal to \\(X_i\\).\nThe values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are called the coefficients of the model.\n\n\n\n\n\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the test data set and the least squares method.\nThis method finds the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the error made by the model \\(\\hat{f}(X_i)\\) when trying to predict the responses of the training set.\n\nTechnically, the method minimizes the following expression\n\n\\[(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_t} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_t} ))^2 \\]\n\nFor the \\(n_t\\) the training data!\n\n\nThe idea in two dimensions\n\n\n\n\n\n\n\nExample 1\n\nWe used the dataset called “Advertising.xlsx” in Canvas.\n\nTV: Money spent on TV ads for a product ($).\nSales: Sales generated from the product ($).\n200 markets\n\n\n# Load the data into Python\nAds_data = pd.read_excel('Advertising.xlsx')\n\n\n\n\n\n\nAds_data.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n\n\n\n\n\n\n\n\n\nNow, let’s choose our predictor and response. In the definition of X_full, the double bracket in [] is important because it allows us to have a pandas DataFrame as output. This makes it easier to fit the linear regression model with scikit-learn.\n\n# Chose the predictor.\nX_full = Ads_data.filter(['TV'])\n\n# Set the response.\nY_full = Ads_data.filter(['Sales'])\n\n\n\nCreate training and validation data\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into a training dataset and a validation dataset. To do this, we use the scikit-learn train_test_split() function.\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.25,\n                                                      random_state = 301655)\n\nWe use 75% of the data for training and the rest for validation.\n\n\nFit a linear regression model in Python\n\nIn Python, we use the LinearRegression() and fit() functions from the scikit-learn to fit a linear regression model.\n\n# 1. Create the linear regression model\nLRmodel = LinearRegression()\n\n# 2. Fit the model.\nLRmodel.fit(X_train, Y_train)\n\n\n\n\nThe following commands allow you to show the estimated coefficients of the model.\n\nprint(\"Coefficients:\", LRmodel.coef_)\n\nCoefficients: [[0.05185463]]\n\n\nWe can also show the estimated intercept.\n\nprint(\"Intercept:\", LRmodel.intercept_)\n\nIntercept: [6.69303889]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 6.69 + 0.051 X_i.\\]\n\n\nLinear regression model assumptions\n\nTo use the regression model, the model errors \\(e_i = Y_i - \\hat{Y}_i\\) obtained on the training data must meet three conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nThese assumptions are evaluated using a graphical analysis of residuals (model errors).\n\n\nIn Python\nTo conduct a residual analysis, we need to obtain the predicted values and residuals of the model first (on the training data).\n\n# Fitted values.\nfitted = LRmodel.predict(X_train)\n\n# Residuals\nresiduals = Y_train - fitted\n\nFor plotting, we put everything together in a pandas dataframe.\n\n# Create a DataFrame for plotting\n#results_df = pd.DataFrame()\n#results_df['predicted'] = list(fitted)\n#results_df['actual'] = list(Y_train)\n#results_df['residual'] = results_df['predicted'] - results_df['actual']\n#results_df = results_df.sort_values(by='residual').reset_index(drop=True)\n#results_df.describe()\n\n\n\n\n\n\nCode\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = fitted, y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Fitted\")\n#plt.xlabel(\"Fitted values\")\n#plt.ylabel(\"Residuals\")\n#plt.show()\n\n\n\n\nCode\n# Plot 1: Residuals vs Fitted\n#plt.figure(figsize=(6, 4))\n#sns.scatterplot(x = range(len(residuals)), y = residuals)\n#plt.axhline(0, color='red', linestyle='--')\n#plt.title(\"Residuals vs Row Order\")\n#plt.xlabel(\"Observation Order\")\n#plt.ylabel(\"Residuals\")\n#plt.show()\n\n\n\n\nPrediction error\nAfter estimating and validating the linear regression model, we can check the quality of its predictions on unobserved data. That is, on the data in the validation set.\nOne metric for this is the mean prediction error (MSE\\(_v\\)):\n\n\\[\\text{MSE}_v = \\frac{(Y_1 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 ))^2 + (Y_2 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_2 ))^2 + \\cdots + (Y_{n_v} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_{n_v} ))^2}{n_v}\\]\n\n\nFor \\(n_v\\), the validation data!\n\nThe smaller \\(\\text{MSE}_v\\), the better the predictions.\n\n\n\n\nIn practice, the square root of the mean prediction error is used:\n\\[\\text{RMSE}_v = \\sqrt{\\text{MSE}_v}.\\]\nThe advantage of \\(\\text{RMSE}_v\\) is that it can be interpreted as:\n\nThe average variability of a model prediction.\n\nFor example, if \\(\\text{RMSE}_v = 1\\), then a prediction of \\(\\hat{Y} = 5\\) will have an (average) error rate of \\(\\pm 1\\).\n\n\nIn Python\n\nTo evaluate the model’s performance, we use the validation dataset. Specifically, we use the predictor matrix stored in X_valid.\n\nIn Python, we make the prediction using the pre-trained LRmodel.\n\nY_pred = LRmodel.predict(X_valid)\n\n\n\n\n\nTo evaluate the model, we use the mean squared error in the Python mse() function. Recall that the responses from the validation dataset are in Y_valid, and the model predictions are in Y_pred.\n\nmse = mean_squared_error(Y_valid, Y_pred)  # Mean Squared Error (MSE)\nprint(round(mse, 2))\n\n15.49\n\n\n\nTo obtain the root mean squared error (RMSE), we simply take the square root of the MSE.\n\nprint(round(mse**(1/2), 2))\n\n3.94\n\n\n\n\nAnother Metric: \\(R^2\\)\n\nIn the context of Data Science, \\(R^2\\) can be interpreted as the squared correlation between the actual responses and those predicted by the model.\nThe higher the correlation, the better the agreement between the predicted and actual responses.\n\n\nWe compute \\(R^2\\) in Python as follows:\n\nrtwo_sc = r2_score(Y_valid, Y_pred)  # Rsquared\nprint(round(rtwo_sc, 2))\n\n0.33\n\n\n\n\nMini-Activity (cooperative mode)\n\n\nConsider the Advertising.xlsx dataset in Canvas.\nUse a model to predict Sales that includes the Radio predictor (money spent on radio ads for a product ($)). What is the \\(\\text{RMSE}_v\\)?\nNow, use a model to predict Sales that includes two predictors: TV and Radio. What is the \\(\\text{RMSE}_v\\)?\nWhich model do you prefer?\n\n\n\nOther candidate functions\nThe linear regression model is one of the most common models for predicting a response. It is simple and easy to calculate and interpret.\nHowever, it can be limited for very complex problems.\nFor this purpose, there are other, more advanced candidate functions \\(\\hat{f}(X)\\), such as:\n\nK nearest neighbors.\nDecision or regression trees.\nEnsamble methods (bagging and random forest)."
  },
  {
    "objectID": "Module3/PredictiveModels.html#time-series",
    "href": "Module3/PredictiveModels.html#time-series",
    "title": "Predictive Models and Time Series",
    "section": "Time Series",
    "text": "Time Series\n\nWhat is a time series?\n\n\nIt is a sequence of observations collected at successive time intervals.\nTime series data is commonly used in fields such as finance, economics, weather forecasting, signal processing, and many others.\nAnalyzing time series data helps us understand patterns, trends, and behaviors over time, enabling prediction, anomaly detection, and decision-making.\n\n\n\nExample 2\n\n\n\n\n\n\n\n\n\nTechnically, a time series is a set of observations about a (discrete) predictor \\(T\\) and a response \\(Y\\).\nObservations of \\(Y\\) are recorded at the moments or times given by the predictor \\(T\\).\nThe special feature of the time series is that the observations of \\(Y\\) are not independent!\n\n\n\n\n\nDay\nT\nTemperature (Y)\n\n\n\n\nMonday\n1\n10\n\n\nTuesday\n2\n12\n\n\nWednesday\n3\n15\n\n\nThursday\n4\n14\n\n\nFriday\n5\n18\n\n\n\n\n\n\n\n\nExample 3: Amtrak data\n\n\nThe Amtrak train company in the USA collects data on the number of passengers traveling on its trains.\nRecords are available from January 1991 to March 2004.\nThe data is available in “Amtrak.xlsx” on Canvas.\n\n\nAmtrak_data = pd.read_excel('Amtrak.xlsx')\n\n\n\n\n\n\nAmtrak_data.head()\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay\n\n\n\n\n\n\n\n\n\nTime series plot in Python\nWe can create a line graph to visualize the evolution of Amtrak train ridership over time using lineplot from seaborn.\n\n\nCode\nplt.figure(figsize=(6, 4))\nsns.lineplot(x='Month', y='Ridership (in 000s)', data = Amtrak_data)\nplt.xlabel('Month')\nplt.ylabel('Ridership')\nplt.title('Amtrak Ridership Over Time')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInformative Series\n\nAn informative time series is a series that contains patterns that we can use to predict future values of the series.\nThe three possible patterns are:\n\n\nTrend: the series has an increasing/decreasing behavior.\nSeasonality: the series has a repeating cyclical pattern in its values.\nAutocorrelation: the series follows a pattern that can be described by previous values of the series.\n\n\n\n\nExample 4: Airline data\n\n\n\n\n\nThis series has an upper trend.\nThis series has cyclical patterns in its values.\nAlthough not immediately visible, we can use the previous values of the series to describe the future ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-informative series: White noise\n\n\n\n\n\nWhite noise is a series whose values, on average, are 0 and have a constant variation.\nIts values are also independent of each other.\nIt is used to describe random or natural error."
  },
  {
    "objectID": "Module3/PredictiveModels.html#linear-regression-model-for-time-series",
    "href": "Module3/PredictiveModels.html#linear-regression-model-for-time-series",
    "title": "Predictive Models and Time Series",
    "section": "Linear Regression Model for Time Series",
    "text": "Linear Regression Model for Time Series\n\nLinear regression model\n\nThe linear regression model is useful for capturing patterns in a time series. In this context, the model takes the form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nWhere \\(i = 1, \\ldots, n_t\\) is the index of the \\(n_t\\) training data.\n\\(\\hat{Y}_i\\) is the prediction of the actual value of the response \\(Y_i\\) at time \\(T_i\\).\n\n\n\nTrend\n\nThe trend of the time series is captured by the value of \\(\\hat{\\beta}_1\\) at\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i\\]\n\nIf \\(\\hat{\\beta}_1\\) is positive, the series has an upward trend.\nIf \\(\\hat{\\beta}_1\\) is negative, the series has a downward trend.\n\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained using the least squares method.\n\n\nModel evaluation\n\nRemember that the errors of the linear regression model (\\(e_i = Y_i - \\hat{Y}_i\\)) must meet two conditions:\n\nOn average, they must be equal to 0.\nThey must have the same dispersion or variability.\nThey must be independent of each other.\n\nIn the context of time series, this means that the model errors \\(e_i\\) must behave like white noise that contains no patterns.\n\n\nExample 3: Amtrak data (cont.)\n\nLet’s fit a linear regression model to the ridership data from Amtrak.\n\n\n\n\n\n\n\n\n\n\n\nCreating a train and a validation data\nIn time series, the order of the data matters because each observation is tied to a specific point in time.\nUnlike typical datasets where observations are independent of one another, time series data follows a sequence where past values influence future ones.\nBecause of this, we cannot randomly split the data using a function like train_test_split(). Doing so might result in a situation where the model learns from future values to predict past ones—which doesn’t make sense and would lead to overly optimistic performance.\n\n\n\nInstead, we want to mimic real-world forecasting: train the model on earlier time periods and test it on later ones.\nTo help us do this properly, we use the code below.\n\n# Define the split point\nsplit_ratio = 0.8  # 80% train, 20% test\nsplit_point = int(len(Amtrak_data) * split_ratio)\n\n# Split the data\nAmtrak_train = Amtrak_data[:split_point]\nAmtrak_validation = Amtrak_data[split_point:]\n\nThis code ensures that the training data always comes before the validation data in time, preserving the temporal order. The proportion of data that goes to training is set using split_ratio.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear regression model\nWe first set the predictor and response.\n\n# Set predictor.\nX_train = Amtrak_train.filter(['t'])\n\n# Set response.\nY_train = Amtrak_train.filter(['Ridership (in 000s)'])\n\nNext, we fit the model using LinearRegression() and fit() from scikit-learn.\n\n# 1. Create linear regression model.\nLRmodelAmtrak = LinearRegression()\n\n# 2. Fit the model to the training data.\nLRmodelAmtrak.fit(X_train, Y_train)\n\n\n\n\n\nLet’s inspect the estimated coefficient for the predictor (time).\n\nprint(LRmodelAmtrak.coef_)\n\n[[-1.2818326]]\n\n\nAnd the intercept.\n\nprint(LRmodelAmtrak.intercept_)\n\n[1810.77686661]\n\n\nThe estimated model then is:\n\\[\\hat{Y}_i = 1810.777 - 1.281 T_i.\\]\n\n\nResidual analysis\n\nWe can validate the model using a residual analysis on the training data. To this end, we first compute the predicted values and residuals of the model.\n\nY_pred = LRmodelAmtrak.predict(X_train)\nresiduals = Y_train - Y_pred\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=X_train, y=residuals)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()\n\n\n\n\n\n\n\nThe model is more flexible than that\n\nIf necessary, the linear regression model can be extended to capture quadratic relationships. For this, the model takes the following form:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 T_i + \\hat{\\beta}_2 T^{2}_i \\]\n\nWhere \\(T^{2}_i\\) is the squared value of the time index.\n\\(\\hat{\\beta}_2\\) is a term that captures possible curvature in the time series.\n\n\n\nIn Python\nTo include a quadratic term, we must augment our predictor matrix with an additional column. The following code shows how to augment X_full by the square of the Amtrak_data['t'] column. This is done using the pandas .concat() function. The resulting matrix is stored in X_quad.\n\nX_quad = pd.concat([X_train, Amtrak_train['t']**2], axis = 1)\n\nNext, we follow the same steps to fit this model.\n\n# 1. Create linear regression model\nQuadmodelAmtrak = LinearRegression()\n\n# 2. Fit linear regression model\nQuadmodelAmtrak.fit(X_quad, Y_train)\n\n\n\n\n\nWe show the estimated coefficients in Python.\n\nprint(\"Intercept = \", QuadmodelAmtrak.intercept_)\nprint(\"Coefficients = \", QuadmodelAmtrak.coef_)\n\nIntercept =  [1866.84019635]\nCoefficients =  [[-4.64563238  0.03397778]]\n\n\n\nThe estimated model thus is\n\\[\\hat{Y}_i = 1866.84 - 4.65 T_i + 0.03 T^2_i.\\]\n\n\nResidual analysis\n\n\n\n\n\nCode\n# Recuerda usar la misma matriz `X_quad`\nY_pred_quad = QuadmodelAmtrak.predict(X_quad)\nresiduals_quad = Y_train - Y_pred_quad\n\nplt.figure(figsize=(8, 6))\nsns.residplot(x=Y_pred_quad, y=residuals_quad, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred_quad)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#plt.figure(figsize=(8, 6))\n#sns.scatterplot(x=Amtrak_train['t'], y=residuals_quad)\n#plt.xlabel(\"Time (t)\")\n#plt.ylabel(\"Residuals\")\n#plt.title(\"Residuals vs. Time\")\n#plt.show()\n\n\n\n\n\n\n\nModel evaluation using validation data\n\nRemember that another way to evaluate the performance of a model is using the \\(\\text{MSE}_v\\) or \\(\\text{RMSE}_v\\) on the validation data. To this end, we need some Python objects.\n\n# Set predictor.\nX_valid = Amtrak_validation.filter(['t'])\n\n# Set response.\nY_valid = Amtrak_validation.filter(['Ridership (in 000s)'])\n\n\n\n\nLet’s compute the \\(\\text{MSE}_v\\) for the linear regression model.\n\nY_val_pred_lin = LRmodelAmtrak.predict(X_valid)\nmse = mean_squared_error(Y_valid, Y_val_pred_lin)  \nprint(round(mse, 2))\n\n62517.47\n\n\nLet’s do the same for the the linear regression model with a quadratic term.\n\nX_quad_valid = pd.concat([X_valid, Amtrak_validation['t']**2], axis = 1)\nY_val_pred_quad = QuadmodelAmtrak.predict(X_quad_valid)\nmse_quad = mean_squared_error(Y_valid, Y_val_pred_quad)  # Mean Squared Error (MSE)\nprint(round(mse_quad, 2))\n\n30271.15\n\n\nWe conclude that the linear model with a quadratic term is better than the linear regression model because the \\(\\text{MSE}_v\\) of the former is smaller than for the latter.\n\n\nMini-Activity (cooperative mode)\n\nConsider the dataset CanadianWorkHours.xlsx in Canvas.\nSplit the data into training and validation\nVisualize the series in Python. The response variable is Working Hours and the predictor is Year.\nUsing Python, answer the question: Which of the following models best fits the series?\n\n\nLinear trend regression model.\nQuadratic trend regression model.\nExponential trend regression model.\n\n\n\nIdentifying Heteroskedasticity\n\nHeteroskedasticity arises when the dispersion of model errors is not constant over time.\nTo see it, let’s go back to the Airline data, which contains the number of passengers of an international airline per month between 1949 and 1960.\n\nAirline_data = pd.read_excel(\"Airline.xlsx\")\n\n\n\n\n\n\nAirline_data.head()\n\n\n\n\n\n\n\n\nT\nNumber of passengers\n\n\n\n\n0\n1\n112\n\n\n1\n2\n118\n\n\n2\n3\n132\n\n\n3\n4\n129\n\n\n4\n5\n121\n\n\n\n\n\n\n\n\n\n\nFor illustrative purposes, we will not split the time series into training and validation datasets.\n\n\nCode\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='T', y='Number of passengers', data = Airline_data)\nplt.xlabel('Time')\nplt.ylabel('Number of passengers')\nplt.title('Number of passengers across time')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s fit a linear regression model.\n\n# Set predictor.\nX_full = Airline_data.filter(['T'])\n\n# Set response.\nY_full = Airline_data.filter(['Number of passengers'])\n\n# 1. Create linear regression\nLRmodelAirline = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirline.fit(X_full, Y_full)\n\n\n\nResidual analysis\n\nHeteroskedasticity: Dispersion of the residuals increases with the predicted value.\n\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values (Y_pred)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nIf we identify heteroskedasticity in the regression model errors, we have several transformation options for our original series.\n\nA common transformations to the time series \\(Y_i\\) is the Natural Logarithm\nIf the original time series contains negative values, it can be lagged by adding the negative of its minimum value.\n\n\n\nIn Python\nThe easiest way to apply the logarithm in Python is to use the log() function from the numpy library\n\nlog_Y_full = np.log( Y_full )\n\nNow, the response to use is in log_Y_full.\n\nThe steps to fit a linear regression model are similar.\n\n# 1. Create linear regression\nLRmodelAirlineTransformed = LinearRegression()\n\n# 2. Fit the model\nLRmodelAirlineTransformed.fit(X_full, log_Y_full)\n\n\n\nResidual analysis\n\n\n\nWith transformation\n\n\nCode\nY_pred_log = LRmodelAirlineTransformed.predict(X_full)\nresiduals_log = log_Y_full - Y_pred_log\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred_log, y=residuals_log, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWithout transformation\n\n\nCode\nY_pred = LRmodelAirline.predict(X_full)\nresiduals = Y_full - Y_pred\nplt.figure(figsize=(6, 4))\nsns.residplot(x=Y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Fitted Values\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do I do if the transformation doesn’t work?\n\n\nIf the log transformation doesn’t significantly reduce heteroskedasticity, there are models for modeling variance called GARCH.\nYou can find literature on these models and their software implementations in a time series textbook such as Time Series Analysis with Applications in R by Cryer and Chan.\n\n\n\nSeasonality\nSeasonality refers to repetitive or cyclical behavior that occurs with a constant frequency.\n\n\n\nExamples:\n\nDemand for winter clothing\nDemand for tourist travel\nAmount of rainfall throughout the year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing seasonality\n\n\n\nThe linear regression model can be extended to capture seasonal patterns in the time series.\nTo do this, an additional categorical predictor is created that indicates the season to which each data item belongs.\nBehind the scenes, the additional categorical predictor is transformed into several auxiliary numerical predictors.\n\n\n\n\n\n\n\nAnalyzing seasonal series in Python\nConsider the data in Amtrak_train with the additional predictor of Season to model seasonality.\n\n\n\n\n\n\n\n\n\nMonth\nt\nRidership (in 000s)\nSeason\n\n\n\n\n0\n1991-01-01\n1\n1708.917\nJan\n\n\n1\n1991-02-01\n2\n1620.586\nFeb\n\n\n2\n1991-03-04\n3\n1972.715\nMar\n\n\n3\n1991-04-04\n4\n1811.665\nApr\n\n\n4\n1991-05-05\n5\n1974.964\nMay\n\n\n\n\n\n\n\n\n\n\nTo fit a linear regression model with a categorical variable like Season, we must transform the text categories into numbers. To do this, we use dummy variables constructed using the following commands.\n\ndummy_data = pd.get_dummies(Amtrak_train['Season'], dtype = 'int')\ndummy_data.head(4)\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\nSimply put, the matrix above contains one column for each month. Each column indicates the observations that belong to the month of the column. For example, the column Apr has the values 0 and 1. The value 1 indicates that the corresponding observation belongs to the month of April. A 0 indicates otherwise.\n\n\n\n\n\n\n\n\n\nApr\nAug\nDec\nFeb\nJan\nJul\nJun\nMar\nMay\nNov\nOct\nSep\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nUnfortunately, we cannot use the matrix as is in the linear regression model. This is due to multicollinearity issues. Technically, this happens because if you add all the columns, the resulting column is a column of 1s, which is already used by the intercept. Therefore, you cannot fit a model with the intercept and all the columns of the dummy variables.\n\nTo solve this problem, we arbitrarily remove a column from the matrix above. For example, let’s remove Dec.\n\ndummy_data = dummy_data.drop([\"Dec\"], axis = 1)\n\n\n\n\nNow, let’s build the complete matrix of predictors, including the column for time, time squared, and the dummy variables.\n\nX_quad_season = pd.concat([Amtrak_train['t'], Amtrak_train['t']**2, dummy_data], \n                          axis = 1)\n\n\nNext, we fit the model with all the terms in the matrix above.\n\n# 0. Asegurarnos que tenemos la respuesta del problema en `Y_full`.\nY_train = Amtrak_train['Ridership (in 000s)']\n\n# 1. Crear modelo de regresión lineal\nSeasonmodelAmtrak = LinearRegression()\n\n# 2. Ajustar el modelo usando los datos de entrenamiento\nSeasonmodelAmtrak.fit(X_quad_season, Y_train)\n\n\n\nEstimated model coefficients\n\n\nprint(\"Intercept = \", SeasonmodelAmtrak.intercept_)\nprint(\"Coefficients = \", SeasonmodelAmtrak.coef_)\n\nIntercept =  1924.3205889003025\nCoefficients =  [-6.12171259e+00  4.96085283e-02  1.22309969e+01  1.77720172e+02\n -2.83088245e+02 -2.25659158e+02  1.15685204e+02 -7.31735621e+00\n  1.59876604e+01  5.17037414e+01 -4.77796001e+01 -3.19949172e+01\n -1.45366826e+02]\n\n\n\n\nResidual analysis\n\n\nCode\nY_pred = SeasonmodelAmtrak.predict(X_quad_season)\nresiduals = Y_train - Y_pred\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=Amtrak_data['t'], y=residuals)\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs. Time\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPredictions on the validation dataset\nPrepare the validation data using dummy variables.\n\nY_valid = Amtrak_validation['Ridership (in 000s)']\n\ndummy_valid = pd.get_dummies(Amtrak_validation['Season'], dtype = 'int')\ndummy_valid = dummy_valid.drop([\"Dec\"], axis = 1)\n\nX_qs_valid = pd.concat([Amtrak_validation['t'], Amtrak_validation['t']**2, \n                        dummy_valid], axis = 1)\n\n\nNow, we compute the validation \\(\\text{MSE}_v\\).\n\nY_pred_valid = SeasonmodelAmtrak.predict(X_qs_valid)\n\nmse_season = mean_squared_error(Y_valid, Y_pred_valid) \nprint(round(mse_season, 2))\n\n4890.5\n\n\n\n\nDisadvantages of linear regression models\n\nDespite their simplicity and versatility, linear regression models are not the best for describing a time series.\nThis is because they do not assume a dependency between consecutive values in the time series. That is, they do not use the fact that, for example, \\(Y_1\\) can help us predict \\(Y_2\\), and \\(Y_2\\) can help us predict \\(Y_3\\), etc.\nModels that help us use past observations to predict future values of the response variable \\(Y\\) are autoregressive models."
  },
  {
    "objectID": "Module3/PredictiveModels.html#return-to-main-page",
    "href": "Module3/PredictiveModels.html#return-to-main-page",
    "title": "Predictive Models and Time Series",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module4/Clustering.html",
    "href": "Module4/Clustering.html",
    "title": "Clustering Methods",
    "section": "",
    "text": "Unsupervised Learning\nClustering Methods\nK-Means Method\nHierarchical Clustering"
  },
  {
    "objectID": "Module4/Clustering.html#unsupervised-learning",
    "href": "Module4/Clustering.html#unsupervised-learning",
    "title": "Clustering Methods",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nLoad the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, sklearn, and scipy libraries in Python.\n\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nUnsupervised learning\nGoal: organize or group data to gain insights. It answers questions like these\n\nIs there an informative way to visualize the data?\nCan we discover subgroups among variables or observations?\n\n. . .\nUnsupervised learning is more challenging than supervised learning because it is subjective and there is no simple objective for the analysis, such as predicting a response.\n. . .\nIt is also known as exploratory data analysis.\n\n\nExamples of Unsupervised Learning\n\n\nMarketing. Identify a segment of customers with a high tendency to purchase a specific product.\nRetail. Group customers based on their preferences, style, clothing choices, and store preferences.\nMedical Science. Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.\nSociology. Classify people based on their demographics, lifestyle, socioeconomic status, etc.\n\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical.\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/Clustering.html#clustering-methods",
    "href": "Module4/Clustering.html#clustering-methods",
    "title": "Clustering Methods",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\nClustering methods\nThey group data in different ways to discover groups with common traits.\n\n\n\n\n\n\n\nClustering methods\n\nTwo classic clustering methods are:\n\nK-Means Method. We seek to divide the observations into K groups.\nHierarchical Clustering. We divide the n observations into 1 group, 2 groups, 3 groups, …, up to n groups. We visualize the divisions using a graph called a dendrogram.\n\n\n\nExample 1\nThe “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:\n\n\n\n\nBill length in millimeters.\nBill depth in millimeters.\nFlipper length in millimeters.\nBody mass in grams.\n\n\n\n\n\n\n\n\n\n\n\n\nData\n\npenguins_data = pd.read_excel(\"penguins.xlsx\")\npenguins_data.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\nData visualization\nCan we group penguins based on their characteristics?\n\n\nCode\nplt.figure(figsize=(8, 5)) # Set figure size.\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\") # Define type of plot.\nplt.show() # Display the plot."
  },
  {
    "objectID": "Module4/Clustering.html#k-means-method",
    "href": "Module4/Clustering.html#k-means-method",
    "title": "Clustering Methods",
    "section": "K-Means Method",
    "text": "K-Means Method\n\nThe K-Means method\n\nGoal: Find K groups of observations such that each observation is in a different group.\n\n\n\n\n\nFor this, the method requires two elements:\n\n\nA measure of “closeness” between observations.\nAn algorithm that groups observations that are close to each other.\n\n\n. . .\nGood clustering is one in which observations within a group are close together and observations in different groups are far apart.\n\n\nHow do we measure the distance between observations?\nFor quantitative predictors, we use the Euclidean distance.\nFor example, if we have two predictors \\(X_1\\) and \\(X_2\\) with observations given in the table:\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\n\n\n\n\nEuclidean distance\n\n\n\n\n\n\n\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }\\]\n\n\n\n\nWe can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, with 3 predictors we have\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\n\n\n\n1\n\\(X_{1,1}\\)\n\\(X_{1,2}\\)\n\\(X_{1,3}\\)\n\n\n2\n\\(X_{2,1}\\)\n\\(X_{2,2}\\)\n\\(X_{2,3}\\)\n\n\n\n\nWhere the Euclidean distance is\n\\[d = \\sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }\\]\n\n\nProblem with Euclidean distance\n\n\nThe Euclidean distance depends on the units of measurement of the predictors!\nPredictors with certain units have greater importance in calculating the distance.\nThis is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.\nThe solution is to standardize the units of the predictors.\n\n\n\nK-Means Algorithm\n\n\n\n\nChoose a value for K, the number of groups.\n\nRandomly assign observations to one of the K groups.\nFind the centroids (average points) of each group.\nReassign observations to the group with the closest centroid.\nRepeat steps 3 and 4 until there are no more changes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (cont.)\nLet’s apply the algorithm to the predictors bill_depth_mm and bill_length_mm of the penguins dataset.\n\n\nCode\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm'])\nX_penguins.head()\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\n\n\n\n\n0\n18.7\n39.1\n\n\n1\n17.4\n39.5\n\n\n2\n18.0\n40.3\n\n\n3\n19.3\n36.7\n\n\n4\n20.6\n39.3\n\n\n\n\n\n\n\n\n\nStandarization\nSince the K-means algorithm works with Euclidean distance, we must standardize the predictors before we start. In this way, all of them will be equally informative in the process.\n\n\nCode\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n\n\n\n\n\nIn Python, we use the KMeans() function of sklearn to apply K-means clustering. KMeans() tells Python we want to train a K-means clustering algorithm and .fit_predict() actually trains it using the data.\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\nThe argument n_clusters sets the desired number of clusters and random_state allows us to reproduce the analysis.\n\n\n\nThe clusters created are contained in the clusters object.\n\n\nCode\nclusters\n\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n       0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2], dtype=int32)\n\n\n\n\n\nTo visualize the clusters, we augment the original dataset X_penguins (without standarization) with the clusters object. usign the code below.\n\nclustered_penguins = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\nclustered_penguins.head()\n\n\n\n\n\n\n\n\nbill_depth_mm\nbill_length_mm\nCluster\n\n\n\n\n0\n18.7\n39.1\n1\n\n\n1\n17.4\n39.5\n1\n\n\n2\n18.0\n40.3\n1\n\n\n3\n19.3\n36.7\n1\n\n\n4\n20.6\n39.3\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data = clustered_penguins, x = 'bill_length_mm', y = 'bill_depth_mm', \n                hue = 'Cluster', palette = 'Set1')\nplt.title('K-means Clustering of Penguins')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe truth: 3 groups of penguins\n\n\nCode\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=penguins_data, x=\"bill_depth_mm\", y=\"bill_length_mm\",\n                hue=\"species\", palette = 'Set1') # Define type of plot.\nplt.show() # Display the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s try using more predictors\n\n\nX_penguins = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', \n                          'flipper_length_mm', 'body_mass_g'])\n\n## Standardize\nscaler = StandardScaler()\nXs_penguins = scaler.fit_transform(X_penguins)\n\n# Fit KMeans with 3 clusters\nkmeans = KMeans(n_clusters = 3, random_state = 301655)\nclusters = kmeans.fit_predict(Xs_penguins)\n\n# Save new clusters into the original data\nclustered_X = (X_penguins\n              .assign(Cluster = clusters)\n              )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese are the three species\n\n\n\nAdelie\n\n\nGentoo\n\n\nChinstrap\n\n\n\n\n\n\nDetermining the number of clusters\n\nA simple way to determine the number of clusters is recording the quality of clustering for different numbers of clusters.\nIn sklearn, we can record the inertia of a partition into clusters. Technically, the inertia is the sum of squared distances of observations to their closest cluster center.\nThe lower the intertia the better because this means that all observations are close to their cluster centers overall.\n\n\n\n\nTo record the intertias for different numbers of clusters, we use the code below.\n\ninertias = []\n\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(Xs_penguins)\n    inertias.append(kmeans.inertia_)\n\n\n\n\n\n\n\nNext, we plot the intertias and look for the elbow in the plot.\nThe elbow represents a number of clusters for which there is no significant improvement in the quality of the clustering.\nIn this case, the number of clusters recommended by this elbow method is 3.\n\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(range(1,11), inertias, marker='o')\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments\n\nSelecting the number of clusters K is more of an art than a science. You’d better get K right, or you’ll be detecting patterns where none really exist.\nWe need to standardize all predictors.\nThe performance of K-means clustering is affected by the presence of outliers.\nThe algorithm’s solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported."
  },
  {
    "objectID": "Module4/Clustering.html#hierarchical-clustering",
    "href": "Module4/Clustering.html#hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nHierarchical clustering\n\n\n\n\n\n\nStart with each observation standing alone in its own group.\nThen, gradually merge the groups that are close together.\nContinue this process until all the observations are in one large group.\nFinally, step back and see which grouping works best.\n\n\n\n\n\n\n\n\n\n\nEssential elements\n\n\n\nDistance between two observations.\n\nWe use Euclidean distance.\nWe must standardize the predictors!\n\nDistance between two groups.\n\n\n\n\nDistance between two groups\n\n\n\n\nThe distance between two groups of observations is called linkage.\nThere are several types of linking. The most commonly used are:\n\nComplete linkage\nAverage linkage\n\n\n\n\n\n\n\n\nComplete linkage\nThe distance between groups is measured using the largest distance between observations.\n\n\n\n\n\n\n\nAverage linkage\nThe distance between groups is the average of all the distances between observations.\n\n\n\n\n\n\n\nHierarchical clustering algorithm\n\nThe steps of the algorithm are as follows:\n\n\nAssign each observation to a cluster.\nMeasure the linkage between all clusters.\nMerge the two most similar clusters.\nThen, merge the next two most similar clusters.\nContinue until all clusters have been merged.\n\n\n\n\nExample 2\n\nLet’s consider a dataset called “Cereals.xlsx.” The data includes nutritional information for 77 cereals, among other data.\n\ncereal_data = pd.read_excel(\"cereals.xlsx\")\n\n\n\n\nHere, we will restrict to 7 numeric predictors\n\nX_cereal = cereal_data.filter(['calories', 'protein', 'fat', 'sodium', 'fiber',\n                              'carbo', 'sugars', 'potass', 'vitamins'])\nX_cereal.head()\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\n\n\n\n\n0\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n\n\n1\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n\n\n2\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n\n\n3\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n\n\n4\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n\n\n\n\n\n\n\n\n\nDo not forget to standardize\n\nSince the hierarchical clustering algorithm also works with distances, we must standardize the predictors to have an accurate analysis.\n\nscaler = StandardScaler()\nXs_cereal = scaler.fit_transform(X_cereal)\n\n\n\n\n\nUnfortunately, the Agglomerative() function in sklearn is not as user friendly compared to other available functions in Python. In particular, the scipy library has a function called linkage() for hierarchical clustering that works as follows.\n\nClust_Cereal = linkage(Xs_cereal, method = 'complete')\n\nThe argument method sets the type of linkage to be used.\n\n\nResults: Dendrogram\n\n\n\n\n\n\nA dendrogram is a tree diagram that summarizes and visualizes the clustering process.\nObservations are on the horizontal axis and at the bottom of the diagram.\nThe vertical axis shows the distance between groups.\nIt is read from top to bottom.\n\n\n\n\n\n\n\n\n\nWhat to do with a dendrogram?\n\n\n\n\nWe draw a horizontal line at a specific height to define the groups.\nThis line defines three groups.\n\n\n\n\n\n\n\n\n\n\n\n\nThis line defines 5 groups.\n\n\n\n\n\n\n\nDendrogram in Python\nTo produce a nice dendrogram in Python, we use the function dendrogram from scipy.\n\n\nCode\nplt.figure(figsize=(8, 4))\ndendrogram(Clust_Cereal, color_threshold=None)\nplt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nComments\n\n\nRemember to standardize the predictors!\nIt’s not easy to choose the correct number of clusters using the dendrogram.\nThe results depend on the linkage measure used.\n\nComplete linkage results in narrower clusters.\nAverage linkage strikes a balance between narrow and thinner clusters.\n\nHierarchical clustering is useful for detecting outliers.\n\n\n\n\n\n\nWith these methods, there is no single correct answer; any solution that exposes some interesting aspect of the data should be considered.\n\nJames et al. (2017)"
  },
  {
    "objectID": "Module4/Clustering.html#return-to-main-page",
    "href": "Module4/Clustering.html#return-to-main-page",
    "title": "Clustering Methods",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module4/PCA.html",
    "href": "Module4/PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Introduction\nDispersion in one or more dimensions\nPrincipal component analysis"
  },
  {
    "objectID": "Module4/PCA.html#introduction",
    "href": "Module4/PCA.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nHere, we use specific functions from the pandas, matplotlib, seaborn, and sklearn libraries in Python.\n\nTypes of learning\n\nIn data science, there are two main types of learning:\n\nSupervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.\nUnsupervised learning. In which we have only multiple predictors. The goal is to discover patterns in your data.\n\n\n\nUnsupervised learning methods\n\n\nClustering Methods aim to find subgroups with similar data in the database.\nPrincipal Component Analysis seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n\nHere we will use these methods on predictors \\(X_1, X_2, \\ldots, X_p,\\) which are numerical."
  },
  {
    "objectID": "Module4/PCA.html#dispersion-in-one-or-more-dimensions",
    "href": "Module4/PCA.html#dispersion-in-one-or-more-dimensions",
    "title": "Principal Component Analysis",
    "section": "Dispersion in one or more dimensions",
    "text": "Dispersion in one or more dimensions\n\nDispersion in one dimension\n\nThe concept of principal components requires an understanding of the dispersion or variability of the data.\nSuppose we have data for a single predictor.\n\n\n\n\n\n\n\n\nDispersion in two dimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing dispersion\nIn some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n\n\n\n\n\n\n\n\n\n\nA single predictor \\(X_2\\) captures much of the spread in the data.\n\n\n\n\n\n\n\n\n\n\nLet’s see another example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see another example\n\n\n\n\n\n\n\n\n\n\nA single predictor captures much of the dispersion in the data. In this case, the new predictor has the form \\(Z_1 = a X_1 + b X_2 + c.\\)\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, we can use two alternative dimensions to capture the dispersion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA new coordinate system\n\n\n\n\n\nThe new coordinate axis is given by two new predictors, \\(Z_1\\) and \\(Z_2\\). Both are given by linear equations of the new predictors.\nThe first axis, \\(Z_1\\), captures a large portion of the dispersion, while \\(Z_2\\) captures a small portion from another angle.\nThe new axes, \\(Z_1\\) and \\(Z_2\\), are called principal components."
  },
  {
    "objectID": "Module4/PCA.html#principal-component-analysis",
    "href": "Module4/PCA.html#principal-component-analysis",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nDimension Reduction\n\nPrincipal Components Analysis (PCA) helps us reduce the dimension of the data.\n\n\nIt creates a new coordinate axis in two (or more) dimensions.\nTechnically, it creates new predictors by combining highly correlated predictors. The new predictors are uncorrelated.\n\n\n\n\nSetup\n\nStep 1. We start with a database with \\(n\\) observations and \\(p\\) predictors.\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n15\n14\n5\n\n\n2\n1\n6\n\n\n10\n3\n17\n\n\n8\n18\n9\n\n\n12\n16\n11\n\n\n\n\n\n\nStep 2. We standardize each predictor individually.\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}\\]\n\n\n\n\nPredictor 1\nPredictor 2\nPredictor 3\n\n\n\n\n\n1.15\n0.46\n-0.96\n\n\n\n-1.52\n-1.20\n-0.75\n\n\n\n0.12\n-0.95\n1.55\n\n\n\n-0.29\n0.97\n-0.13\n\n\n\n0.53\n0.72\n0.29\n\n\nSum\n0\n0\n0\n\n\nVariance\n1\n1\n1\n\n\n\n\n\n\n\nStep 3. We assume that the standardized database is an \\(n\\times p\\) matrix \\(\\mathbf{X}\\).\n\\[\\mathbf{X} = \\begin{pmatrix}\n1.15    &   0.46    &   -0.96   \\\\\n-1.52   &   -1.20   &   -0.75   \\\\\n0.12    &   -0.95   &   1.55    \\\\\n-0.29   &   0.97    &   -0.13   \\\\\n0.53    &   0.72    &   0.29    \\\\\n\\end{pmatrix}\\]\n\n\nAlgorithm\n\nThe PCA algorithm has its origins in linear algebra.\n\nIts basic idea is:\n\nCreate a matrix \\(\\mathbf{C}\\) with the correlations between the predictors of the matrix \\(\\mathbf{X}\\).\nSplit the matrix \\(\\mathbf{C}\\) into three parts, which give us the new coordinate axis and the importance of each axis.\n\n\n\nCorrelation matrix\n\nContinuing with our example, the correlation matrix contains the correlations between two columns of \\(\\mathbf{X}\\).\n\n\n\nPartitioning the correlation matrix\n\nThe \\(\\mathbf{C}\\) matrix is partitioned using the eigenvalue and eigenvector decomposition method.\n\n\n\n\n\n\n\nThe columns of \\(\\mathbf{B}\\) define the axes of the new coordinate system. These axes are called principal components.\nThe diagonal values in \\(\\mathbf{A}\\) define the individual importance of each principal component (axis).\n\n\n\nProportion of the dispersion explained by the component\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion of the dispersion in the data that is captured by the first component is \\(\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\mathbf{A} = \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.33    \\\\\n\\end{pmatrix}\\]\n\n\n\n\nThe proportion captured by the second component is \\(\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36\\).\nThe proportion captured by the third component is \\(\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11.\\)\n\n\n\n\n\n\nComments\n\nPrincipal components can be used to approximate a matrix.\nFor example, we can approximate the matrix \\(\\mathbf{C}\\) by setting the third component equal to zero.\n\n\\[\\begin{pmatrix}\n-0.68   &   0.35    &   0.00    \\\\\n-0.72   &   -0.13   &   0.00    \\\\\n0.16    &   0.93    &   0.00\\\\\n\\end{pmatrix} \\begin{pmatrix}\n1.60     &  0.00    &   0.00    \\\\\n0.00     &  1.07     &  0.00    \\\\\n0.00     &  0.00     &  0.00    \\\\\n\\end{pmatrix} \\begin{pmatrix}\n-0.68   &   -0.72   &   0.16    \\\\\n0.35    &   -0.13   &   0.93    \\\\\n0.00    &   0.00    &   0.00    \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0.86    &   0.73    &   0.18    \\\\\n0.73    &   0.85    &   -0.30   \\\\\n0.18    &   -0.30   &   0.96    \\\\\n\\end{pmatrix}\\]\n\n\n\\[\\approx \\begin{pmatrix}\n1.00    &   0.58    &   0.11    \\\\\n0.58    &   1.00    &   -0.23   \\\\\n0.11    &   -0.23   &   1.00    \\\\\n\\end{pmatrix} = \\mathbf{C}\\]\n\n\n\n\n\n\n\n\n\nApproximations are useful for storing large matrices.\nThis is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\nThis is the idea behind image compression.\n\n\n\n\n\n\n\n\n\nExample 1\n\nConsider a database of the 100 most popular songs on TikTok. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements.\nEnergy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\nThe overall volume of a track in decibels (dB). Loudness values are averaged across the entire track.\n\n\n\n\n\nOther predictors are:\n\nSpeech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\nA confidence measure from 0 to 1 about whether the track is acoustic.\nDetects the presence of an audience in the recording.\nA measure from 0 to 1 that describes the musical positivity a track conveys.\n\n\n\nThe data\n\ntiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\ntiktok_data.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nalbum\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\nliveness\nvalence\ntempo\n\n\n\n\n0\nSay So\nDoja Cat\nHot Pink\n0.787\n0.673\n-4.583\n0.1590\n0.26400\n0.0904\n0.779\n110.962\n\n\n1\nBlinding Lights\nThe Weeknd\nAfter Hours\n0.514\n0.730\n-5.934\n0.0598\n0.00146\n0.0897\n0.334\n171.005\n\n\n2\nSupalonely (feat. Gus Dapperton)\nBENEE\nHey u x\n0.862\n0.631\n-4.746\n0.0515\n0.29100\n0.1230\n0.841\n128.978\n\n\n3\nSavage\nMegan Thee Stallion\nSuga\n0.843\n0.741\n-5.609\n0.3340\n0.02520\n0.0960\n0.680\n168.983\n\n\n4\nMoral of the Story\nAshe\nMoral of the Story\n0.572\n0.406\n-8.624\n0.0427\n0.58700\n0.1020\n0.265\n119.812\n\n\n\n\n\n\n\n\n\nStandardize the data\n\nRemember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n\n# Select the predictors\nfeatures = ['danceability', 'energy', 'loudness', 'speechiness',\n            'acousticness', 'liveness', 'valence', 'tempo']\nX_tiktok = tiktok_data.filter(features)  \n\n# Standardize the data\nscaler = StandardScaler()\nXs_tiktok = scaler.fit_transform(X_tiktok)\n\n\n\nPCA in Python\n\nWe tell Python that we want to apply PCA using the function PCA() from sklearn. Next, we run the algorithm using .fit_transform()\n\npca = PCA()\nPCA_tiktok = pca.fit_transform(Xs_tiktok)\n\n\n\n\n\n\n\n\nThe Screen or Summary Plot tells you the variability captured by each component. This variability is given by the Eigenvalue. From 1 to 8 components.\nThe first component covers most of the data dispersion.\nThis graph is used to define the total number of components to use.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to generate a scree plot is below.\n\nexplained_var = pca.explained_variance_ratio_\n\nplt.figure(figsize=(5, 5))\nplt.plot(range(1, len(explained_var) + 1), explained_var, \n         marker='o', linestyle='-')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.xticks(range(1, len(explained_var) + 1))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\nBiplot\n\n\n\n\nDisplays the graphical observations on the new coordinate axis given by the first two components.\nHelps visualize data for three or more predictors using a two-dimensional scatter plot.\nA red line indicates the growth direction of the labeled variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to generate the biplot is lenghty but it can be broken into three steps.\nStep 1. Create a DataFrame with the PCA results\n\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\n\n\n\n\n0\n1.103065\n0.558086\n-0.800688\n0.446496\n0.605944\n-0.044089\n0.287325\n-0.413604\n\n\n1\n0.805080\n-0.766973\n1.580513\n-2.215856\n0.359655\n0.708123\n-0.882761\n0.113058\n\n\n2\n1.330433\n0.728161\n-0.288982\n0.376298\n0.786185\n-1.134308\n0.178388\n-0.242497\n\n\n3\n1.496277\n2.095014\n1.351398\n-0.621691\n0.390949\n0.494101\n0.024648\n-0.080720\n\n\n4\n-1.973362\n-0.966108\n-0.302071\n-1.266269\n0.414639\n-0.335677\n-0.076711\n-0.140126\n\n\n\n\n\n\n\n\n\n\nStep 2. Create biplot of first two principal components\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3. Add more information to the biplot.\n\n\nCode\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWith some extra lines of code, we label the points in the plot.\n\n\nCode\npca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\npca_df = (pca_df\n          .assign(songs = tiktok_data['track_name'])\n          )\n\nplt.figure(figsize=(10, 5.5))\nsns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Biplot')\nplt.grid(True)\nplt.tight_layout()\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n\n# Add labels for each song\nfor i in range(pca_df.shape[0]):\n    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n             pca_df['songs'][i], fontsize=8, alpha=0.7)\n\n\n# Add variable vectors\nloadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\nfor i, feature in enumerate(features):\n    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n              color='red', alpha=0.5, head_width=0.05)\n    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n\nplt.show()"
  },
  {
    "objectID": "Module4/PCA.html#return-to-main-page",
    "href": "Module4/PCA.html#return-to-main-page",
    "title": "Principal Component Analysis",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module3/Nonparametric.html",
    "href": "Module3/Nonparametric.html",
    "title": "Nonparametric Methods",
    "section": "",
    "text": "Introduction\nLogistic regression\nEstimating a logistic regression model\nClassification performance"
  },
  {
    "objectID": "Module3/Nonparametric.html#return-to-main-page",
    "href": "Module3/Nonparametric.html#return-to-main-page",
    "title": "Nonparametric Methods",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/EnsembleMethods.html",
    "href": "Module2/EnsembleMethods.html",
    "title": "Ensamble Methods",
    "section": "",
    "text": "Introduction to Ensemble Methods\nBagging\nRandom Forests"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#ensamble-methods",
    "href": "Module2/EnsembleMethods.html#ensamble-methods",
    "title": "Ensamble Methods",
    "section": "Ensamble Methods",
    "text": "Ensamble Methods\n\nLoad the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nHere, we use specific functions from the pandas, matplotlib, seaborn and sklearn libraries in Python.\n\n\nDecision trees\n\n\n\n\n\nSimple and useful for interpretations.\nCan handle continuous and categorical predictors and responses. So, they can be applied to both classification and regression problems.\nComputationally efficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations of decision trees\n\n\nIn general, decision trees do not work well for classification and regression problems.\nHowever, decision trees can be combined to build effective algorithms for these problems.\n\n\n\nEnsamble methods\n\nEnsemble methods refer to frameworks to combine decision trees.\nHere, we will cover a popular ensamble method:\n\nBagging. Ensemble many deep trees.\n\nQuintessential method: Random Forests."
  },
  {
    "objectID": "Module2/EnsembleMethods.html#bagging",
    "href": "Module2/EnsembleMethods.html#bagging",
    "title": "Ensamble Methods",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap samples\n\nBootstrap samples are samples obtained with replacement from the original sample. So, an observation can occur more than one in a bootstrap sample.\nBootstrap samples are the building block of the bootstrap method, which is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. \n\n\n\n\n\n\n\n\nBagging\nGiven a training dataset, bagging averages the predictions from decision trees over a collection of bootstrap samples.\n\n\n\n\n\n\n\nPredictions\n\nLet \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_p)\\) be a vector of new predictor values. For classification problems with 2 classes:\n\nEach classification tree outputs the probability for class 1 and 2 depending on the region \\(\\mathbf{x}\\) falls in.\nFor the b-th tree, we denote the probabilities as \\(\\hat{p}^{b}_0(\\boldsymbol{x})\\) and \\(\\hat{p}^{b}_1(\\boldsymbol{x})\\) for class 0 and 1, respectively.\n\n\n\n\n\n\nUsing the probabilities, a standard tree follows the Bayes Classifier to output the actual class:\n\n\\[\\hat{T}_{b}(\\boldsymbol{x}) =\n    \\begin{cases}\n      1, & \\hat{p}^{b}_1(\\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\hat{p}^{b}_1(\\boldsymbol{x}) \\leq 0.5\n    \\end{cases}\\]\n\n\n\n\nCompute the proportion of trees that output a 0 as\n\n\\[p_{bag, 0} = \\frac{1}{B} \\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 0).\\]\n\nCompute the proportion of trees that output a 1 as:\n\n\\[p_{bag, 1} = \\frac{1}{B}\\sum_{b=1}^{B} I(\\hat{T}_{b}(\\boldsymbol{x}) = 1).\\]\n\n\n\n\n\nClassify according to a majority vote between \\(p_{bag, 0}\\) and \\(p_{bag, 1}\\).\n\n\n\nImplementation\n\n\nHow many trees? No risk of overfitting, so use plenty.\nNo pruning necessary to build the trees. However, one can still decide to apply some pruning or early stopping mechanism.\nThe size of bootstrap samples is the same as the size of the training dataset, but we can use a different size.\n\n\n\nExample 1\n\nThe data “AdultReduced.xlsx” comes from the UCI Machine Learning Repository and is derived from US census records. In this data, the goal is to predict whether a person’s income was high (defined in 1994 as more than $50,000) or low.\nPredictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc. The data contains 7,508 records.\n\n\nRead the dataset\n\n# Load the data\nAdult_data = pd.read_excel('AdultReduced.xlsx')\n\n# Preview the data.\nAdult_data.head(3)\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation.num\nmarital.status\noccupation\nrelationship\nrace\nsex\ncapital.gain\ncapital.loss\nhours.per.week\nnative.country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\nsmall\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\nsmall\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\nsmall\n\n\n\n\n\n\n\n\n\nSelected predictors.\n\nage: Age of the individual.\nsex: Sex of the individual (male or female).\nrace: Race of the individual (W, B, Amer-Indian, Amer-Pacific, or Other).\neducation.num: number of years of education.\nhours.per.week: Number of work hours per week.\n\n\n# Choose the predictors.\nAdult_Subset = Adult_data.filter(['age', 'education.num', 'capital.gain', \n                                  'hours.per.week', 'income'])\n\n\n\nSet the target class\nLet’s set the target class and the reference class using the get_dummies() function.\n\n# Select answer\nY = Adult_Subset['income']\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y, dtype = 'int')\n# Show.\nY_dummies.head(4)\n\n\n\n\n\n\n\n\nlarge\nsmall\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n3\n0\n1\n\n\n\n\n\n\n\n\n\n\n\nHere we’ll use the large target class. So, let’s use the corresponding column as our response variable.\n\n# Choose target category.\nY_target = Y_dummies['large']\n\nY_target.head()\n\n0    0\n1    0\n2    0\n3    0\n4    1\nName: large, dtype: int64\n\n\n\n\nTraining and validation datasets\n\nTo evaluate a model’s performance on unobserved data, we split the current dataset into training and validation datasets. To do this, we use train_test_split() from scikit-learn.\n\n# Create predictor matrix\nX_full = Adult_Subset.drop(columns = ['income'])\n\n# Split into training and validation\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target, \n                                                      test_size = 0.2)\n\nWe use 80% of the dataset for training and the rest for validation.\n\n\nBagging in Python\n\nWe define a bagging algorithm for classification using the BaggingClassifier function from scikit-learn.\nThe n_estimators argument is the number of decision trees to generate in bagging. Ideally, it should be high, around 500.\n\n# Set the bagging algorithm.\nBaggingalgorithm = BaggingClassifier(n_estimators = 500, \n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nBaggingalgorithm.fit(X_train, Y_train)\n\nrandom_state allows us to obtain the same bagging algorithm in different runs of the algorithm.\n\n\nPredictions\nPredict the classes using bagging.\n\npredicted_class = Baggingalgorithm.predict(X_valid)\n\npredicted_class\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\n\nConfusion matrix\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, predicted_class)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()\n\n\n\n\n\n\n\n\n\n\nAccuracy\n\nThe accuracy of the bagging classifier is 78%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, predicted_class)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.81\n\n\n\n\nA single deep tree\nTo compare the bagging, let’s use a single tree with cost complexity pruning.\n\n\nCode\n# We tell Python that we want a classification tree\nclf_simple = DecisionTreeClassifier(ccp_alpha=0.0,\n                                    random_state=507134)\n\n# We train the classification tree using the training data.\nclf_simple.fit(X_train, Y_train)\n\n\nLet’s compute the accuracy of the pruned tree.\n\nsingle_tree_Y_pred = clf_simple.predict(X_valid)\naccuracy = accuracy_score(Y_valid, single_tree_Y_pred)\nprint( round(accuracy, 2) )\n\n0.8\n\n\n\n\nAdvantages\n\n\nBagging will have lower prediction errors than a single classification tree.\nThe fact that, for each tree, not all of the original observations were used, can be exploited to produce an estimate of the accuracy for classification.\n\n\n\nLimitations\n\n\nLoss of interpretability: the final bagged classifier is not a tree, and so we forfeit the clear interpretative ability of a classification tree.\nComputational complexity: we are essentially multiplying the work of growing (and possibly pruning) a single tree by B.\nFundamental issue: bagging a good model can improve predictive accuracy, but bagging a bad one can seriously degrade predictive accuracy.\n\n\n\nOther issues\n\n\nSuppose a variable is very important and decisive.\n\nIt will probably appear near the top of a large number of trees.\nAnd these trees will tend to vote the same way.\nIn some sense, then, many of the trees are “correlated”.\nThis will degrade the performance of bagging.\n\n\n\n\n\n\nBagging is unable to capture simple decision boundaries"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#random-forest",
    "href": "Module2/EnsembleMethods.html#random-forest",
    "title": "Ensamble Methods",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom Forest\n\nExactly as bagging, but…\n\nWhen splitting the nodes using the CART algorithm, instead of going through all possible splits for all possible variables, we go through all possible splits on a random sample of a small number of variables \\(m\\), where \\(m &lt; p\\).\n\nRandom forests can reduce variability further.\n\n\nWhy does it work?\n\n\nNot so dominant predictors will get a chance to appear by themselves and show “their stuff”.\nThis adds more diversity to the trees.\nThe fact that the trees in the forest are not (strongly) correlated means lower variability in the predictions and so, a bettter performance overall.\n\n\n\nTuning parameter\n\nHow do we set \\(m\\)?\n\nFor classification, use \\(m = \\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is 1.\n\nIn practice, sometimes the best values for these parameters will depend on the problem. So, we can treat \\(m\\) as a tuning parameter.\n\nNote that if \\(m = p\\), we get bagging.\n\n\n\nThe final product is a black box\n\n\n\n\n\n\nA black box. Inside the box are several hundred trees, each slightly different.\nYou put an observation into the black box, and the black box classifies it or predicts it for you.\n\n\n\nRandom Forest in Python\n\nIn Python, we define a RandomForest algorithm for classification using the RandomForestClassifier function from scikit-learn. The n_estimators argument is the number of decision trees to generate in the RandomForest, and random_state allows you to reprudce the results.\n\n# Set the bagging algorithm.\nRFalgorithm = RandomForestClassifier(n_estimators = 500,\n                                     random_state = 59227)\n\n# Train the bagging algorithm.\nRFalgorithm.fit(X_train, Y_train)\n\n\n\nConfusion matrix\nEvaluate the performance of random forest.\n\n\nCode\n# Predict class.\nRF_predicted = RFalgorithm.predict(X_valid)\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_valid, RF_predicted)\n\n# Visualize the matrix.\nConfusionMatrixDisplay(cm).plot()\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\n\nThe accuracy of the random forest classifier is 79%.\n\n# Compute accuracy.\naccuracy = accuracy_score(Y_valid, RF_predicted)\n\n# Show accuracy.\nprint( round(accuracy, 2) )\n\n0.81"
  },
  {
    "objectID": "Module2/EnsembleMethods.html#return-to-main-page",
    "href": "Module2/EnsembleMethods.html#return-to-main-page",
    "title": "Ensamble Methods",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html",
    "href": "Module2/IntrotoDataScience.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Data Science\nSupervised and Unsupervised Learning"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#data-science",
    "href": "Module2/IntrotoDataScience.html#data-science",
    "title": "Introduction to Data Science",
    "section": "Data Science",
    "text": "Data Science\n\nData Science is …\na multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from vast amounts of structured and unstructured data.\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Similar Concepts\n\n\n\nData mining is a process of discovering patterns in large data sets using methods at the intersection of statistics and database systems.\nPredictive modeling is the process of developing a model so that we can understand and quantify the accuracy of the model’s prediction in yet-to-be-seen future data sets.\nStatistical learning refers to a set of tools (statistical models and data mining methods) for modeling and understanding complex data sets.\n\n\n\n\nIn 2004…\nHurricane Frances battered the Caribbean and threatened to directly affect Florida’s Atlantic coast.\n. . .\n\n\n\n\n\n\n\n\n\n\n\n. . .\nResidents headed for higher ground, but in Arkansas, Walmart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology.\n\n\n\n\n\n\n\n\nA week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressured her staff to create forecasts based on what had happened when Hurricane Charley hit the area several weeks earlier.\n\nBacked by trillions of bytes of purchase history stored in Walmart’s data warehouse, he said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as he put it.\n\n\n\n\n\n\n\n\n\n\nThe result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, not just the typical flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”\n\n\n\n\n\n\n\n\n\n\n\nCross-Industry Standard Process (CRISP) for Data Science\n\n\n\n\n\n\n\nCRISP Model\n\n\nBusiness Understanding: What does the business need?\nData Understanding: What data do we have or need? Is it clean?\nData Preparation: How do we organize the data for modeling?\nModeling: What modeling techniques should we apply?\nEvaluation: Which model best meets business objectives?\nImplementation: How do stakeholders access the results?\n\n\n\nBusiness Understanding\n\n\nBusiness understanding refers to defining the business problem you are trying to solve.\nThe goal is to reframe the business problem as a data science problem.\nReframing the problem and designing a solution is often an iterative process.\n\n\n\nProblems in Data Science\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes that individual belongs to. For example, “Among all T-Mobile customers, which ones are likely to respond to a given offer?”\n. . .\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”\n\n\n\n\nClustering attempts to group individuals in a population based on their similarity, but not for any specific purpose. For example, “Do our customers form natural groups or segments?”\n\n\nDiscussion\n\n\nOften, reframing the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary to formulate an acceptable solution.\nThe key to great success is creative problem formulation by an analyst on how to frame the business problem as one or more data science problems.\n\n\n\nData Understanding I\n\n\n\nIf the goal is to solve a business problem, data constitutes the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or without any explicit purpose.\n\n\n\n\nData Understanding II\n\n\nData costs vary. Some data will be available for free, while others will require effort to obtain.\n\n\n\nA key part of the data understanding phase is estimating the costs and benefits of each data source and deciding whether further investment is justified.\nEven after acquiring all the data sets, compiling them may require additional effort.\n\n\n\n\nExample\n\nIn the 1980s, credit cards were essentially priced uniformly because companies didn’t have adequate information systems to deal with differential pricing on a massive scale.\n\nAround 1990, Richard Fairbanks and Nigel Morris realized that information technology was powerful enough to enable more sophisticated predictive models and offer different terms (today: pricing, credit limits, low introductory rate balance transfers, cash back, and loyalty points).\n\n\n\n\nSignet Bank’s management was convinced that modeling profitability, not just the probability of default, was the right strategy.\nThey knew that a small proportion of customers actually account for more than 100% of a bank’s profit from credit card transactions (because the rest are either breaking even or losing money).\nIf they could model profitability, they could make better offers to the best customers and “skim the cream” of the big banks’ clientele.\n\n\n\n\nBut Signet Bank had a really big problem implementing this strategy.\nThey didn’t have the right data to model profitability for offering different terms to different customers!\nSince the bank offered credit with a specific set of terms and a specific default model, they had the data to model profitability (1) for the terms they actually offered in the past, and (2) for the type of customer actually offered credit.\n\n\n\n\nWhat could Signet Bank do? They put into play a fundamental data science strategy: acquire the necessary data at a cost!\nIn this case, data on customer profitability with different credit terms could be generated by conducting experiments. Different terms were randomly offered to different customers.\nThis might seem silly outside the context of data analytics thinking: you’re likely to lose money!\nThis is true. In this case, the losses are the cost of data acquisition.\n\n\nWhat happened?\nAs expected, Signet’s number of bad accounts skyrocketed.\nThe losses continued for several years while data scientists worked to build predictive models from the data, evaluate them, and implement them to improve profits.\nBecause the company viewed these losses as investments in data, they persisted despite complaints from stakeholders.\nEventually, Signet’s credit card business turned around and became so profitable that it was spun off to separate it from the bank’s other operations, which were now overshadowing the success of its consumer lending business.\n\n\nRichard Fairbanks and Nigel Morris\n\n\n\n\n\n\n\nFounders of\n\n\n\n\n\n\n\n\n\n\nMost Used Data Science Tools\n\nPython\nR\nSAS\nExcel\nPower BI\nTableau\nApache Spark\n\nhttps://hackr.io/blog/top-data-analytics-tools\n\n\nOther Tools Used\n\nRapidMiner (https://rapidminer.com/products/studio/)\nJMP (https://www.jmp.com/es_mx/home.html)\nMinitab (https://www.minitab.com/es-mx/products/minitab/)\nTrifacta (https://www.trifacta.com/)\nBigML (https://bigml.com/)\nMLBase (http://www.mlbase.org/)\nGoogle Cloud AutoML (https://cloud.google.com/automl/)"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#supervised-and-unsupervised-learning",
    "href": "Module2/IntrotoDataScience.html#supervised-and-unsupervised-learning",
    "title": "Introduction to Data Science",
    "section": "Supervised and Unsupervised Learning",
    "text": "Supervised and Unsupervised Learning\n\nTerminology\n\nPredictors. They are represented using the notation \\(X_1\\) for the first predictor, \\(X_p\\) for the second predictor, …, and \\(X_p\\) for the p-th predictor.\nResponse. \\(Y\\) represents the response variable, which we will attempt to predict.\n\n. . .\nWe want to establish the following relationship\n\\[\nY = f(X_1, X_2, \\ldots, X_p) + \\epsilon,\n\\]\nwhere \\(f\\) is a function of the predictors and \\(\\epsilon\\) is a natural (random) error.\n\n\nTypes of Learning\n\nIn data science (and machine learning), there are two main types of learning:\n\nSupervised learning\nUnsupervised learning\n\n\n\n\n\n\n\nSupervised Learning…\nIncludes algorithms that learn by example. The user provides the supervised algorithm with a known data set that includes the corresponding known inputs and outputs. The algorithm must find a method to determine how to reach those inputs and outputs.\nWhile the user knows the correct answers to the problem, the algorithm identifies patterns in the data, learns from observations, and makes predictions.\nThe algorithm makes predictions that can be corrected by the user, and this process continues until the algorithm reaches a high level of accuracy and performance.\n\n\nPopular Supervised Algorithms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning…\n\nstudies data to identify patterns. There is no answer key or human operator to provide instruction. The machine determines correlations and relationships by analyzing the available data.\nIn this process, the unsupervised algorithm is left to interpret large data sets. The algorithm attempts to organize that data in some way to describe its structure.\nAs it evaluates more data, its ability to make decisions about it gradually improves and becomes more refined.\n\n\nPopular Unsupervised Algorithms\n\n\n\n\n\n\n\nTwo Data Sets\n\nIn supervised learning, there are several types of data.\nTraining data is the data used to construct \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is the data that was NOT used in the fitting process, but is used to test the model’s performance on unanalyzed data.\n\n\n\n\n\n\n\n\nYogi Berra\n\n\nIt’s though to make predictions, especially about the future.\n\n\n\nLet’s Play\n\nLet’s play with supervised models.\n\nhttps://quickdraw.withgoogle.com/\nhttps://tenso.rs/demos/rock-paper-scissors/\nhttps://teachablemachine.withgoogle.com/"
  },
  {
    "objectID": "Module2/IntrotoDataScience.html#return-to-main-page",
    "href": "Module2/IntrotoDataScience.html#return-to-main-page",
    "title": "Introduction to Data Science",
    "section": "Return to main page",
    "text": "Return to main page"
  },
  {
    "objectID": "Module1/DataBases.html",
    "href": "Module1/DataBases.html",
    "title": "Introduction to Data Bases",
    "section": "",
    "text": "Dashboards para Visualizar Indicadores\nBases de Datos para Almacenar Indicadores"
  },
  {
    "objectID": "Module1/DataBases.html#dashboards-para-visualizar-indicadores",
    "href": "Module1/DataBases.html#dashboards-para-visualizar-indicadores",
    "title": "Introduction to Data Bases",
    "section": "Dashboards para Visualizar Indicadores",
    "text": "Dashboards para Visualizar Indicadores\n\nEl tablero de control integral\nUna vez que se definene los indicadores usando el Balance Scorecard y el formato de documentación, se procede a diseñar los tableros de control:\n\npantallas,\nestilos de gráficos y tablas,\nniveles de agregación,\nreportes predeterminados,\nrequerimientos de drill-down.\n\nEsto es tanto para el nivel general de la organización, como para los BSC funcionales.\n\n\nDespliegue del tablero de indicadores\nPara la implementación de los tableros de indicadores se pueden seguir diversas estrategias, por ejemplo:\n\nDesarrollo en una plataforma especializada para sistemas de indicadores (https://www.predictiveanalyticstoday.com/open-source-balanced-scorecard-software/)\nDesarrollo con herramientas genéricas OLAP On-line Analytical Processing (https://www.softwaretestinghelp.com/best-olap-tools/)\nImplementaciones aisladas de corto alcance en hoja de cálculo.\n\n\n\nEjemplo de tablero de indicadores de Tableu\n\nhttps://www.tableau.com/es-mx\n\n\nOtro Ejemplo"
  },
  {
    "objectID": "Module1/DataBases.html#bases-de-datos-para-almacenar-indicadores",
    "href": "Module1/DataBases.html#bases-de-datos-para-almacenar-indicadores",
    "title": "Introduction to Data Bases",
    "section": "Bases de Datos para Almacenar Indicadores",
    "text": "Bases de Datos para Almacenar Indicadores\n\nContexto\nAunque los datos para el cálculo de algunos indicadores tienen su origen fuera de la organización, la gran mayoría de los datos provienen de las bases de datos internas del negocio.\nExiste mucha diversidad de empresas, unas que tienen aplicaciones aisladas con bases de datos dispersas y con archivos en hojas de cálculo, hasta empresas muy organizadas con un sistema de bases centralizado en un servidor de datos y aplicaciones vinculadas.\n\n\n\nPara obtener los datos necesarios para el cálculo de indicadores, muchas veces será necesario integrar datos de diversas fuentes en la empresa. Para esto, se usan herramientas de ETL (extract-transform-load) y data warehousing.\n\n\n¿Qué es una Base de Datos?\nBásicamente, una tabla de datos donde los renglones representan un conjunto de ocurrencias de una entidad (clientes, productos, pacientes, pedidos) y las columnas representan atributos o características que describen a la entidad (cliente: ID + nombre + domicilio + e-mail + saldo ) En el ámbito de TI el concepto de “base de datos” no se refiere a una tabla, sino a un conjunto de tablas relacionadas.\n\n\nTerminología de BD\nLas tablas contienen datos que se refieren a:\n\nalguna entidad acerca de la cual la organización necesita mantener información\nrelaciones entre entidades.\n\nA los renglones de la tabla se les denomina registros.\nA las columnas de la tabla se les denomina campos.\n\n\n\n\n\nLos registros son ocurrencias diferentes de la entidad correspondiente.\nLos campos son atributos que describen la entidad.\nCada registro tiene uno o varios campos que identifican de manera única cada registro, esos campos se denominan “llave”.\n\n\n\n\n\n\nBases de Datos Relacionales\n\nActualmente el modelo de bases más utilizado en el mundo es el de bases de datos relacionales, ver por ejemplo:\nhttps://db-engines.com/en/ranking\nhttps://www.dataversity.net/database-management-trends-in-2020/\nAnte el surgimiento de aplicaciones de big data hace que estas bases de datos sean la principal fuente de datos para indicadores en las empresas.\n\n\n\nEjemplo de Base de Datos Relacional\n\n\n\nEjemplo\n\n\n\nLa base de datos tiene 4 tablas: Pacientes, Medicinas, Recetas y Detalle de las recetas.\nLa tabla que contiene datos de la entidad “paciente” tiene los siguientes campos: identificador único, nombre, domicilio, fecha de nacimiento, teléfono, y foto.\n\n\n\n\n\n\n\n\n\n\n\nLa tabla de recetas contiene los datos generales de cada receta expedida: número de receta (es la llave), fecha, hora de consulta, un indicador de si contiene medicamentos controlados o no, y el identificador del paciente, este campo permite relacionar los datos de la receta con los datos del paciente.\n\n\n\n\n\n\n\n\n\n\n\nLa tabla de medicinas tiene los campos: identificador de la medicina, descripción genérica, agente activo, presentación más común, y contra-indicaciones.\n\n\n\n\n\n\n\n\n\n\n\nLa tabla de detalle de la receta contiene los renglones de cada receta. Como hay recetas que pueden tener un solo medicamento, puede haber algunas con 2, 3 o más medicamentos, en el modelo relacional se guardan los renglones de todas las recetas en una sola tabla, todos los renglones de una receta tienen el mismo “número de receta” pero diferente “id del medicamento”…\n\n\n\n\n\n\n\n\n\n\n\nEn la tabla Detalle de Receta puede haber múltiples registros con el mismo Número de receta, porque la receta puede amparar varios medicamentos. También puede haber múltiples registros con el mismo ID de medicamento, porque un medicamento puede aparecer en muchas recetas.\n\n\n\n\n\n\n\n\n\n\n\nEn este caso, para identificar de manera única un renglón de una receta en particular se requieren los dos identificadores, el de la receta y el del medicamento, esto constituye una llave compuesta.\n\n\n\n\n\n\n\nConsultas (queries) a la BD\nUna vez que se tiene la base de datos, es posible contestar preguntas como:\n\n¿En cuáles colonias viven los pacientes a quienes se les ha recetado turbocicloxina?\n¿A cuántos pacientes se les ha recetado turbocicloxina en el último mes?\n¿Cuáles agentes activos se les han administrado a los pacientes que viven en Prados #520 en el último año?\n\n\n\nSQL\n\nLas herramientas y plataformas de Business Intelligence con las que contamos actualmente, nos permiten hacer fácilmente consultas como las del ejemplo anterior, simplemente “arrastrando” campos y aplicando filtros en un lienzo de diseño.\nInternamente las herramientas procesan las consultas mediante un lenguaje de manejo de bases de datos que se llama SQL Structured Query Language.\nPara hacer consultas más complejas puede ser necesario que se haga la consulta escribiendo directamente el código de SQL.\nEn este módulo no estudiaremos SQL."
  },
  {
    "objectID": "Module1/DataBases.html#return-to-main-page",
    "href": "Module1/DataBases.html#return-to-main-page",
    "title": "Introduction to Data Bases",
    "section": "Return to main page",
    "text": "Return to main page"
  }
]
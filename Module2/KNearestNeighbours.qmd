---
title: "*K* nearest neighbors"
subtitle: "IN2004B: Generation of Value with Data Analytics"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: false
    footer: "Tecnologico de Monterrey"
    logo: IN1002b_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Load the libraries

Before we start, let's import the data science libraries into Python.

```{python}
#| echo: true
#| output: false

# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay 
from sklearn.metrics import accuracy_score, recall_score, precision_score
```

Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.

## *K*-nearest neighbors (KNN)

</br>

KNN a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.

**Basic idea**: Predict a new observation using the *K* closest observations in the training dataset.

To predict the response for a new observation, KNN uses the *K* nearest neighbors (observations) in [***terms of the predictors!***]{style="color:brown;"}

The predicted response for the new observation is the most common response among the *K* nearest neighbors.

## The algorithm has 3 steps:

</br></br>

::: incremental
1.  Choose the number of nearest neighbors (*K*).

2.  For a new observation, find the *K* closest observations in the training data (ignoring the response).

3.  For the new observation, the algorithm predicts the value of the most common response among the *K* nearest observations.
:::

## Nearest neighbour

Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.

![](images/KNNsequence1.png){fig-align="center"}

[A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.]{style="color:white;"}

## Nearest neighbour

Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.

![](images/KNNsequence2.png){fig-align="center"}

A new observation arrives, and we don't know which group it belongs to. [If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.]{style="color:white;"}

## Nearest neighbour

Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.

![](images/KNNsequence3.png){fig-align="center"}

A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.

## Nearest neighbour

Suppose we have two groups: red and green group. The number line shows the value of a predictor for our training data.

![](images/KNNsequence4.png){fig-align="center"}

A new observation arrives, and we don't know which group it belongs to. If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.

## Banknote data

::::: columns
::: {.column width="70%"}
```{python}
#| fig-align: center
#| echo: false
#| output: true

from matplotlib.patches import Circle

bank_data = pd.read_excel("banknotes.xlsx")
# Set response variable as categorical.
bank_data['Status'] = pd.Categorical(bank_data['Status'])
# Set plot style
sns.set(style="whitegrid")

# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6.3, 6.3))
sns.scatterplot(
    data=bank_data,
    x='Top',
    y='Bottom',
    hue='Status',
    palette={'genuine': 'blue', 'counterfeit': 'orange'},
    s=15,
    edgecolor=None,
    legend='full'
)

# Axis labels
plt.xlabel("Top")
plt.ylabel("Bottom")

# Clean layout
plt.tight_layout()
plt.show()
```
:::

::: {.column width="30%"}
:::
:::::

Using $K = 3$, that's 2 votes for "genuine" and 2 for "fake." So we classify it as "genius."

## Banknote data

::::: columns
::: {.column width="70%"}
```{python}
#| fig-align: center
#| echo: false
#| output: true

from matplotlib.patches import Circle

bank_data = pd.read_excel("banknotes.xlsx")
# Set response variable as categorical.
bank_data['Status'] = pd.Categorical(bank_data['Status'])
# Set plot style
sns.set(style="whitegrid")

# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6.3, 6.3))
sns.scatterplot(
    data=bank_data,
    x='Top',
    y='Bottom',
    hue='Status',
    palette={'genuine': 'blue', 'counterfeit': 'orange'},
    s=15,
    edgecolor=None,
    legend='full'
)

# Add star point at (10, 10)
plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')

# Axis labels
plt.xlabel("Top")
plt.ylabel("Bottom")

# Clean layout
plt.tight_layout()
plt.show()
```
:::

::: {.column width="30%"}
:::
:::::

Using $K = 3$, that's 2 votes for "genuine" and 2 for "fake." So we classify it as "genius."

## Banknote data

::::: columns
::: {.column width="70%"}
```{python}
#| fig-align: center
#| echo: false
#| output: true

from matplotlib.patches import Circle

bank_data = pd.read_excel("banknotes.xlsx")
# Set response variable as categorical.
bank_data['Status'] = pd.Categorical(bank_data['Status'])
# Set plot style
sns.set(style="whitegrid")

# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6.3, 6.3))
sns.scatterplot(
    data=bank_data,
    x='Top',
    y='Bottom',
    hue='Status',
    palette={'genuine': 'blue', 'counterfeit': 'orange'},
    s=15,
    edgecolor=None,
    legend='full'
)

# Add star point at (10, 10)
plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')

# Add circle around the point with diameter = 1 unit (radius = 0.5)
circle = Circle((10, 10), 0.5, edgecolor='black', facecolor='none', linewidth=2)
plt.gca().add_patch(circle)

# Axis labels
plt.xlabel("Top")
plt.ylabel("Bottom")

# Clean layout
plt.tight_layout()
plt.show()
```
:::

::: {.column width="30%"}
</br>

Using $K = 3$, that's 3 votes for "counterfeit" and 0 for "genuine." So we classify it as "counterfeit."
:::
:::::

## Banknote data

::::: columns
::: {.column width="70%"}
```{python}
#| fig-align: center
#| echo: false
#| output: true

from matplotlib.patches import Circle

bank_data = pd.read_excel("banknotes.xlsx")
# Set response variable as categorical.
bank_data['Status'] = pd.Categorical(bank_data['Status'])
# Set plot style
sns.set(style="whitegrid")

# Create the scatter plot using seaborn for discrete color mapping
plt.figure(figsize=(6.3, 6.3))
sns.scatterplot(
    data=bank_data,
    x='Top',
    y='Bottom',
    hue='Status',
    palette={'genuine': 'blue', 'counterfeit': 'orange'},
    s=15,
    edgecolor=None,
    legend='full'
)

# Add star point at (10, 10)
plt.plot(10, 10, marker='*', markersize=15, color='red', label='Special Point')

# Add circle around the point with diameter = 1 unit (radius = 0.5)
circle = Circle((10, 10), 0.5, edgecolor='black', facecolor='none', linewidth=2)
plt.gca().add_patch(circle)

# Axis labels
plt.xlabel("Top")
plt.ylabel("Bottom")

# Clean layout
plt.tight_layout()
plt.show()
```
:::

::: {.column width="30%"}
</br>

Using $K = 3$, that's 3 votes for "counterfeit" and 0 for "genuine." So we classify it as "counterfeit."

[Closeness is based on Euclidean distance.]{style="color:darkblue;"}
:::
:::::

## Implementation Details

</br></br></br>

**Ties**

-   If there are more than *K* nearest neighbors, include them all.

-   If there is a tie in the vote, set a rule to break the tie. For example, randomly select the class.

## 

[KNN uses the Euclidean distance between points]{style="color:darkblue;"}. So it ignores units.

-   Example: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).

-   These people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.

-   Therefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.

. . .

**Therefore, as a first step, we must transform the predictors so that they have the same units!**

## Standardization

</br>

Standardization refers to *centering* and *scaling* each numerical predictor individually. This places all predictors on the same scale.

To **center** a predictor variable, the mean value of the predictor is subtracted from all values.

Therefore, the centered predictor has a mean of zero (i.e., its average value is zero).

## 

</br>

To **scale** a predictor, each of its values is divided by its standard deviation.

When scaling the data, the values have a common standard deviation of one.

In mathematical terms, we standardize a predictor as:

$${\color{blue} \tilde{X}_{i}} = \frac{{ X_{i} - \bar{X}}}{ \sqrt{\frac{1}{n -1} \sum_{i=1}^{n} (X_{i} - \bar{X})^2}},$$

with $\bar{X} = \sum_{i=1}^n \frac{x_i}{n}$.

## Example 1

A market analyst is studying quality characteristics of cars. Specifically, the analyst is investigating the miles per gallon (mpg) of cars can be predicted using:

-   $X_1:$ cylinders. Number of cylinders between 4 and 8
-   $X_2:$ displacement. Engine displacement (cu. inches)
-   $X_3:$ horsepower. Engine horsepower
-   $X_4:$ weight. Vehicle weight (lbs.)
-   $X_5:$ acceleration. Time to accelerate from 0 to 60 mph (sec.)
-   $X_6:$ origin. Origin of car (American, European, Japanese)

## 

</br></br>

The dataset is in the file "auto.xlsx". Let's read the data using **pandas**.

```{python}
#| echo: true

# Load the Excel file into a pandas DataFrame.
auto_data = pd.read_excel("auto.xlsx")

# Set categorical variables.
auto_data['origin'] = pd.Categorical(auto_data['origin'])
```

## Two predictors in original units

Consider the previously created `auto_data` dataset. Consider two points on the graph: $(175, 5140)$ and $(69, 1613)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

plt.figure(figsize=(5,5))
sns.scatterplot(data = auto_data, x = 'horsepower', y = 'weight')
plt.scatter(x = 175, y = 5140, color = 'red')
plt.scatter(x = 69, y = 1613, color = 'red')
plt.xlabel('Horsepower', fontsize=14)
plt.ylabel('Weight', fontsize=14)
plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(69 - 175)^2 + (1613-5140)^2}$ $= \sqrt{11236 + 12439729}$ $= 3528.592$.
:::
:::::

## Standardization in Python

</br>

To standardize **numeric** predictors, we use the `StandardScaler()` function. We also apply the function to variables using the `fit_transform()` function.

</br>

```{python}
#| echo: true

#scaler = StandardScaler()
#Xs = scaler.fit_transform(complete_sbAuto)
```

## 

Unfortunately, the resulting object isn't a Pandas data frame. So, we converted it to this format.

```{python}
#| echo: true

#scaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)
#scaled_df.head()
```

## Two predictors in standardized units

On the new scale, the two points are now: $(1.82, 2.53)$ and $(-0.91, -1.60)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

#plt.figure(figsize=(5,5))
#sns.scatterplot(data = scaled_df, x = 'horsepower', y = 'weight')
#plt.scatter(x = 1.83, y = 2.54, color = 'red')
#plt.scatter(x = -0.90, y = -1.60, color = 'red')
#plt.xlabel('Standardized horsepower', fontsize=14)
#plt.ylabel('Standardized weight', fontsize=14)
#plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}$ $= \sqrt{7.45 + 17.05} = 4.95$.
:::
:::::

## Discussion

</br>

KNN is intuitive and simple and can produce decent predictions. However, KNN has some disadvantages:

-   When the training dataset is very large, KNN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. ("*Lazy learner*").

-   In this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with.

## 

::: {style="font-size: 90%;"}
-   The predictive performance of KNN deteriorates as the number of predictors increases.

-   This is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.

-   This is known as the ***curse of dimensionality***.
:::

![](images/clipboard-72810347.png){fig-align="center"}

::: {style="font-size: 50%;"}
<https://aiaspirant.com/curse-of-dimensionality/>
:::

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)

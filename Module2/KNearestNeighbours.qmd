---
title: "*K* nearest neighbors"
subtitle: "IN2004B: Generation of Value with Data Analytics"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: false
    footer: "Tecnologico de Monterrey"
    logo: IN1002b_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Load the libraries

Before we start, let's import the data science libraries into Python.

```{python}
#| echo: true
#| output: false

# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay 
from sklearn.metrics import accuracy_score, recall_score, precision_score
```

Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.


## *K* nearest neighbors

This is a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.

**Basic idea**: Predict a new observation using the *K* closest observations in the training dataset.

To predict the response for a new observation, *K*-NN uses the *K* nearest neighbors (observations) in [***terms of the predictors!***]{style="color:aqua;"}

The predicted response for the new observation is the most common response among the *K* nearest neighbors.

## The algorithm has 3 steps:

1.  Choose the number of nearest neighbors (*K*).

2.  For a new observation, find the *K* closest observations in the training data (ignoring the response).

3.  For the new observation, the algorithm predicts the value of the most common response among the *K* nearest observations.

## 

Suppose we have two groups: the red group and the green group. The number line shows the value of a variable for our training data.

A new observation arrives, and we don't know which group it belongs to.

![](images/ball_example.png){fig-align="center"}

If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.

## 

Using $K = 3$, that's 2 votes for "genuine" and 2 for "fake." So we classify it as "genius."

![](images/ball_example2.png){fig-align="center"}

Closeness is based on Euclidean distance.

## Implementation Details

**Ties**

-   If there are more than *K* nearest neighbors, include them all.

-   If there is a tie in the vote, set a rule to break the tie. For example, randomly select the class.

## 

***KNN uses the Euclidean distance between points***. So it ignores units.

-   Example: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).

-   These people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.

-   Therefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.

## 

As a first step, we must transform the predictors so that they have the same units!

This requires a predictor standardization process, which is done in Python.

## Standardization

</br>

Standardization refers to *centering* and *scaling* each numerical predictor individually. This places all predictors on the same scale.

To **center** a predictor variable, the mean value of the predictor is subtracted from all values.

Therefore, the centered predictor has a mean of zero (i.e., its average value is zero).

## 

</br>

To **scale** a predictor, each of its values is divided by its standard deviation.

When scaling the data, the values have a common standard deviation of one.

In mathematical terms, we standardize a predictor as:

$${\color{blue} \tilde{X}_{i}} = \frac{{ X_{i} - \bar{X}}}{ \sqrt{\frac{1}{n -1} \sum_{i=1}^{n} (X_{i} - \bar{X})^2}},$$

with $\bar{X} = \sum_{i=1}^n \frac{x_i}{n}$.

## Example 1 (cont.)

We use the five numeric predictors from the `complete_sbAuto` dataset.

```{python}
#| echo: true

#complete_sbAuto.head()
```

## Two predictors in original units

Consider the previously created `complete_sbAuto` dataset. Consider two points on the graph: $(175, 5140)$ and $(69, 1613)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

#plt.figure(figsize=(5,5))
#sns.scatterplot(data = complete_sbAuto, x = 'horsepower', y = 'weight')
#plt.scatter(x = 175, y = 5140, color = 'red')
#plt.scatter(x = 69, y = 1613, color = 'red')
#plt.xlabel('Horsepower', fontsize=14)
#plt.ylabel('Weight', fontsize=14)
#plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(69 - 175)^2 + (1613-5140)^2}$ $= \sqrt{11236 + 12439729}$ $= 3528.592$.
:::
:::::

## Standardization in Python

</br>

To standardize **numeric** predictors, we use the `StandardScaler()` function. We also apply the function to variables using the `fit_transform()` function.

</br>

```{python}
#| echo: true

#scaler = StandardScaler()
#Xs = scaler.fit_transform(complete_sbAuto)
```

## 

Unfortunately, the resulting object isn't a Pandas data frame. So, we converted it to this format.

```{python}
#| echo: true

#scaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)
#scaled_df.head()
```

## Two predictors in standardized units

On the new scale, the two points are now: $(1.82, 2.53)$ and $(-0.91, -1.60)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

#plt.figure(figsize=(5,5))
#sns.scatterplot(data = scaled_df, x = 'horsepower', y = 'weight')
#plt.scatter(x = 1.83, y = 2.54, color = 'red')
#plt.scatter(x = -0.90, y = -1.60, color = 'red')
#plt.xlabel('Standardized horsepower', fontsize=14)
#plt.ylabel('Standardized weight', fontsize=14)
#plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}$ $= \sqrt{7.45 + 17.05} = 4.95$.
:::
:::::

## Discussion

*K*-NN is intuitive and simple and can produce decent predictions. However, *K*-NN has some disadvantages:

-   When the training dataset is very large, *K*-NN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. ("Lazy learner").

-   In this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with.

## 

-   The predictive performance of *K*-NN deteriorates as the number of predictors increases.

-   This is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.

-   This is known as the ***curse of dimensionality***.

![](images/clipboard-72810347.png)

<https://aiaspirant.com/curse-of-dimensionality/>

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)

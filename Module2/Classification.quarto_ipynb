{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Classification Trees\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN2004B_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Introduction\n",
        "2.  Training, Validation, and Test Data\n",
        "3.  Classification Trees\n",
        "4.  Classification Algorithm Metrics\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "</br>\n",
        "\n",
        "Before we start, let's import the data science libraries into Python.\n"
      ],
      "id": "624bac7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score"
      ],
      "id": "db475076",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.\n",
        "\n",
        "## scikit-learn library\n",
        "\n",
        "-   **scikit-learn** is a robust and popular library for machine learning in Python\n",
        "-   It provides simple, efficient tools for data mining and data analysis\n",
        "-   It is built on top of libraries such as **NumPy**, **SciPy**, and **Matplotlib**\n",
        "-   <https://scikit-learn.org/stable/>\n",
        "\n",
        "![](images/scikitlearn.png){fig-align=\"center\"}\n",
        "\n",
        "## Main data science problems\n",
        "\n",
        "</br>\n",
        "\n",
        "[**Regression Problems**]{style=\"color:green;\"}. The response is numerical. For example, a person's income, the value of a house, or a patient's blood pressure.\n",
        "\n",
        "[**Classification Problems**]{style=\"color:blue;\"}. The response is categorical and involves *K* different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\n",
        "\n",
        "The predictors can be *numerical* or *categorical*.\n",
        "\n",
        "## Main data science problems\n",
        "\n",
        "</br>\n",
        "\n",
        "[**Regression Problems**. The response is numerical. For example, a person's income, the value of a house, or a patient's blood pressure.]{style=\"color:gray;\"}\n",
        "\n",
        "[**Classification Problems**]{style=\"color:blue;\"}. The response is categorical and involves *K* different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\n",
        "\n",
        "The predictors can be *numerical* or *categorical*.\n",
        "\n",
        "## Terminology\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Recall that\n",
        "\n",
        "-   $X$ represents a predictor or explanatory variable.\n",
        "-   $\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)$ represents a collection of $p$ predictors.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "[Response]{style=\"text-decoration: underline;\"}:\n",
        "\n",
        "::: incremental\n",
        "-   $Y$ is a [**categorical variable**]{style=\"color:darkgreen;\"} that takes [**2 categories**]{style=\"color:darkgreen;\"} or [**classes**]{style=\"color:darkgreen;\"}.\n",
        "\n",
        "-   For example, $Y$ can take [0]{style=\"color:darkgreen;\"} or [1]{style=\"color:darkgreen;\"}, [A]{style=\"color:darkgreen;\"} or [B]{style=\"color:darkgreen;\"}, [no]{style=\"color:darkgreen;\"} or [yes]{style=\"color:darkgreen;\"}, [spam]{style=\"color:darkgreen;\"} or [no spam]{style=\"color:darkgreen;\"}.\n",
        "\n",
        "-   When classes are strings, they are usually encoded as 0 and 1.\n",
        "\n",
        "    -   The **target class** is the one for which $Y = 1$.\n",
        "    -   The **reference class** is the one for which $Y = 0$.\n",
        ":::\n",
        "\n",
        "## Classification algorithms\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Classification algorithms use predictor values [*to predict the class*]{style=\"color:#4682B4;\"} of the response (either target or reference).\n",
        "\n",
        "</br>\n",
        "\n",
        "That is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n",
        "\n",
        "## The goal of classification algorithms\n",
        "\n",
        "</br>\n",
        "\n",
        "[**Goal**]{style=\"color:darkgreen;\"}: Develop a function $C(\\boldsymbol{X})$ for predicting $Y = \\{0, 1\\}$ from $\\boldsymbol{X}$.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "To achieve this goal, most algorithms consider functions $C(\\boldsymbol{X})$ that [**predict the probability**]{style=\"color:brown;\"} that $Y$ takes the value of 1.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "A probability for each class can be very useful for gauging the model’s confidence about the predicted classification.\n",
        "\n",
        "## Example 1\n",
        "\n",
        "Consider a spam filter where $Y$ is the email type.\n",
        "\n",
        "-   The target class is spam. In this case, $Y=1$.\n",
        "-   The reference class is not spam. In this case, $Y=0$.\n",
        "\n",
        ". . .\n",
        "\n",
        "![](images/spam.png){fig-align=\"center\" width=\"556\" height=\"178\"}\n",
        "\n",
        ". . .\n",
        "\n",
        "Both emails would be classified as spam. However, we would have greater confidence in our classification for the second email.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Technically, $C(\\boldsymbol{X})$ works with the *conditional probability*:\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "$$P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$$\n",
        ":::\n",
        "\n",
        "In words, this is the probability that $Y$ takes a value of 1 [**given that**]{style=\"color:brown;\"} the predictors $\\boldsymbol{X}$ have taken the values $\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)$.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "The conditional probability that $Y$ takes the value of 0 is\n",
        "\n",
        "$$P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).$$\n",
        "\n",
        "## Bayes classifier\n",
        "\n",
        "</br>\n",
        "\n",
        "It turns out that, if we know the true structure of $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$, we can build a good classification function called the [**Bayes classifier**]{style=\"color:darkblue;\"}:\n",
        "\n",
        "$$C(\\boldsymbol{X}) =\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) > 0.5 \\\\\n",
        "      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n",
        "    \\end{cases}.$$\n",
        "\n",
        "This function classifies to the most probable class using the conditional distribution $P(Y | \\boldsymbol{X} = \\boldsymbol{x})$.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "[HOWEVER, we don’t (and will never) know the true form of $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$!]{style=\"color:red;\"}\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "To overcome this issue, we several methods:\n",
        "\n",
        "::: incremental\n",
        "-   [**Logistic Regression**]{style=\"color:brown;\"}: Impose an structure on $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$. This was covered in IN1002B.\n",
        "-   [**Classification Trees**]{style=\"color:darkblue;\"}: Estimate $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ directly. What we will cover today.\n",
        "-   [**Ensemble methods**]{style=\"color:orange;\"} and [***K*****-Nearest Neighbours**]{style=\"color:darkgreen;\"}: Estimate $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ directly. (If time permits).\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Once we estimate $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ using one of these methods, we plug it into the Bayes classifier:\n",
        "\n",
        "$$\\hat{C}(\\boldsymbol{X}) =\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) > 0.5 \\\\\n",
        "      0, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n",
        "    \\end{cases}.$$\n",
        "\n",
        "where\n",
        "\n",
        "-   $\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ is an estimate of $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$.\n",
        "-   $\\hat{C}(\\boldsymbol{X})$ is an estimate of the true Bayes Classifier $C(\\boldsymbol{X})$\n",
        "\n",
        "# Training, Validation, and Test Data\n",
        "\n",
        "## Two datasets\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The application of data science algorithms needs two data sets:\n",
        "\n",
        "::: incremental\n",
        "-   [**Training data**]{style=\"color:blue;\"} is data that we use to train or construct the approximation $\\hat{C}(\\boldsymbol{X})$.\n",
        "\n",
        "-   [**Test data**]{style=\"color:green;\"} is data that we use to evaluate the classification performance of $\\hat{C}(\\boldsymbol{X})$ only.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/training.png){width=\"256\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "</br>\n",
        "\n",
        "A random sample of $n$ observations.\n",
        "\n",
        "Use it to **construct** $\\hat{C}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/test.png){width=\"262\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "Another random sample of $n_t$ observations, which is independent of the training data.\n",
        "\n",
        "Use it to **evaluate** $\\hat{C}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Validation dataset\n",
        "\n",
        "In many practical situations, a test dataset is not available. To overcome this issue, we use a [**validation dataset**]{style=\"color:orange;\"}.\n",
        "\n",
        "![](images/validation.png){fig-align=\"center\" width=\"645\"}\n",
        "\n",
        ". . .\n",
        "\n",
        "**Idea**: Apply model to your [**validation dataset**]{style=\"color:orange;\"} to mimic what will happen when you apply it to test dataset.\n",
        "\n",
        "## Example 2: Identifying Counterfeit Banknotes\n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/clipboard-270396609.png)\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The data is located in the file \"banknotes.xlsx\".\n"
      ],
      "id": "6b2c0aac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "bank_data.head()"
      ],
      "id": "35de7b38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How do we generate validation data?\n",
        "\n",
        "We split the current dataset into a training and a validation dataset. To this end, we use the function `train_test_split()` from **scikit-learn**.\n",
        "\n",
        "</br>\n",
        "\n",
        "The function has three main inputs:\n",
        "\n",
        "-   A pandas dataframe with the predictor columns only.\n",
        "-   A pandas dataframe with the response column only.\n",
        "-   The parameter `test_size` which sets the portion of the dataset that will go to the validation set.\n",
        "\n",
        "## Create the predictor matrix\n",
        "\n",
        "We use the function `.filter()` from **pandas** to select two predictors: Top and Bottom.\n"
      ],
      "id": "c02cb4ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of predictors.\n",
        "X_full = bank_data.filter(['Top', 'Bottom'])\n",
        "X_full.head(4)"
      ],
      "id": "534aeb6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the response column\n",
        "\n",
        "We do the same to extract the column `Status` from the data frame. We store the result in `Y_full`.\n"
      ],
      "id": "38334628"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of responses.\n",
        "Y_full = bank_data['Status']\n",
        "Y_full.head(4)"
      ],
      "id": "70442961",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set the target category\n",
        "\n",
        "To set the target category in the response we use the `get_dummies()` function.\n"
      ],
      "id": "c455c9f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Create dummy variables.\n",
        "Y_dummies = pd.get_dummies(Y_full, dtype = 'int')\n",
        "\n",
        "# Select target variable.\n",
        "Y_target_full = Y_dummies['counterfeit']\n",
        "\n",
        "# Show target variable.\n",
        "Y_target_full.head() "
      ],
      "id": "b95a64f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's partition the dataset\n",
        "\n",
        "</br>\n"
      ],
      "id": "7eef29d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Split the dataset into training and validation.\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_target_full, \n",
        "                                                      test_size = 0.3, random_state=507134)"
      ],
      "id": "204e7041",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The function makes a clever partition of the data using the *empirical* distribution of the response.\n",
        "\n",
        "-   Technically, it splits the data so that the distribution of the response under the training and validation sets is similar.\n",
        "\n",
        "-   Usually, the proportion of the dataset that goes to the validation set is 20% or 30%.\n",
        "\n",
        "## \n",
        "\n",
        "The predictors and response in the training dataset are in the objects `X_train` and `Y_train`, respectively. We compile these objects into a single dataset using the function `.concat()` from **pandas**. The argument `axis = 1` tells `.concat()` to concatenate the datasets by their rows.\n"
      ],
      "id": "2bc75a3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "training_dataset = pd.concat([X_train, Y_train], axis = 1)\n",
        "training_dataset.head(4)"
      ],
      "id": "eaded9bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Equivalently, the predictors and response in the validation dataset are in the objects `X_valid` and `Y_valid`, respectively.\n"
      ],
      "id": "a82f275e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "validation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\n",
        "validation_dataset.head()"
      ],
      "id": "1fb76830",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Work on your training dataset\n",
        "\n",
        "</br>\n",
        "\n",
        "After we have partitioned the data, we **work on the** [**training data**]{style=\"color:blue;\"} to develop our predictive pipeline.\n",
        "\n",
        "The pipeline has two main steps:\n",
        "\n",
        "1.  Data preprocessing.\n",
        "2.  Algorithm development.\n",
        "\n",
        "Note that all preprocessing techniques will also be applied to the [**validation**]{style=\"color:orange;\"} and [**test**]{style=\"color:green;\"} datasets to prepare it for your algorithm!\n",
        "\n",
        "# Classification Trees\n",
        "\n",
        "## Decision tree\n",
        "\n",
        "It is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n",
        "\n",
        "</br>\n",
        "\n",
        "Main characteristics:\n",
        "\n",
        "-   Simple and useful for interpretation.\n",
        "\n",
        "-   Can handle numerical and categorical predictors and responses.\n",
        "\n",
        "-   Computationally efficient.\n",
        "\n",
        "-   Nonparametric technique.\n",
        "\n",
        "## Basic idea of a decision tree\n",
        "\n",
        "Stratify or segment the predictor space into several simpler regions.\n"
      ],
      "id": "87b38af7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(9.3, 5.3))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=11.3, ymin=0, ymax=0.438, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Add region labels\n",
        "plt.text(8.5, 11.5, r\"$R_1$\", fontsize=30, fontweight='bold')\n",
        "plt.text(8.5, 8.0, r\"$R_2$\", fontsize=30, fontweight='bold')\n",
        "plt.text(10.95, 7.2, r\"$R_3$\", fontsize=20, fontweight='bold')\n",
        "plt.text(11.5, 8.0, r\"$R_4$\", fontsize=30, fontweight='bold')\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "5551db8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "0fd74abc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=11.3, ymin=0, ymax=0.438, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Add region labels\n",
        "plt.text(8.5, 11.5, r\"$R_1$\", fontsize=30, fontweight='bold')\n",
        "plt.text(8.5, 8.0, r\"$R_2$\", fontsize=30, fontweight='bold')\n",
        "plt.text(10.95, 7.2, r\"$R_3$\", fontsize=20, fontweight='bold')\n",
        "plt.text(11.5, 8.0, r\"$R_4$\", fontsize=30, fontweight='bold')\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f37b1153",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_plot.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "![](images/tree_scheme.png){fig-align=\"center\"}\n",
        "\n",
        "## How do you build a decision tree?\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Building decision trees involves two main procedures:\n",
        "\n",
        "1.  [Grow a large tree.]{style=\"color:darkblue;\"}\n",
        "\n",
        "2.  [Prune the tree to prevent overfitting.]{style=\"color:darkblue;\"}\n",
        "\n",
        "After building a “good” tree, we can predict new observations that are not in the data set we used to build it.\n",
        "\n",
        "## How do we grow a tree?\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "**Using the CART algorithm!**\n",
        "\n",
        "::: incremental\n",
        "-   The algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\n",
        "\n",
        "-   Basically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that **minimizes** the so-called [***impurity***]{style=\"color:purple;\"}.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "8ecf0317"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=7.2, color='black', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\", color='red')\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "61c26874",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "1c0d7612"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=7.5, color='black', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\", color='red')\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "44cd1afe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "e3ad9da9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=7.7, color='black', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\", color='red')\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ecac326e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "86a754e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=7.9, color='black', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\", color='red')\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "838825c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "126c6e6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=12.5, color='black', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\", color='red')\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "57619a3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "60179b3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axvline(x=7.8, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4a5f461b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "7959196b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axvline(x=8.0, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "098db9c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "cd724224"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axvline(x=8.2, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "9545b75d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "c6bc6648"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axvline(x=12.5, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fe049444",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "f2b96792"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "2dba58af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_1.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "e3499e87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=7.8, ymin=9.55, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "54ab4733",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "2976ea7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=8.0, ymin=9.55, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "21bfb378",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "fcafb9ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=8.2, ymin=9.55, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c51fb838",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "2e0d0a82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=12.5, ymin=9.55, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "5908a5d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "143f3341"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=7.8, ymin=0, ymax=0.43, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=8.0, ymin=0, ymax=0.43, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=8.2, ymin=0, ymax=0.43, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=12.5, ymin=0, ymax=0.43, color='black', linewidth=3)  # Left vertical\n",
        "\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "64ff80a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_4.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "28f2b505"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='orange', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.43, color='green', linewidth=3)  # Left vertical\n",
        "\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\", color = \"red\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c1a54089",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_sequence_6.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "We repeat the partitioning process until:\n",
        "\n",
        "-   *impurity* does not improve in any of the terminal nodes, or\n",
        "-   each terminal node has no less than, say, 5 observations.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/full_tree.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## What is impurity?\n",
        "\n",
        "Node impurity refers to the homogeneity of the response classes at that node.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/Impurity1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/Impurity2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "[*The CART algorithm minimizes impurity between tree nodes.*]{style=\"color:darkgray;\"}\n",
        "\n",
        "## How do we measure impurity?\n",
        "\n",
        "::::::: center\n",
        ":::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "</br>\n",
        "\n",
        "There are three different metrics for impurity:\n",
        "\n",
        "-   Misclassification risk.\n",
        "\n",
        "-   Cross entropy.\n",
        "\n",
        "-   Gini impurity index.\n",
        ":::\n",
        "\n",
        ":::: {.column width=\"60%\"}\n",
        "![](images/Metrics1.png){fig-align=\"center\" width=\"469\"} ![](images/Metrics2.png){fig-align=\"center\" width=\"396\"}\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "p: Proportion of elements in the target class\n",
        ":::\n",
        "::::\n",
        "::::::\n",
        ":::::::\n",
        "\n",
        "## Mathematically\n",
        "\n",
        "</br>\n",
        "\n",
        "Let $p_1$ and $p_2$ be the proportion of observations in the target and reference class, respectively, in a node.\n",
        "\n",
        "-   Misclassification risk: $1 - \\max\\{p_1, p_2\\}$\n",
        "\n",
        "-   Cross entropy: $- (p_1 \\log(p_1) + p_2 \\log(p_2))$\n",
        "\n",
        "-   Gini impurity index: $p_1(1 - p_1) + p_2(1 - p_2)$\n",
        "\n",
        "## In Python\n",
        "\n",
        "In Python, we use the `DecisionTreeClassifier()` and `fit()` functions from **scikit-learn** to train a classification tree.\n"
      ],
      "id": "2d83b04b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# We tell Python we want a classification tree\n",
        "clf = DecisionTreeClassifier(min_samples_leaf= 5, ccp_alpha=0, \n",
        "                              random_state=507134)\n",
        "\n",
        "# We train the classification tree using the training data.\n",
        "clf.fit(X_train, Y_train)"
      ],
      "id": "e3b067ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameter `min_samples_leaf` controls the minimum number of observations in a terminal node, and the `cc_alpha` controls the tree complexity (to be described later). The parameter `random_state` allows you to reproduce the same tree in different runs of the Python code.\n",
        "\n",
        "## Plotting the tree\n",
        "\n",
        "</br>\n",
        "\n",
        "To see the decision tree, we use the `plot_tree` function from **scikit-learn** and some commands from **matplotlib**. Specifically, we use the `plt.figure()` functions to define the size of the figure and `plt.show()` to display the figure.\n"
      ],
      "id": "cedfd3f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plot_tree(clf, filled=True, rounded=True)\n",
        "plt.show()"
      ],
      "id": "c2549672",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n"
      ],
      "id": "8ab9fcaf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_tree(clf, filled=True, rounded=True)\n",
        "plt.show()"
      ],
      "id": "8d5d7dbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The tree in the predictor space\n",
        "\n",
        "</br>\n"
      ],
      "id": "b27d60b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(9.3, 5.3))\n",
        "sns.scatterplot(\n",
        "    data=training_dataset,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='counterfeit',\n",
        "    palette={0: 'blue', 1: 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.25, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.41, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=10.25, ymin=0.41, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=11.3, ymin=0, ymax=0.41, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e174b03d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimated conditional probabilities\n",
        "\n",
        "</br>\n",
        "\n",
        "After training a classification tree, we can calculate the estimated conditional probability $\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$.\n",
        "\n",
        "To this end, let\n",
        "\n",
        "-   $\\hat{p}_1(\\boldsymbol{x}) = \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$\n",
        "-   $\\hat{p}_0(\\boldsymbol{x}) = 1 - \\hat{p}_1(\\boldsymbol{x})$\n",
        "\n",
        "be the estimated probabilities that $\\boldsymbol{x}$ belongs to class 1 or 0.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "Essentially, we calculate $\\hat{p}_1(\\boldsymbol{x})$ and $\\hat{p}_0(\\boldsymbol{x})$ as follows:\n",
        "\n",
        "1.  Select the region or terminal node where $\\boldsymbol{x}$ belongs.\n",
        "2.  $\\hat{p}_1(\\boldsymbol{x})$ is the proportion of observations in *that* terminal node that belong to class 1. $\\hat{p}_0(\\boldsymbol{x})$ is the proportion of observations that belong to class 0.\n",
        "\n",
        "## Visually\n",
        "\n",
        "Let $(\\hat{p}_0(\\boldsymbol{x}), \\hat{p}_1(\\boldsymbol{x}))$ be the estimated probabilities.\n"
      ],
      "id": "867abbb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(9.3, 5.3))\n",
        "sns.scatterplot(\n",
        "    data=training_dataset,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='counterfeit',\n",
        "    palette={0: 'blue', 1: 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.25, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.41, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=10.25, ymin=0.41, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=11.3, ymin=0, ymax=0.41, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Add region labels\n",
        "plt.text(9.05, 11.5, r\"(0.3, 0.7)\", fontsize=30)\n",
        "plt.text(11.5, 11.5, r\"(0, 1)\", fontsize=30)\n",
        "plt.text(9.05, 7.2, r\"(1, 0)\", fontsize=30)\n",
        "plt.text(11.05, 7.2, r\"(0.7, 0.3)\", fontsize=20, rotation=90)\n",
        "plt.text(11.5, 7.2, r\"(0.3, 0.7)\", fontsize=30)\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "06381f1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimated Bayes Classifier\n",
        "\n",
        "</br>\n",
        "\n",
        "Once we have estimated the conditional probability using the classification tree, we plug it into the Bayes classifier to have our approximated function:\n",
        "\n",
        "$$\\hat{C}(\\boldsymbol{x}) =\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) > 0.5 \\\\\n",
        "      0, & \\text{if}\\ \\hat{p}_1(\\boldsymbol{x}) \\leq 0.5\n",
        "    \\end{cases},$$\n",
        "\n",
        "where $\\hat{p}_1(\\boldsymbol{x})$ depends on the region or terminal node $\\boldsymbol{x}$ falls in.\n",
        "\n",
        "## Simplified decision boundary\n",
        "\n",
        "</br>\n"
      ],
      "id": "3eeac170"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Make sure the feature order matches exactly what was used in clf.fit\n",
        "X = X_train[['Top', 'Bottom']]  \n",
        "\n",
        "\n",
        "# Create decision boundary plot\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    X,\n",
        "    response_method=\"predict\",\n",
        "    xlabel=\"Top\",\n",
        "    ylabel=\"Bottom\",\n",
        "    grid_resolution=200,\n",
        "    cmap=plt.cm.Paired,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "# Overlay actual data points\n",
        "disp.ax_.scatter(\n",
        "    X['Top'],\n",
        "    X['Bottom'],\n",
        "    c=Y_train,\n",
        "    cmap=plt.cm.Paired,\n",
        "    edgecolor='k'\n",
        ")\n",
        "\n",
        "disp.ax_.set_title(\"Decision Boundary of Classification Tree\")\n",
        "plt.show()"
      ],
      "id": "6bf735a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision boundary logistic regression\n",
        "\n",
        "</br>\n"
      ],
      "id": "b81d5eed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Prepare features in the same order: Top (x-axis), Bottom (y-axis)\n",
        "X = X_train[['Top', 'Bottom']]\n",
        "\n",
        "# Train logistic regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, Y_train)\n",
        "\n",
        "# Create decision boundary plot\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    log_reg,\n",
        "    X,\n",
        "    response_method=\"predict\",\n",
        "    xlabel=\"Top\",\n",
        "    ylabel=\"Bottom\",\n",
        "    grid_resolution=200,\n",
        "    cmap=plt.cm.Paired,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "# Overlay actual training data points\n",
        "disp.ax_.scatter(\n",
        "    X['Top'],\n",
        "    X['Bottom'],\n",
        "    c=Y_train,\n",
        "    cmap=plt.cm.Paired,\n",
        "    edgecolor='k'\n",
        ")\n",
        "\n",
        "disp.ax_.set_title(\"Decision Boundary of Logistic Regression\")\n",
        "plt.show()"
      ],
      "id": "946911e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation details\n",
        "\n",
        "-   Categorical predictors with unordered levels $\\{A, B, C\\}$. We order the levels in a specific way (works for binary and regression problems).\n",
        "\n",
        "-   Predictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new \"NA\" level.\n",
        "\n",
        "-   Tertiary or quartary splits. There is not much improvement.\n",
        "\n",
        "-   Diagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability.\n",
        "\n",
        "# Classification Metrics\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "</br>\n",
        "\n",
        "We evaluate a classification tree by classifying observations that were not used for training.\n",
        "\n",
        "That is, we use the classifier to predict categories in the validation data set using only the predictor values from this set.\n",
        "\n",
        "In Python, we use the commands:\n"
      ],
      "id": "1440a440"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Predict classes.\n",
        "predicted_class = clf.predict(X_valid)"
      ],
      "id": "76417d60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "The `predict()` function generates actual classes to which each observation was assigned.\n"
      ],
      "id": "30469854"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "predicted_class"
      ],
      "id": "ef8b3b3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n"
      ],
      "id": "60f3baf9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Ensure feature order matches the training model\n",
        "X_train_plot = X_train[['Top', 'Bottom']]\n",
        "X_valid_plot = X_valid[['Top', 'Bottom']]\n",
        "\n",
        "# Create decision boundary plot\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    X_train_plot,\n",
        "    response_method=\"predict\",\n",
        "    xlabel=\"Top\",\n",
        "    ylabel=\"Bottom\",\n",
        "    grid_resolution=200,\n",
        "    cmap=plt.cm.Paired,\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "# Overlay training points (colored by class)\n",
        "disp.ax_.scatter(\n",
        "    X_train_plot['Top'],\n",
        "    X_train_plot['Bottom'],\n",
        "    c=Y_train,\n",
        "    cmap=plt.cm.Paired,\n",
        "    edgecolor='k'\n",
        ")\n",
        "\n",
        "# Overlay validation points (black)\n",
        "disp.ax_.scatter(\n",
        "    X_valid_plot['Top'],\n",
        "    X_valid_plot['Bottom'],\n",
        "    c='black',\n",
        "    edgecolor='k',\n",
        "    label='Validation'\n",
        ")\n",
        "\n",
        "disp.ax_.set_title(\"Decision Boundary of Classification Tree\")\n",
        "disp.ax_.legend()\n",
        "plt.show()"
      ],
      "id": "f5a8cae7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "The `predict_proba()` function generates the probabilities used by the algorithm to assign the classes.\n"
      ],
      "id": "f5662c2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Predict probabilities.\n",
        "predicted_probability = clf.predict_proba(X_valid)\n",
        "predicted_probability"
      ],
      "id": "2c61c651",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion matrix\n",
        "\n",
        "-   Table to evaluate the performance of a classifier.\n",
        "\n",
        "-   Compares actual values with the predicted values of a classifier.\n",
        "\n",
        "-   Useful for binary and multiclass classification problems.\n",
        "\n",
        "![](images/confusion_matrix.png){fig-align=\"center\"}\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "We calculate the confusion matrix using the homonymous function **scikit-learn**.\n"
      ],
      "id": "faf36ed7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Compute confusion matrix.\n",
        "cm = confusion_matrix(Y_valid, predicted_class)\n",
        "\n",
        "# Show confusion matrix.\n",
        "print(cm)"
      ],
      "id": "6844c4d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "We can display the confusion matrix using the `ConfusionMatrixDisplay()` function.\n"
      ],
      "id": "ab10366d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "id": "37e3a269",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy\n",
        "\n",
        "It is a simple metric for summarizing the information in the confusion matrix. It is the proportion of correct classifications for both classes, out of the total classifications performed.\n",
        "\n",
        "</br>\n",
        "\n",
        "In Python, we calculate accuracy using the **scikit-learn** `accuracy_score()` function.\n"
      ],
      "id": "c7af5663"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "accuracy = accuracy_score(Y_valid, predicted_class)\n",
        "print( round(accuracy, 2) )"
      ],
      "id": "a1c4197f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The higher the accuracy, the better the performance of the classification algorithm.\n",
        "\n",
        "## Pruning the tree\n",
        "\n",
        "In some cases, we can optimize the performance of the tree by pruning it. That is, we collapse two internal (non-terminal) nodes.\n",
        "\n",
        "![](images/clipboard-1949573140.png){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "a3d65b7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "plt.axvline(x=11.3, ymin=0, ymax=0.438, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Add region labels\n",
        "plt.text(8.5, 11.5, r\"$R_1$\", fontsize=30, fontweight='bold')\n",
        "plt.text(8.5, 8.0, r\"$R_2$\", fontsize=30, fontweight='bold')\n",
        "plt.text(10.95, 7.2, r\"$R_3$\", fontsize=20, fontweight='bold')\n",
        "plt.text(11.5, 8.0, r\"$R_4$\", fontsize=30, fontweight='bold')\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d4b1d45a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_plot.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "4fe0df66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| \n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create the scatter plot using seaborn for discrete color mapping\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    data=bank_data,\n",
        "    x='Top',\n",
        "    y='Bottom',\n",
        "    hue='Status',\n",
        "    palette={'genuine': 'blue', 'counterfeit': 'orange'},\n",
        "    s=15,\n",
        "    edgecolor=None,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Add decision boundaries\n",
        "plt.axhline(y=9.55, color='black', linewidth=3)     # Horizontal line\n",
        "plt.axvline(x=10.95, ymin=0, ymax=0.438, color='black', linewidth=3)  # Left vertical\n",
        "#plt.axvline(x=11.3, ymin=0, ymax=0.438, color='black', linewidth=3)  # Right vertical\n",
        "\n",
        "# Add region labels\n",
        "plt.text(8.5, 11.5, r\"$R_1$\", fontsize=30, fontweight='bold')\n",
        "plt.text(8.5, 8.0, r\"$R_2$\", fontsize=30, fontweight='bold')\n",
        "plt.text(10.95, 7.2, r\"$R_3$\", fontsize=20, fontweight='bold')\n",
        "\n",
        "# Axis labels\n",
        "plt.xlabel(\"Top\")\n",
        "plt.ylabel(\"Bottom\")\n",
        "\n",
        "# Clean layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "efc39e78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/tree_plot_simple.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "To prune a tree, we use an advanced algorithm to measure the contribution of the tree's branches.\n",
        "\n",
        "This algorithm minimizes the following function:\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "$$\\text{Missclassification rate of tree} + \\alpha (\\text{\\# terminal nodes}),$$\n",
        ":::\n",
        "\n",
        "where $\\alpha$ is a tuning parameter that **places greater weight on the number of tree nodes** (or size).\n",
        "\n",
        "-   Large values of $\\alpha$ result in small trees with few nodes.\n",
        "\n",
        "-   Small values of $\\alpha$ result in large trees with many nodes.\n",
        "\n",
        "## Apply penalty for large trees\n",
        "\n",
        "Now, let's illustrate the pruning technique to find a small decision tree that performs well. To do this, we will train several decision trees using different values of $\\alpha$, which weighs the contribution of the tree branches.\n",
        "\n",
        "</br>\n",
        "\n",
        "In Python, this is achieved using the `cost_complexity_pruning_path()` function with the following syntax.\n"
      ],
      "id": "8cf4601f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "path = clf.cost_complexity_pruning_path(X_train, Y_train)\n",
        "ccp_alphas = path.ccp_alphas"
      ],
      "id": "0b49d68a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "The `ccp_alphas` object contains the different values of $\\alpha$ used. To train a decision tree using different $\\alpha$ values, we use the following code that iterates over the values contained in `ccp_alphas`.\n"
      ],
      "id": "d4347978"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf_alpha = DecisionTreeClassifier(min_samples_leaf= 5, \n",
        "                                       ccp_alpha=ccp_alpha, random_state=507134)\n",
        "    clf_alpha.fit(X_train, Y_train)\n",
        "    clfs.append(clf_alpha)"
      ],
      "id": "1c61d63d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Now, for each of those trees contained in `clfs`, we evaluate the performance in terms of accuracy for the training and validation data.\n"
      ],
      "id": "2655e991"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "train_scores = [clf.score(X_train, Y_train) for clf in clfs]\n",
        "validation_scores = [clf.score(X_valid, Y_valid) for clf in clfs]"
      ],
      "id": "a67087e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `.score()` function computes the accuracy of a classification tree.\n",
        "\n",
        "## \n",
        "\n",
        "We visualize the results using the following plot.\n"
      ],
      "id": "c8e3e833"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center \n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and validation data\")\n",
        "ax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "id": "077ced25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing the best tree\n",
        "\n",
        "</br>\n",
        "\n",
        "We can see that the best $\\alpha$ value for the validation dataset is 0.\n",
        "\n",
        "To train our new reduced tree, we use the `DecisionTreeClassifier()` function again with the `ccp_alpha` parameter set to the best $\\alpha$ value. The object containing this new tree is `clf_simple`.\n"
      ],
      "id": "fc1494e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# We tell Python that we want a classification tree\n",
        "clf_simple = DecisionTreeClassifier(ccp_alpha=0, min_samples_leaf= 5)\n",
        "\n",
        "# We train the classification tree using the training data.\n",
        "clf_simple.fit(X_train, Y_train)"
      ],
      "id": "3ca1bcfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Once this is done, we can visualize the small tree using the `plot_tree()` function.\n"
      ],
      "id": "0ceecbe9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plot_tree(clf_simple, filled=True, rounded=True)\n",
        "plt.show()"
      ],
      "id": "d17c6e69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "The accuracy of the pruned tree is:\n"
      ],
      "id": "c7bfb784"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "single_tree_Y_pred = clf_simple.predict(X_valid)\n",
        "accuracy = accuracy_score(Y_valid, single_tree_Y_pred)\n",
        "print( round(accuracy, 2) )"
      ],
      "id": "cd307686",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comments on accuracy\n",
        "\n",
        "</br>\n",
        "\n",
        "-   Accuracy is easy to calculate and interpret.\n",
        "\n",
        "-   It works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\n",
        "\n",
        "-   However, there are situations in which identifying the target class is more important than the reference class.\n",
        "\n",
        "-   For example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading.\n",
        "\n",
        "## An example\n",
        "\n",
        "</br>\n",
        "\n",
        "-   Let's say we want to create a classifier that tells us whether a mobile phone company's customer will churn next month.\n",
        "\n",
        "-   Customers who churn significantly decrease the company's revenue. That's why it's important to retain these customers.\n",
        "\n",
        "-   To retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\n",
        "\n",
        "-   Ideally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "-   In other words, we want to avoid making wrong decisions about customers who will churn.\n",
        "\n",
        "-   Wrong decisions about loyal customers aren't as relevant.\n",
        "\n",
        "-   Because if we classify a loyal customer as one who will churn, the customer will get a good deal. They'll probably pay less but stay anyway.\n",
        "\n",
        "## Another example\n",
        "\n",
        "-   Another example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\n",
        "\n",
        "-   The classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\n",
        "\n",
        "-   A healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease.\n",
        "\n",
        "## Classification-specific metrics\n",
        "\n",
        "</br></br>\n",
        "\n",
        "To overcome this limitation of accuracy, there are several class-specific metrics. The most popular are:\n",
        "\n",
        "-   [**Sensitivity**]{style=\"color:darkblue;\"} or *recall*\n",
        "\n",
        "-   [**Precision**]{style=\"color:darkgreen;\"}\n",
        "\n",
        "-   **Type I error**\n",
        "\n",
        "These metrics are calculated from the confusion matrix.\n",
        "\n",
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "[**Sensitivity**]{style=\"color:darkblue;\"} or *recall* = OO/(OO + OR) “How many records of the target class did we predict correctly?”\n",
        "\n",
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "[**Precision**]{style=\"color:darkgreen;\"} = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "In Python, we compute sensitivity and precision using the following commands:\n"
      ],
      "id": "8616e686"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "recall = recall_score(Y_valid, predicted_class)\n",
        "print( round(recall, 2) )"
      ],
      "id": "f7d27be8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n"
      ],
      "id": "b65bd3c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "precision = precision_score(Y_valid, predicted_class)\n",
        "print( round(precision, 2) )"
      ],
      "id": "db0ca691",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "**Type I error** = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "Unfortunately, there is no simple command to calculate the type-I error in **sklearn**. To overcome this issue, we must calculate it manually.\n"
      ],
      "id": "a9db305f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Confusion matrix to compute Type-I error\n",
        "tn, fp, fn, tp = confusion_matrix(Y_valid, predicted_class).ravel()\n",
        "\n",
        "# Type-I error rate = False Positive Rate = FP / (FP + TN)\n",
        "type_I_error = fp / (fp + tn)\n",
        "print( round(type_I_error, 2) )"
      ],
      "id": "311d427d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "</br>\n",
        "\n",
        "-   There is generally a trade-off between sensitivity and Type I error.\n",
        "\n",
        "-   Intuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\n",
        "\n",
        "-   Possible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error.\n",
        "\n",
        "## Disadvantages of decision trees\n",
        "\n",
        "-   Decision trees have high variance. A small change in the training data can result in a very different tree.\n",
        "\n",
        "-   It has trouble identifying simple data structures.\n",
        "\n",
        "![](images/clipboard-3265772983.png){fig-align=\"center\"}\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN2004B/)"
      ],
      "id": "e5af9642"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
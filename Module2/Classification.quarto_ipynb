{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Classification Methods\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN1002b_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1. Introduction\n",
        "2. Classification and Regression Trees (CART)\n",
        "3. Classification Algorithm Metrics\n",
        "4. *K* Nearest Neighbors\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "Before we start, let's import the data science libraries into Python.\n"
      ],
      "id": "badf1b18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "19d55d5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use specific functions from the **pandas**, **matplotlib**, and **sklearn** libraries in Python.\n",
        "\n",
        "## Main Data Science Problems\n",
        "\n",
        "</br>\n",
        "\n",
        "[**Regression Problems**]{style=\"color:green;\"}. The response is numerical. For example, a person's income, the value of a house, or a patient's blood pressure.\n",
        "\n",
        "[**Classification Problems**]{style=\"color:blue;\"}. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\n",
        "\n",
        "The predictors ($\\boldsymbol{X}$) can be *numerical* or *categorical*.\n",
        "\n",
        "## Main Data Science Problems\n",
        "\n",
        "</br>\n",
        "\n",
        "[**Regression Problems**. The response is numerical. For example, a person's income, the value of a house, or a patient's blood pressure.]{style=\"color:gray;\"}\n",
        "\n",
        "[**Classification Problems**]{style=\"color:blue;\"}. The response is categorical and involves K different categories. For example, the brand of a product purchased (A, B, C) or whether a person defaults on a debt (yes or no).\n",
        "\n",
        "The predictors ($\\boldsymbol{X}$) *numerical* or *categorical*.\n",
        "\n",
        "## Terminology\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Explanatory variables or predictors:\n",
        "\n",
        "-   $X$ represents an explanatory variable or predictor.\n",
        "-   $\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)$ represents a collection of $p$ predictors.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "[Response]{style=\"text-decoration: underline;\"}:\n",
        "\n",
        "::: incremental\n",
        "- $Y$ is a [**categorical variable**]{style=\"color:darkgreen;\"} that takes [**2 categories**]{style=\"color:darkgreen;\"} or [**classes**]{style=\"color:darkgreen;\"}.\n",
        "\n",
        "- For example, $Y$ can take [0]{style=\"color:darkgreen;\"} or [1]{style=\"color:darkgreen;\"}, [A]{style=\"color:darkgreen;\"} or [B]{style=\"color:darkgreen;\"}, [no]{style=\"color:darkgreen;\"} or [yes]{style=\"color:darkgreen;\"}, [spam]{style=\"color:darkgreen;\"} or [no spam]{style=\"color:darkgreen;\"}.\n",
        "\n",
        "- When classes are strings, they are usually encoded as 0 and 1.\n",
        "  - The **target class** is the one for which $Y = 1$. \n",
        "  - The **reference class** is the one for which $Y = 0$.\n",
        ":::\n",
        "\n",
        "## Classification Algorithms\n",
        "\n",
        "</br>\n",
        "\n",
        "Classification algorithms use predictor values [to predict the class]{style=\"color:blue;\"} of the response (target or reference).\n",
        "\n",
        "</br>\n",
        "\n",
        "That is, for an unseen record, they use predictor values to predict whether the record belongs to the target class or not.\n",
        "\n",
        "</br>\n",
        "\n",
        "Technically, [**they predict the probability**]{style=\"color:purple;\"} that the record belongs to the target class.\n",
        "\n",
        "## Classification Algorithms\n",
        "\n",
        "</br></br>\n",
        "\n",
        "[**Goal**]{style=\"color:darkgreen;\"}: Develop a function $C(\\boldsymbol{X})$ for predicting $Y = \\{0, 1\\}$ from $\\boldsymbol{X}$.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "To achieve this goal, most algorithms consider functions $C(\\boldsymbol{X})$ that [**predict the probability**]{style=\"color:brown;\"} that $Y$ takes the value of 1.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "A probability for each class can be very useful for gauging the model’s confidence about the predicted classification.\n",
        "\n",
        "## Example 1\n",
        "\n",
        "Consider a spam filter where $Y$ is the email type.\n",
        "\n",
        "- The target class is spam. In this case, $Y=1$.\n",
        "- The reference class is not spam. In this case, $Y=0$.\n",
        "\n",
        ". . .\n",
        "\n",
        "![](images/spam.png){fig-align=\"center\" width=\"556\" height=\"178\"}\n",
        "\n",
        ". . .\n",
        "\n",
        "Both emails would be classified as spam. However, we would have greater confidence in our classification for the second email.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Technically, $C(\\boldsymbol{X})$ works with the *conditional probability*:\n",
        "\n",
        "$$P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$$\n",
        "\n",
        "In words, this is the probability that $Y$ takes a value of 1 [**given that**]{style=\"color:brown;\"} the predictors $\\boldsymbol{X}$ have taken the values $\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)$.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "The conditional probability that $Y$ takes the value of 0 is\n",
        "\n",
        "$$P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).$$\n",
        "\n",
        "## Bayes Classifier\n",
        "\n",
        "</br>\n",
        "\n",
        "It turns out that, if we know the true structure of $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$, we can build a good classification function called the [**Bayes classifier**]{style=\"color:darkblue;\"}:\n",
        "\n",
        "$$C(\\boldsymbol{X}) =\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) > 0.5 \\\\\n",
        "      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n",
        "    \\end{cases}.$$\n",
        "\n",
        "This function classifies to the most probable class using the conditional distribution $P(Y | \\boldsymbol{X} = \\boldsymbol{x})$.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "[HOWEVER, we don’t (and will never) know the true form of $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$!]{style=\"color:red;\"}\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "To overcome this issue, we several standard solutions:\n",
        "\n",
        "::: incremental\n",
        "-   [**Logistic Regression**]{style=\"color:brown;\"}: Impose an structure on $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$. This was covered in IN1002B.\n",
        "-   [**Classification Trees**]{style=\"color:darkblue;\"}: Estimate $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ directly. What we will cover today.\n",
        "-   [**_K_-Nearest Neighbours**]{style=\"color:darkgreen;\"}: Estimate $P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})$ directly. (Optional).\n",
        ":::\n",
        "\n",
        "## Two datasets\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The application of data science models needs two data sets:\n",
        "\n",
        "::: incremental\n",
        "-   [**Training data**]{style=\"color:blue;\"} is data that we use to train or construct the estimated function $\\hat{f}(\\boldsymbol{X})$.\n",
        "\n",
        "-   [**Test data**]{style=\"color:green;\"} is data that we use to evaluate the predictive performance of $\\hat{f}(\\boldsymbol{X})$ only.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/training.png){width=\"256\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "</br>\n",
        "\n",
        "A random sample of $n$ observations.\n",
        "\n",
        "Use it to **construct** $\\hat{f}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/test.png){width=\"262\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "Another random sample of $n_t$ observations, which is independent of the training data.\n",
        "\n",
        "Use it to **evaluate** $\\hat{f}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Validation Dataset\n",
        "\n",
        "In many practical situations, a test dataset is not available. To overcome this issue, we use a [**validation dataset**]{style=\"color:orange;\"}.\n",
        "\n",
        "![](images/validation.png){fig-align=\"center\" width=\"645\"}\n",
        "\n",
        ". . .\n",
        "\n",
        "**Idea**: Apply model to your [**validation dataset**]{style=\"color:orange;\"} to mimic what will happen when you apply it to test dataset.\n",
        "\n",
        "## Example 1\n",
        "\n",
        "</br>\n",
        "\n",
        "The \"BostonHousing.xlsx\" contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\n",
        "\n",
        "The **goal** is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\n",
        "\n",
        "The [response]{style=\"color:darkred;\"} is the median value of owner-occupied homes in \\$1000s, contained in the column `MEDV`.\n",
        "\n",
        "## The predictors\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}\n",
        "-   `CRIM`: per capita crime rate by town.\n",
        "-   `ZN`: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "-   `INDUS`: proportion of non-retail business acres per town.\n",
        "-   `CHAS`: Charles River ('Yes' if tract bounds river; 'No' otherwise).\n",
        "-   `NOX`: nitrogen oxides concentration (parts per 10 million).\n",
        "-   `RM`: average number of rooms per dwelling.\n",
        "-   `AGE`: proportion of owner-occupied units built prior to 1940.\n",
        "-   `DIS`: weighted mean of distances to five Boston employment centers\n",
        "-   `RAD`: index of accessibility to radial highways ('Low', 'Medium', 'High').\n",
        "-   `TAX`: full-value property-tax rate per \\$10,000.\n",
        "-   `PTRATIO`: pupil-teacher ratio by town.\n",
        "-   `LSTAT`: lower status of the population (percent).\n",
        ":::\n",
        "\n",
        "## Read the dataset\n",
        "\n",
        "</br>\n",
        "\n",
        "We read the dataset and set the variable `CHAS` and `RAD` as categorical.\n"
      ],
      "id": "ad35e71d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "Boston_data = pd.read_excel('BostonHousing.xlsx')\n",
        "\n",
        "Boston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\n",
        "Boston_data['RAD'] = pd.Categorical(Boston_data['RAD'], \n",
        "                                      categories=[\"Low\", \"Medium\", \"High\"], \n",
        "                                      ordered=True)"
      ],
      "id": "d8e2925d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n"
      ],
      "id": "c93128a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "Boston_data.head()"
      ],
      "id": "bbc8ccf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How do we generate validation data?\n",
        "\n",
        "We split the current dataset into a training and a validation dataset. To this end, we use the function `train_test_split()` from **scikit-learn**.\n",
        "\n",
        "</br>\n",
        "\n",
        "The function has three main inputs:\n",
        "\n",
        "-   A pandas dataframe with the predictor columns only.\n",
        "-   A pandas dataframe with the response column only.\n",
        "-   The parameter `test_size` which sets the portion of the dataset that will go to the validation set.\n",
        "\n",
        "## Create the predictor matrix\n",
        "\n",
        "We use the function `.drop()` from **pandas**. This function drops one or more columns from a data frame. Let's drop the response column `MEDV` and store the result in `X_full`.\n"
      ],
      "id": "214217c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of predictors.\n",
        "X_full = Boston_data.drop(columns = ['MEDV']) \n",
        "X_full.head(4)"
      ],
      "id": "09d79e88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the response column\n",
        "\n",
        "We use the function `.filter()` from **pandas** to extract the column `MEDV` from the data frame. We store the result in `Y_full`.\n"
      ],
      "id": "993d590e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of responses.\n",
        "Y_full = Boston_data.filter(['MEDV'])\n",
        "Y_full.head(4)"
      ],
      "id": "6f5f17fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's partition the dataset\n",
        "\n",
        "</br>\n"
      ],
      "id": "8ef753b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Split the dataset into training and validation.\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n",
        "                                                      test_size = 0.3)"
      ],
      "id": "c875b5f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The function makes a clever partition of the data using the *empirical* distribution of the response.\n",
        "\n",
        "-   Technically, it splits the data so that the distribution of the response under the training and validation sets is similar.\n",
        "\n",
        "-   Usually, the proportion of the dataset that goes to the validation set is 20% or 30%.\n",
        "\n",
        "## \n",
        "\n",
        "The predictors and response in the training dataset are in the objects `X_train` and `Y_train`, respectively. We compile these objects into a single dataset using the function `.concat()` from **pandas**. The argument `axis = 1` tells `.concat()` to concatenate the datasets by their rows.\n"
      ],
      "id": "b6d8b78f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "training_dataset = pd.concat([X_train, Y_train], axis = 1)\n",
        "training_dataset.head(4)"
      ],
      "id": "ae5733b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Equivalently, the predictors and response in the validation dataset are in the objects `X_valid` and `Y_valid`, respectively.\n"
      ],
      "id": "e2b45cb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "validation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\n",
        "validation_dataset.head()"
      ],
      "id": "639779b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Work on your training dataset\n",
        "\n",
        "After we have partitioned the data, we **work on the** [**training data**]{style=\"color:blue;\"} to develop our predictive pipeline.\n",
        "\n",
        "The pipeline has two main steps:\n",
        "\n",
        "1.  Data preprocessing.\n",
        "2.  Model development.\n",
        "\n",
        "We will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\n",
        "\n",
        "Note that all preprocessing techniques will also be applied to the [**validation dataset**]{style=\"color:orange;\"} and [**test dataset**]{style=\"color:green;\"} to prepare it for your model!\n",
        "\n",
        "\n",
        "# Classification and Regression Trees (CART)\n",
        "\n",
        "## Decision Tree\n",
        "\n",
        "It is a supervised learning algorithm that predicts or classifies observations using a hierarchical tree structure.\n",
        "\n",
        "- Simple and useful for interpretation.\n",
        "\n",
        "- Can handle numerical and categorical predictors and responses.\n",
        "\n",
        "- Computationally efficient.\n",
        "\n",
        "- Nonparametric technique.\n",
        "\n",
        "## Example 2: Identifying Counterfeit Banknotes\n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/clipboard-270396609.png)\n",
        "\n",
        "Dataset\n",
        "\n",
        "The data is located in the file \"banknotes.xlsx\".\n"
      ],
      "id": "ee491523"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "bank_data = pd.read_excel(\"banknotes.xlsx\")\n",
        "# Set response variable as categorical.\n",
        "bank_data['Status'] = pd.Categorical(bank_data['Status'])\n",
        "bank_data.head()"
      ],
      "id": "82f5a811",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Training Data\n",
        "\n",
        "We split the current dataset into two datasets: training and validation. To do this, we use the scikit-learn `train_test_split()` function.\n"
      ],
      "id": "0e6a0121"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of predictors.\n",
        "X_full = bank_data.drop(columns = ['Status'])\n",
        "\n",
        "# Set full matrix of responses.\n",
        "Y_full = bank_data['Status']\n",
        "\n",
        "# Split the dataset.\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_full, Y_full, \n",
        "                                                    test_size=0.3)"
      ],
      "id": "86c7f906",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `test_size` parameter sets the portion of the dataset that will go into the validation set.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "- The function intelligently partitions the data using the *empirical* distribution of the response.\n",
        "\n",
        "- Technically, it splits the data so that the response distribution in the training and validation sets is similar.\n",
        "\n",
        "- Typically, the proportion of the data set allocated to the test set is 20% or 30%.\n",
        "\n",
        "- Later, we will use the [**validation data set**]{style=\"color:orange;\"} to evaluate the classification performance of the estimated logistic regression model for classifying unobserved data.\n",
        "\n",
        "## Basic idea of a decision tree\n",
        "\n",
        "Stratify or segment the prediction space into several simpler regions.\n",
        "\n",
        "![](images/Screenshot%202025-07-28%20at%2011.47.35%20a.m..png){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.012.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.013.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## How do you build a decision tree?\n",
        "\n",
        "Building decision trees involves two main procedures.\n",
        "\n",
        "1. [Grow a large tree.]{style=\"color:darkblue;\"}\n",
        "\n",
        "2. [Pruning the tree to prevent overfitting.]{style=\"color:darkblue;\"}\n",
        "\n",
        "After building a “good” tree, we can predict new observations that are not in the data set we used to build it.\n",
        "\n",
        "## How do we grow a tree?\n",
        "\n",
        "**Using the CART algorithm!**\n",
        "\n",
        "The algorithm uses a recursive binary splitting strategy that builds the tree using a greedy top-down approach.\n",
        "\n",
        "Basically, at a given node, it considers all variables and all possible splits of that variable. Then, for classification, it chooses the best variable and splits it that **minimizes** the so-called [***impurity***]{style=\"color:purple;\"}.\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.016.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.017.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.018.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.019.jpeg)\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.020.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.021.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.022.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.023.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.024.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "We repeat the partitioning process until the terminal nodes have no less than 5 observations.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.025.jpeg){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## What is impurity?\n",
        "\n",
        "Node impurity refers to the homogeneity of the response classes at that node.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/Impurity1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/Impurity2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "[*The CART algorithm minimizes impurity between tree nodes.*]{style=\"color:darkgray;\"}\n",
        "\n",
        "# ¿Cómo medimos la impureza?\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "There are three different metrics for impurity:\n",
        "\n",
        "- Risk of misclassification.\n",
        "\n",
        "- Cross entropy.\n",
        "\n",
        "- Gini impurity index.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/Metrics1.png){fig-align=\"center\"} ![](images/Metrics2.png){fig-align=\"center\"} [Proportion of elements in a class]{.smallcaps}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Pruning the Tree\n",
        "\n",
        "To avoid overfitting, we pruned some of the tree's branches. More specifically, we collapsed two internal (non-terminal) nodes.\n",
        "\n",
        "![](images/clipboard-1949573140.png)\n",
        "\n",
        "## \n",
        "\n",
        "![](images/Modulo%202%20-%20Algoritmos%20de%20Clasificacion.029.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "To prune a tree, we use an advanced algorithm to measure the contribution of the tree's branches.\n",
        "\n",
        "The algorithm has a tuning parameter called $\\alpha$, which **places greater weight on the number of tree nodes** (or size):\n",
        "\n",
        "- Large values of $\\alpha$ result in small trees with few nodes.\n",
        "\n",
        "- Small values of $\\alpha$ result in large trees with many nodes.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "- Categorical predictors with unordered levels $\\{A, B, C\\}$. We order the levels in a specific way (works for binary and regression problems).\n",
        "\n",
        "- Predictors with missing values. For quantitative predictors, we use multiple imputation. For categorical predictors, we create a new \"NA\" level.\n",
        "\n",
        "- Tertiary or quartary splits. There is not much improvement.\n",
        "\n",
        "- Diagonal splits (using a linear combination for partitioning). These can lead to improvement, but they impair interpretability.\n",
        "\n",
        "# Python Example\n",
        "\n",
        "The “AdultReduced.jmp” data comes from the UCI Machine Learning Repository and is derived from US Census records.\n",
        "\n",
        "In this data, the goal is to predict whether a person's income was high (defined in 1994 as more than $50,000) or low.\n",
        "\n",
        "Predictors include education level, job type (e.g., never worked and local government), capital gains/losses, hours worked per week, country of origin, etc.\n",
        "\n",
        "The data contains 7,508 records.\n",
        "\n",
        "## Disadvantage of Decision Trees\n",
        "\n",
        "- Decision trees have high variance. A small change in the training data can result in a very different tree.\n",
        "\n",
        "- It has trouble identifying simple data structures.\n",
        "\n",
        "![](images/clipboard-3265772983.png)\n",
        "\n",
        "# Classification Algorithm Metrics\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "</br>\n",
        "\n",
        "We evaluate a logistic regression classifier by classifying observations that were not used for training or estimation.\n",
        "\n",
        "That is, we use the classifier to predict categories in the test data set using only the predictor values from this set.\n",
        "\n",
        "In Python, we use the commands:\n"
      ],
      "id": "2ac6d8d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Remove problematic predictor from the test set.\n",
        "#X_val = X_val.drop(columns = ['Right'])\n",
        "\n",
        "# Add constant to the predictor matrix from the test set.\n",
        "#X_val = sm.add_constant(X_val)\n",
        "\n",
        "# Predict probabilities.\n",
        "#predicted_probability = logit_model.predict(X_val)"
      ],
      "id": "90a1aafb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "The `predict()` function generates [**probabilities**]{style=\"color:brown;\"} instead of the actual classes.\n"
      ],
      "id": "747d27e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "#predicted_probability.head()"
      ],
      "id": "a574e51c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the probabilities that a bill is \"counterfeit\" based on its characteristics (predictor values).\n",
        "\n",
        "To convert the probabilities into real-world classes, we round them:\n"
      ],
      "id": "117b0a4e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#predicted_classes = round(predicted_probability).astype('int')"
      ],
      "id": "b0847c79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n"
      ],
      "id": "6a87d140"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#predicted_classes.head()"
      ],
      "id": "0811c1b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Observations with probabilities greater than 0.5 are classified as \"false.\"\n",
        "- Observations with probabilities less than 0.5 are classified as \"genuine.\"\n",
        "\n",
        "Now, we compare the predictions with the actual categories in the [**validation dataset**]{style=\"color:orange;\"}. [A good logistic regression model shows good agreement between its predictions and the actual categories.]{style=\"color:darkblue;\"}\n",
        "\n",
        "## Confusion Matrix\n",
        "\n",
        "- Table used to evaluate the performance of a classifier.\n",
        "\n",
        "- Compares actual values with the predicted values of a classifier.\n",
        "\n",
        "- Useful for binary and multiclass classification problems.\n",
        "\n",
        "![](images/confusion_matrix.png){fig-align=\"center\"}\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "We calculate the confusion matrix using the homonymous function **scikit-learn**.\n"
      ],
      "id": "46c701e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Create dummy variables for test set.\n",
        "#Y_dummies = pd.get_dummies(Y_val, dtype = 'int')\n",
        "\n",
        "# Select target variable from test set.\n",
        "#Y_target_test = Y_dummies['counterfeit']\n",
        "\n",
        "# Compute confusion matrix.\n",
        "#cm = confusion_matrix(Y_target_test, predicted_classes)\n",
        "\n",
        "# Show confusion matrix.\n",
        "#print(cm)"
      ],
      "id": "9f098e46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "We can display the confusion matrix using the `ConfusionMatrixDisplay()` function.\n"
      ],
      "id": "53eeeaaa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "#ConfusionMatrixDisplay(cm).plot()"
      ],
      "id": "8b2097c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy\n",
        "\n",
        "A simple metric for summarizing the information in the confusion matrix is **accuracy**. It is the proportion of correct classifications for both classes, out of the total classifications performed.\n",
        "\n",
        "In Python, we calculate accuracy using the **scikit-learn** `accuracy_score()` function.\n"
      ],
      "id": "eb00ac62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "#accuracy = accuracy_score(Y_target_test, predicted_classes)\n",
        "#print( round(accuracy, 2) )"
      ],
      "id": "5d63e320",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n",
        "\n",
        "The higher the accuracy, the better the performance of the classifier.\n",
        "\n",
        "## Observaciones\n",
        "\n",
        "</br>\n",
        "\n",
        "- Accuracy is easy to calculate and interpret.\n",
        "\n",
        "- It works well when the data set has a balanced class distribution (i.e., cases 1 and 0 are approximately equal).\n",
        "\n",
        "- However, there are situations in which identifying the target class is more important than the reference class.\n",
        "\n",
        "- For example, it is not ideal for unbalanced data sets. When one class is much more frequent than the other, accuracy can be misleading.\n",
        "\n",
        "## An example\n",
        "\n",
        "- Let's say we want to create a classifier that tells us whether a mobile phone company's customer will churn next month.\n",
        "\n",
        "- Customers who churn significantly decrease the company's revenue. That's why it's important to retain these customers.\n",
        "\n",
        "- To retain that customer, the company will send them a text message with an offer for a low-cost mobile plan.\n",
        "\n",
        "- Ideally, our classifier correctly identifies customers who will churn, so they get the offer and, hopefully, stay.\n",
        "\n",
        "##\n",
        "\n",
        "- In other words, we want to avoid making wrong decisions about customers who will churn.\n",
        "\n",
        "- Wrong decisions about loyal customers aren't as relevant.\n",
        "\n",
        "- Because if we classify a loyal customer as one who will churn, the customer will get a good deal. They'll probably pay less but stay anyway.\n",
        "\n",
        "## Another example\n",
        "\n",
        "- Another example is developing an algorithm (classifier) that can quickly identify patients who may have a rare disease and need a more extensive and expensive medical evaluation.\n",
        "\n",
        "- The classifier must make correct decisions about patients with the rare disease, so they can be evaluated and eventually treated.\n",
        "\n",
        "- A healthy patient who is misclassified with the disease will only incur a few extra dollars to pay for the next test, only to discover that the patient does not have the disease.\n",
        "\n",
        "## Classification-Specific Metrics\n",
        "\n",
        "To overcome this limitation of accuracy and error rate, there are several class-specific metrics. The most popular are:\n",
        "\n",
        "- [**Sensitivity**]{style=\"color:darkblue;\"} or *recall*\n",
        "\n",
        "- [**Precision**]{style=\"color:darkgreen;\"}\n",
        "\n",
        "- **Type I error**\n",
        "\n",
        "These metrics are calculated from the confusion matrix.\n",
        "\n",
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "[**Sensitivity**]{style=\"color:darkblue;\"} or *recall* = OO/(OO + OR) “How many records of the target class did we predict correctly?”\n",
        "\n",
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "[**Precision**]{style=\"color:darkgreen;\"} = OO/(OO + RO) How many of the records we predicted as target class were classified correctly?\n",
        "\n",
        "## \n",
        "\n",
        "![](images/classspecific_metrics.png){fig-align=\"center\"}\n",
        "\n",
        "**Type I error** = RO/(RO + RR) “How many of the reference records did we incorrectly predict as targets?”\n",
        "\n",
        "## Discussion\n",
        "\n",
        "- There is generally a trade-off between sensitivity and Type I error.\n",
        "\n",
        "- Intuitively, increasing the sensitivity of a classifier is likely to increase Type I error, because more observations are predicted as positive.\n",
        "\n",
        "- Possible trade-offs between sensitivity and Type I error may be appropriate when there are different penalties or costs associated with each type of error.\n",
        "\n",
        "## Example\n",
        "\n",
        "Assuming the target class is \"large\"\n",
        "\n",
        "- Sensitivity = 566/(566 + 214) = 0.726\n",
        "\n",
        "- Accuracy = 566/(566 + 156) = 0.783\n",
        "\n",
        "- Type 1 Error = 156/(156 + 655) = 0.192\n",
        "\n",
        "## Activity 2.1: Classification and Metrics (cooperative mode)\n",
        "\n",
        "Pair with a partner.\n",
        "\n",
        "Using the data in the \"weight-height.csv\" table, apply the CART procedure to build a decision tree useful for predicting a person's sex based on their weight and height.\n",
        "\n",
        "In this example, the predictor variables are continuous, and the predictor variable is binary.\n",
        "\n",
        "##\n",
        "\n",
        "Interpret the Precision, Accuracy, Sensitivity, and Type 1 Error values for the validation set. If the software doesn't report them, perform the calculations using the confusion matrix. Use \"Female\" as the target class.\n",
        "\n",
        "Discuss the effectiveness of the resulting model.\n",
        "\n",
        "# *K* nearest neighbors\n",
        "\n",
        "This is a supervised learning algorithm that uses proximity to make classifications or predictions about the clustering of a single data point.\n",
        "\n",
        "**Basic idea**: Predict a new observation using the *K* closest observations in the training dataset.\n",
        "\n",
        "To predict the response for a new observation, *K*-NN uses the *K* nearest neighbors (observations) in [***terms of the predictors!***]{style=\"color:aqua;\"}\n",
        "\n",
        "The predicted response for the new observation is the most common response among the *K* nearest neighbors.\n",
        "\n",
        "## The algorithm has 3 steps:\n",
        "\n",
        "1. Choose the number of nearest neighbors (*K*).\n",
        "\n",
        "2. For a new observation, find the *K* closest observations in the training data (ignoring the response).\n",
        "\n",
        "3. For the new observation, the algorithm predicts the value of the most common response among the *K* nearest observations.\n",
        "\n",
        "##\n",
        "\n",
        "Suppose we have two groups: the red group and the green group. The number line shows the value of a variable for our training data.\n",
        "\n",
        "A new observation arrives, and we don't know which group it belongs to.\n",
        "\n",
        "![](images/ball_example.png){fig-align=\"center\"}\n",
        "\n",
        "If we had chosen $K=3$, then the three nearest neighbors would vote on which group the new observation belongs to.\n",
        "\n",
        "##\n",
        "\n",
        "Using $K = 3$, that's 2 votes for \"genuine\" and 2 for \"fake.\" So we classify it as \"genius.\"\n",
        "\n",
        "![](images/ball_example2.png){fig-align=\"center\"}\n",
        "\n",
        "Closeness is based on Euclidean distance.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "**Ties**\n",
        "\n",
        "- If there are more than *K* nearest neighbors, include them all.\n",
        "\n",
        "- If there is a tie in the vote, set a rule to break the tie. For example, randomly select the class.\n",
        "\n",
        "##\n",
        "\n",
        "***KNN uses the Euclidean distance between points***. So it ignores units.\n",
        "\n",
        "- Example: two predictors: height in cm and arm span in feet. Compare two people: (152.4, 1.52) and (182.88, 1.85).\n",
        "\n",
        "- These people are separated by 30.48 units of distance in the first variable, but only by 0.33 units in the second.\n",
        "\n",
        "- Therefore, the first predictor plays a much more important role in classification and can bias the results to the point where the second variable becomes useless.\n",
        "\n",
        "##\n",
        "\n",
        "As a first step, we must transform the predictors so that they have the same units!\n",
        "\n",
        "This requires a predictor standardization process, which is done in Python.\n",
        "\n",
        "## Standardization\n",
        "\n",
        "</br>\n",
        "\n",
        "Standardization refers to *centering* and *scaling* each numerical predictor individually. This places all predictors on the same scale.\n",
        "\n",
        "To **center** a predictor variable, the mean value of the predictor is subtracted from all values.\n",
        "\n",
        "Therefore, the centered predictor has a mean of zero (i.e., its average value is zero).\n",
        "\n",
        "##\n",
        "\n",
        "</br>\n",
        "\n",
        "To **scale** a predictor, each of its values is divided by its standard deviation.\n",
        "\n",
        "When scaling the data, the values have a common standard deviation of one.\n",
        "\n",
        "In mathematical terms, we standardize a predictor as:\n",
        "\n",
        "$${\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2}},$$\n",
        "\n",
        "with $\\bar{X} = \\sum_{i=1}^n \\frac{x_i}{n}$.\n",
        "\n",
        "## Example 1 (cont.)\n",
        "\n",
        "We use the five numeric predictors from the `complete_sbAuto` dataset.\n"
      ],
      "id": "16339b6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "#complete_sbAuto.head()"
      ],
      "id": "ecf03f48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two predictors in original units\n",
        "\n",
        "Consider the previously created `complete_sbAuto` dataset. Consider two points on the graph: $(175, 5140)$ and $(69, 1613)$.\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "f718b3e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "#plt.figure(figsize=(5,5))\n",
        "#sns.scatterplot(data = complete_sbAuto, x = 'horsepower', y = 'weight')\n",
        "#plt.scatter(x = 175, y = 5140, color = 'red')\n",
        "#plt.scatter(x = 69, y = 1613, color = 'red')\n",
        "#plt.xlabel('Horsepower', fontsize=14)\n",
        "#plt.ylabel('Weight', fontsize=14)\n",
        "#plt.show()"
      ],
      "id": "b074040e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "</br>\n",
        "\n",
        "The distance between these points is $\\sqrt{(69 - 175)^2 + (1613-5140)^2}$ $= \\sqrt{11236 + 12439729}$ $= 3528.592$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Standardization in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "To standardize **numeric** predictors, we use the `StandardScaler()` function. We also apply the function to variables using the `fit_transform()` function.\n",
        "\n",
        "</br>\n"
      ],
      "id": "e3eb8239"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "#scaler = StandardScaler()\n",
        "#Xs = scaler.fit_transform(complete_sbAuto)"
      ],
      "id": "141ed6a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Unfortunately, the resulting object isn't a Pandas data frame. So, we converted it to this format.\n"
      ],
      "id": "f2e0c512"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "#scaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)\n",
        "#scaled_df.head()"
      ],
      "id": "d38d4900",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two predictors in standardized units\n",
        "\n",
        "On the new scale, the two points are now: $(1.82, 2.53)$ and $(-0.91, -1.60)$.\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "88958565"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "#plt.figure(figsize=(5,5))\n",
        "#sns.scatterplot(data = scaled_df, x = 'horsepower', y = 'weight')\n",
        "#plt.scatter(x = 1.83, y = 2.54, color = 'red')\n",
        "#plt.scatter(x = -0.90, y = -1.60, color = 'red')\n",
        "#plt.xlabel('Standardized horsepower', fontsize=14)\n",
        "#plt.ylabel('Standardized weight', fontsize=14)\n",
        "#plt.show()"
      ],
      "id": "37b43387",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "</br>\n",
        "\n",
        "The distance between these points is $\\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}$ $= \\sqrt{7.45 + 17.05} = 4.95$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Discussion\n",
        "\n",
        "*K*-NN is intuitive and simple and can produce decent predictions. However, *K*-NN has some disadvantages:\n",
        "\n",
        "- When the training dataset is very large, *K*-NN is computationally expensive. This is because, to predict an observation, we need to calculate the distance between that observation and all the others in the dataset. (\"Lazy learner\").\n",
        "\n",
        "- In this case, a decision tree is more advantageous because it is easy to build, store, and make predictions with.\n",
        "\n",
        "##\n",
        "\n",
        "- The predictive performance of *K*-NN deteriorates as the number of predictors increases.\n",
        "\n",
        "- This is because the expected distance to the nearest neighbor increases dramatically with the number of predictors, unless the size of the dataset increases exponentially with this number.\n",
        "\n",
        "- This is known as the ***curse of dimensionality***.\n",
        "\n",
        "![](images/clipboard-72810347.png)\n",
        "\n",
        "<https://aiaspirant.com/curse-of-dimensionality/>\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)"
      ],
      "id": "5c62c1f2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/alanrvazquez/.virtualenvs/r-reticulate/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
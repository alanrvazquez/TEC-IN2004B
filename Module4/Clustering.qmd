---
title: "Clustering Methods"
subtitle: "IN2004B: Generation of Value with Data Analytics"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: false
    footer: "Tecnologico de Monterrey"
    logo: IN2004B_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Agenda

</br>

1.  Unsupervised Learning

2.  Clustering Methods

3.  K-Means Method

4.  Hierarchical Clustering

# Unsupervised Learning

## Load the libraries

Before we start, let's import the data science libraries into Python.

```{python}
#| echo: true
#| output: false

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
```

Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn** and **sklearn** libraries in Python.

## Types of learning

</br></br>

In data science, there are two main types of learning:

-   [Supervised learning]{style="color:blue;"}. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.

-   [Unsupervised learning]{style="color:green;"}. In which we have only multiple predictors. The goal is to discover patterns in your data.

## Types of learning

</br></br>

In data science, there are two main types of learning:

-   [Supervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.]{style="color:gray;"}

-   [Unsupervised learning]{style="color:green;"}. In which we have only multiple predictors. The goal is to discover patterns in your data.

## Unsupervised learning

**Goal**: organize or *group* data to gain insights. It answers questions like these

-   Is there an informative way to visualize the data?
-   Can we discover subgroups among variables or observations?

. . .

Unsupervised learning is more challenging than supervised learning because it is [**subjective**]{style="color:darkgreen;"} and there is no simple objective for the analysis, such as predicting a response.

. . .

It is also known as *exploratory data analysis*.

## Examples of Unsupervised Learning

</br>

-   *Marketing.* Identify a segment of customers with a high tendency to purchase a specific product.

-   *Retail.* Group customers based on their preferences, style, clothing choices, and store preferences.

-   *Medical Science.* Facilitate the efficient diagnosis and treatment of patients, as well as the discovery of new drugs.

-   *Sociology.* Classify people based on their demographics, lifestyle, socioeconomic status, etc.

## Unsupervised learning methods

</br></br>

-   [**Clustering Methods**]{style="color:#8B004F;"} aim to find subgroups with similar data in the database.

-   [**Principal Component Analysis**]{style="color:#017373;"} seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.

Here we will use these methods on predictors $X_1, X_2, \ldots, X_p$, which are *numerical*.

## Unsupervised learning methods

</br></br>

-   [**Clustering Methods**]{style="color:#8B004F;"} aim to find subgroups with similar data in the database.

-   [**Principal Component Analysis** seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.]{style="color:gray;"}

Here we will use these methods on predictors $X_1, X_2, \ldots, X_p$, which are *numerical*.

# Clustering Methods

## Clustering methods

They group data in different ways to discover groups with common traits.

![](images/clipboard-4025099075.png){fig-align="center"}

## Clustering methods

</br></br>

Two classic clustering methods are:

-   [**K-Means Method**]{style="color:pink;"}. We seek to divide the observations into *K* groups.

-   [**Hierarchical Clustering**]{style="color:darkpink;"}. We divide the *n* observations into 1 group, 2 groups, 3 groups, ..., up to *n* groups. We visualize the divisions using a graph called a **dendrogram**.

## Example 1

The “penguins.xlsx” database contains data on 342 penguins in Antarctica. The data includes:

:::::: center
::::: columns
::: {.column width="50%"}
-   Bill length in millimeters.
-   Bill depth in millimeters.
-   Flipper length in millimeters.
-   Body mass in grams.
:::

::: {.column width="50%"}
![](images/clipboard-2240851715.png){fig-align="center" width="163" height="372"}
:::
:::::
::::::

## Data

```{python}
#| output: true
#| echo: true

penguins_data = pd.read_excel("penguins.xlsx")
penguins_data.head()
```

## Data visualization

Can we group penguins based on their characteristics?

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: true 

plt.figure(figsize=(8, 5)) # Set figure size.
sns.scatterplot(data=penguins_data, x="bill_depth_mm", y="bill_length_mm") # Define type of plot.
plt.show() # Display the plot.
```

# K-Means Method

## The K-Means method

</br>

**Goal**: Find *K* groups of observations such that each observation is in a different group.

![](images/clipboard-3145794211.png)

## 

</br></br>

For this, the method requires two elements:

::: incremental
1.  A measure of "closeness" between observations.

2.  An algorithm that groups observations that are close to each other.
:::

. . .

Good clustering is one in which observations within a group are close together and observations in different groups are far apart.

## How do we measure the distance between observations?

For quantitative predictors, we use the **Euclidean distance**.

For example, if we have two predictors $X_1$ and $X_2$ with observations given in the table:

| Observation | $X_1$     | $X_2$     |
|-------------|-----------|-----------|
| 1           | $X_{1,1}$ | $X_{1,2}$ |
| 2           | $X_{2,1}$ | $X_{2,2}$ |

## Euclidean distance

</br>

![](images/distancia_euclideana.png){fig-align="center"}

::: center
$$d = \sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 }$$
:::


## 

We can extend the Euclidean distance to measure the distance between observations when we have more predictors. For example, with 3 predictors we have

| Observation | $X_1$      | $X_2$      | $X_3$      |
|-------------|------------|------------|------------|
| 1           | $X_{1,1}$ | $X_{1,2}$ | $X_{1,3}$ |
| 2           | $X_{2,1}$ | $X_{2,2}$ | $X_{2,3}$ |

</br>

Where the Euclidean distance is

$$d = \sqrt{(X_{1,1} - X_{2,1})^2 + (X_{1,2} - X_{2,2})^2 + (X_{1,3} - X_{2,3})^2 }$$

## Problem with Euclidean distance

</br>

- The Euclidean distance depends on the units of measurement of the predictors!

- Predictors with certain units have greater importance in calculating the distance.

- This is not good since we want all predictors to have equal importance when calculating the Euclidean distance between two observations.

- The solution is to **standardize** the units of the predictors.

## K-Means Algorithm

:::::: center
::::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
Choose a value for *K*, the number of groups.

1. Randomly assign observations to one of the *K* groups.
2. Find the *centroids* (average points) of each group.
3. Reassign observations to the group with the closest centroid.
4. Repeat steps 3 and 4 until there are no more changes.
:::
:::

::: {.column width="50%"}
![](images/clipboard-1847659123.png){fig-align="center"}
:::
:::::
::::::

## Example 1 (cont.)

Let's apply the algorithm to the predictors `bill_depth_mm` and  `bill_length_mm` of the penguins dataset.

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: true 

X = penguins_data.filter(['bill_depth_mm', 'bill_length_mm'])
X.head()
```

##

</br></br>

In Python, we use the `KMeans()` function of **sklearn** to apply K-means clustering. `KMeans()` tells Python we want to train a K-means clustering algorithm and `.fit_predict()` actually trains it using the data.

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: false 

# Fit KMeans with 3 clusters
kmeans = KMeans(n_clusters = 3, random_state = 301655)
clusters = kmeans.fit_predict(X)
```

The argument `n_clusters` sets the desired number of clusters and `random_state` allows us to reproduce the analysis.

##

The clusters created are contained in the `clusters` object.

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: true 

clusters
```

## 

To visualize the clusters, we can augment the dataset `X` with the `clusters` object. usign the code below.

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: false 

clustered_X = (X
              .assign(Cluster = clusters)
              )

clustered_X.head()
```

## 

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: true 

plt.figure(figsize=(9, 6))
sns.scatterplot(data = clustered_X, x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'Cluster', palette = 'Set1')
plt.title('K-means Clustering of Penguins')
plt.xlabel('Bill Length (mm)')
plt.ylabel('Bill Depth (mm)')
plt.tight_layout()
plt.show()
```

## The truth: 3 groups of penguins

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: true 

plt.figure(figsize=(9, 6))
sns.scatterplot(data=penguins_data, x="bill_depth_mm", y="bill_length_mm",
                hue="species", palette = 'Set1') # Define type of plot.
plt.show() # Display the plot.
```

##

</br>

::::::: center
:::::: columns
::: {.column width="50%"}
```{python}
#| output: true
#| echo: false
#| fig-align: center
#| code-fold: false 

plt.figure(figsize=(5, 5))
sns.scatterplot(data = clustered_X, x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'Cluster', palette = 'Set1')
plt.title('K-means Clustering of Penguins')
plt.xlabel('Bill Length (mm)')
plt.ylabel('Bill Depth (mm)')
plt.tight_layout()
plt.show()
```
:::

::: {.column width="50%"}
```{python}
#| output: true
#| echo: false
#| fig-align: center
#| code-fold: false 

plt.figure(figsize=(6, 6))
sns.scatterplot(data=penguins_data, x="bill_depth_mm", y="bill_length_mm",
                hue="species", palette = 'Set1') # Define type of plot.
plt.show() # Display the plot.
```
:::
:::
:::

## Let's try using more predictors

</br>

```{python}
#| output: true
#| echo: true
#| fig-align: center
#| code-fold: false 

X = penguins_data.filter(['bill_depth_mm', 'bill_length_mm', 
                          'flipper_length_mm', 'body_mass_g'])

# Fit KMeans with 3 clusters
kmeans = KMeans(n_clusters = 3, random_state = 301655)
clusters = kmeans.fit_predict(X)

# Save new clusters
clustered_X = (X
              .assign(Cluster = clusters)
              )
```


##

::::::: center
:::::: columns
::: {.column width="50%"}
```{python}
#| output: true
#| echo: false
#| fig-align: center
#| code-fold: false 

plt.figure(figsize=(5, 5))
sns.scatterplot(data = clustered_X, x = 'bill_length_mm', y = 'bill_depth_mm', hue = 'Cluster', palette = 'Set1')
plt.title('K-means Clustering of Penguins')
plt.xlabel('Bill Length (mm)')
plt.ylabel('Bill Depth (mm)')
plt.tight_layout()
plt.show()
```
:::

::: {.column width="50%"}
```{python}
#| output: true
#| echo: false
#| fig-align: center
#| code-fold: false 

plt.figure(figsize=(6, 6))
sns.scatterplot(data=penguins_data, x="bill_depth_mm", y="bill_length_mm",
                hue="species", palette = 'Set1') # Define type of plot.
plt.show() # Display the plot.
```
:::
:::
:::


## These are the three species

::::::: center
:::::: columns
::: {.column width="33%"}
Adelie

![](images/clipboard-1367554877.png)
:::

::: {.column width="33%"}
Gentoo

![](images/clipboard-3518959291.png)
:::

::: {.column width="33%"}
Chinstrap

![](images/clipboard-2663292782.png)
:::
::::::
:::::::

## Comments

- Selecting the number of clusters *K* is more of an art than a science. You'd better get *K* right, or you'll be detecting patterns where none really exist.

- We need to standardize all predictors.

- The performance of *K*-means clustering is affected by the presence of outliers.

- The algorithm's solution is sensitive to the starting point. Because of this, it is typically run multiple times, and the best clustering among all runs is reported.

# Agrupación Jerárquica

## Agrupación Jerárquica

:::::: center
::::: columns
::: {.column width="60%"}
-   Comienza con cada observación por sí sola en su propio grupo.

-   Luego, fusiona gradualmente los grupos que están cerca unos de otros.

-   Continuamos este proceso hasta que todas las observaciones estén en un grupo grande.

-   Finalmente, damos un paso atrás y vemos qué agrupación funciona mejor.
:::

::: {.column width="40%"}
![](images/clipboard-2325345248.png)
:::
:::::
::::::

## Componentes Esenciales

</br>

1.  Distancia entre dos observaciones.

-   Usamos la distancia euclidiana.
-   Debemos de estandarizar los predictores!

2.  Distancia entre [**dos grupos**]{style="color:darkgreen;"}.

## Distancia entre grupos.

:::::: center
::::: columns
::: {.column width="60%"}
La distancia entre dos grupos de observaciones se llama [***vinculación***]{style="color:pink;"}.

Hay varios tipos de vinculación. Los más usados son

-   Vinculación completa
-   Vinculación promedio
:::

::: {.column width="40%"}
![](images/vinculacion.png)
:::
:::::
::::::

## Vinculación Completa

La distancia entre grupos se mide utilizando la mayor distancia entre observaciones.

![](images/completa.png)

## Promedio

La distancia entre grupos es el promedio de todas las distancias entre observaciones.

![](images/promedio.png)

## Algoritmo de Agrupamiento Jerárquico

Los pasos del algoritmo son los siguientes:

1.  Asigna cada observación a un grupo.
2.  Mide el vínculo entre todos los grupos.
3.  Fusiona los dos grupos que sean más similares.
4.  Luego, fusiona los dos siguientes grupos que sean más similares.
5.  Continua hasta que todos los grupos hayan sido fusionados.

## Ejemplo

Consideremos un conjunto de datos en el archivo “Cereals.xlsx”. Los datos incluyen información nutricional de 77 cereales, entre otros datos.

## En Python

## Resultados: Dendrograma

:::::: center
::::: columns
::: {.column width="60%"}
-   Un dendrograma es un diagrama de arbol que resume y visualiza el proceso de agrupamiento.
-   Las observaciones estan en el eje horizontal y en la parte inferior del diagrama.
-   El eje vertical muestra la distancia entre los grupos.
-   Se lee de arriba a abajo.
:::

::: {.column width="40%"}
![](images/clipboard-2041051251.png)
:::
:::::
::::::

## ¿Qué hacer con un dendrograma?

:::::: center
::::: columns
::: {.column width="60%"}
Dibujamos una linea horizontal a una altura específica para definir los grupos.

Esta linea define 3 grupos.
:::

::: {.column width="40%"}
![](images/dendrograma1.png)
:::
:::::
::::::

## 

:::::: center
::::: columns
::: {.column width="60%"}
Esta linea define 5 grupos.
:::

::: {.column width="40%"}
![](images/dendrograma2.png)
:::
:::::
::::::

## Dendrograma en Python

## Comentarios

-   Recuerda que debemos estandarizar los predictores!

-   No es sencillo elegir el número correcto de grupos usando el dendrograma.

-   Los resultados dependen de la medida de vinculación utilizada.

    -   La vinculación completa resulta en grupos más estrechos.
    -   La vinculación promedio logra un equilibrio entre grupos estrechos y más delgados.

-   La agrupación jerárquica es útil para detectar valores atípicos.

## 

</br> </br>

> *Con estos métodos, no existe una única respuesta correcta; se debe considerar cualquier solución que exponga algunos aspectos interesantes de los datos.*

James et al. (2017)

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)

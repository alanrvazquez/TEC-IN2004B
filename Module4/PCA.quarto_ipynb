{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Principal Component Analysis\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN1002b_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Introducción\n",
        "2.  Dispersión en una o varias dimensiones\n",
        "3.  Análisis de componentes principales\n",
        "\n",
        "# Introducción\n",
        "\n",
        "## Tipos de Aprendizaje\n",
        "\n",
        "En ciencia de datos, existen dos tipos principales de aprendizaje:\n",
        "\n",
        "-   [Aprendizaje supervisado (*supervised learning*). En el cual tenemos varios predictores y una respuesta. El objetivo es predecir la respuesta usando los valores de los predictores.]{style=\"color:gray;\"}\n",
        "\n",
        "-   [Aprendizaje sin supervisión (*unsupervised learning*)]{style=\"color:green;\"}. En el cual solo tenemos varios predictores. El objetivo es descubrir patrones en sus datos.\n",
        "\n",
        "## Métodos de aprendizaje sin supervisión\n",
        "\n",
        "[Los **Métodos de Agrupamiento** tienen como objetivo encontrar subgrupos con datos similares en la base de datos.]{style=\"color:gray;\"}\n",
        "\n",
        "El [**Análisis de Componentes Principales**]{style=\"color:aqua;\"} busca una representación alternativa de los datos para facilitar su comprensión cuando hay muchos predictores en la base de datos.\n",
        "\n",
        "Aquí nos usaremos estos métodos en predictores $X_1, X_2, \\ldots, X_p$ que son numéricos.\n",
        "\n",
        "# Dispersión en una o varias dimensiones\n",
        "\n",
        "## Dispersión en una dimensión\n",
        "\n",
        "El concepto de componentes principales requiere de entender la dispersión o variabilidad de los datos.\n",
        "\n",
        "Supongamos que tenemos datos de un solo predictor.\n",
        "\n",
        "::: center\n",
        "\\[***Observaciones de un predictor***\\]\n",
        ":::\n",
        "\n",
        "![](images/unadispersion.png)\n",
        "\n",
        "## Dispersión en dos dimensiones\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/dosdispersion1.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/dosdispersion2.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Capturando dispersión\n",
        "\n",
        "En algunos casos, podemos capturar la dispersión de datos en dos dimensiones (predictores) usando una sola dimensión.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/capturedispersion1.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Capturando dispersión\n",
        "\n",
        "En algunos casos, podemos capturar la dispersión de datos en dos dimensiones (predictores) usando una sola dimensión.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/capturedispersion2.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "Un solo predictor $X_2$ captura gran parte de la dispersión en los datos.\n",
        "\n",
        "![](images/capturedispersion3.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Veamos otro ejemplo\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/otroejemplo1.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Veamos otro ejemplo\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/otroejemplo1.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "Un solo predictor captura gran parte de la dispersión en los datos. En este caso, el nuevo predictor tiene la forma $Z_1 = a X_1 + b X_2 + c.$\n",
        "\n",
        "![](images/otroejemplo2.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "Alternativamente, podemos utilizar dos dimensiones alternativas para capturar la dispersión.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/alternativas1.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "Un solo predictor captura gran parte de la dispersión en los datos. En este caso, el nuevo predictor tiene la forma $Z_1 = a X_1 + b X_2 + c.$\n",
        "\n",
        "![](images/alternativas2.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Un Nuevo Eje de Coordenadas\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   El nuevo eje de coordenadas esta dado por dos nuevos predictores $Z_1$ y $Z_2$. Los dos son dados por ecuaciones lineales de los nuevos predictores.\n",
        "\n",
        "-   El primer eje $Z_1$ captura gran porción de la dispersión, mientras que $Z_2$ captura poca porción desde otro ángulo.\n",
        "\n",
        "-   Los nuevos ejes $Z_1$ y $Z_2$ se llaman [***componentes principales***]{style=\"color:DARKgray;\"}.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/alternativas3.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "# Análisis de componentes principales\n",
        "\n",
        "## Reducción de Dimension\n",
        "\n",
        "</br>\n",
        "\n",
        "El [**Análisis de Componentes Principales**]{style=\"color:aqua;\"} (ACP) nos ayuda a reducir la dimensión de los datos.\n",
        "\n",
        "-   Crea un nuevo eje de coordenadas en dos (o más) dimensiones.\n",
        "\n",
        "-   Técnicamente, crea nuevos predictores combinando predictores altamente [*correlacionados*]{.underline}. Los nuevos predictores no están correlacionados.\n",
        "\n",
        "## Preparación\n",
        "\n",
        "**Paso 1**. Comenzamos con una base de datos con $n$ observaciones y $p$ predictores.\n",
        "\n",
        "| **Predictor 1** | **Predictor 2** | **Predictor 3** |\n",
        "|-----------------|-----------------|-----------------|\n",
        "| 15              | 14              | 5               |\n",
        "| 2               | 1               | 6               |\n",
        "| 10              | 3               | 17              |\n",
        "| 8               | 18              | 9               |\n",
        "| 12              | 16              | 11              |\n",
        "\n",
        "## \n",
        "\n",
        "**Paso 2**. Estandarizamos cada predictor individualmente.\n",
        "\n",
        "$${\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}$$\n",
        "\n",
        "|              | Predictor 1 | Predictor 2 | Predictor 3 |\n",
        "|--------------|-------------|-------------|-------------|\n",
        "|              | 1.15        | 0.46        | -0.96       |\n",
        "|              | -1.52       | -1.20       | -0.75       |\n",
        "|              | 0.12        | -0.95       | 1.55        |\n",
        "|              | -0.29       | 0.97        | -0.13       |\n",
        "|              | 0.53        | 0.72        | 0.29        |\n",
        "| **Suma**     | *0*         | *0*         | *0*         |\n",
        "| **Varianza** | *1*         | *1*         | *1*         |\n",
        "\n",
        "## \n",
        "\n",
        "**Paso 3**. Asumimos que la base de datos estandarizada es una matrix $\\mathbf{X}$ de $n\\times p$.\n",
        "\n",
        "$$\\mathbf{X} = \\begin{pmatrix}\n",
        "1.15    &   0.46    &   -0.96   \\\\\n",
        "-1.52   &   -1.20   &   -0.75   \\\\\n",
        "0.12    &   -0.95   &   1.55    \\\\\n",
        "-0.29   &   0.97    &   -0.13   \\\\\n",
        "0.53    &   0.72    &   0.29    \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "## Algoritmo\n",
        "\n",
        "</br>\n",
        "\n",
        "El algoritmo de ACP tiene su origen en el álgebra lineal.\n",
        "\n",
        "Su idea básica es:\n",
        "\n",
        "1.  Crear una matriz $\\mathbf{C}$ con las correlaciones entre los predictores de la matriz $\\mathbf{X}$.\n",
        "\n",
        "2.  Partir la matrix $\\mathbf{C}$ en tres partes que nos dan el nuevo eje de coordenadas y la importancia de cada eje.\n",
        "\n",
        "## Matriz de Correlación\n",
        "\n",
        "Siguiendo con nuestro ejemplo, la matriz de correlación contiene las correlaciones entre dos columnas de $\\mathbf{X}$.\n",
        "\n",
        "![](images/correlations.png)\n",
        "\n",
        "## Partición de la matriz de correlación\n",
        "\n",
        "La partición de la matrix $\\mathbf{C}$ se hace usando el [*método de descomposición por valores y vectores propios*]{style=\"color:aqua;\"}.\n",
        "\n",
        "![](images/descomposition1.png)\n",
        "\n",
        "## \n",
        "\n",
        "![](images/descomposition2.png)\n",
        "\n",
        "-   Las columnas de $\\mathbf{B}$ definen los ejes del nuevo sistema de coordenadas. Estos ejes se llaman [***componentes principales***]{style=\"color:green;\"}.\n",
        "\n",
        "-   Los valores diagonales en $\\mathbf{A}$ definen la importancia individual de cada componente principal (eje).\n",
        "\n",
        "## Proporción de la dispersión explicada por el componente\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "$$\\mathbf{A} = \\begin{pmatrix}\n",
        "1.60     &  0.00    &   0.00    \\\\\n",
        "0.00     &  1.07     &  0.00    \\\\\n",
        "0.00     &  0.00     &  0.33    \\\\\n",
        "\\end{pmatrix}$$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "La proporción de la dispersión en los datos que es capturada por el primer componente es $\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53$. La proporción capturada por el segundo componente es $\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36$. La proporción capturada por el tercer componente es $\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11$.\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Comentarios\n",
        "\n",
        "Los componentes principales se pueden usar para aproximar una matriz.\n",
        "\n",
        "Por ejemplo, podemos aproximar la matriz $\\mathbf{C}$ al fijar el tercer componente igual a cero.\n",
        "\n",
        "$$\\begin{pmatrix}\n",
        "-0.68   &   0.35    &   0.00    \\\\\n",
        "-0.72   &   -0.13   &   0.00    \\\\\n",
        "0.16    &   0.93    &   0.00\\\\\n",
        "\\end{pmatrix} \\begin{pmatrix}\n",
        "1.60     &  0.00    &   0.00    \\\\\n",
        "0.00     &  1.07     &  0.00    \\\\\n",
        "0.00     &  0.00     &  0.00    \\\\\n",
        "\\end{pmatrix} \\begin{pmatrix}\n",
        "-0.68   &   -0.72   &   0.16    \\\\\n",
        "0.35    &   -0.13   &   0.93    \\\\\n",
        "0.00    &   0.00    &   0.00    \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "0.86    &   0.73    &   0.18    \\\\\n",
        "0.73    &   0.85    &   -0.30   \\\\\n",
        "0.18    &   -0.30   &   0.96    \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "$$\\approx \\begin{pmatrix}\n",
        "1.00    &   0.58    &   0.11    \\\\\n",
        "0.58    &   1.00    &   -0.23   \\\\\n",
        "0.11    &   -0.23   &   1.00    \\\\\n",
        "\\end{pmatrix} = \\mathbf{C}$$ \\##\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "-   Las aproximaciones son útiles para almacenar matrices grandes.\n",
        "\n",
        "-   Esto se porque solo necesitamos almacenar los valores propios más grandes y sus vectores propios correspondientes para recuperar una aproximación de alta calidad de la matriz completa.\n",
        "\n",
        "-   Esta es la idea detrás de la **compresión de imagenes**.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](images/clipboard-2295232858.png)\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Ejemplo\n",
        "\n",
        "Considera una base de datos de las 100 canciones más populares en TikTok. Los datos están en el archivo “TikTok 2020 reduced.xlsx“. Se tienen observaciones de varios predictores como:\n",
        "\n",
        "- La bailabilidad describe qué tan adecuada es una pista para bailar basándose en una combinación de elementos musicales.\n",
        "\n",
        "- La energía es una medida de 0 a 1 y representa una medida perceptiva de intensidad y actividad.\n",
        "\n",
        "- El volumen general de una pista en decibelios (dB). Los valores de sonoridad se promedian en toda la pista.\n",
        "\n",
        "- El habla detecta la presencia de palabras habladas en una pista. Cuanto más exclusivamente parecida a un discurso sea la grabación.\n",
        "\n",
        "- Una medida de confianza de 0 a 1 sobre si la pista es acústica.\n",
        "\n",
        "- Detecta la presencia de una audiencia en la grabación.\n",
        "\n",
        "- Una medida de 0 a 1 que describe la positividad musical que transmite una pista.\n",
        "\n",
        "## APC en Python\n",
        "\n",
        "## \n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "- El **Scree o Summary Plot** te dice la variabilidad capturada por cada componente. Dicha variabilidad es dada por el *Eigen value* o valor propio. De 1 a 8 componentes.\n",
        "\n",
        "- El primer componente abarca la mayor parte de la dispersión de los datos.\n",
        "\n",
        "- Esta gráfica se usa para definir el número total de componentes a usar.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "TBD\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Biplot\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "- Muestra las observaciones gráficas en el nuevo eje de coordenadas dado por los dos primeros componentes. \n",
        "- Ayuda a visualizar los datos de tres o más predictores usando una gáfica de dispersión en 2 dimensiones. \n",
        "- Una linea roja da las dirección de crecimiento de la variable etiquetada. \n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "TBD\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)"
      ],
      "id": "6c64b54d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Principal Component Analysis\"\n",
        "subtitle: \"IN2004B: Generation of Value with Data Analytics\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: false\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN2004B_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Introduction\n",
        "2.  Dispersion in one or more dimensions\n",
        "3.  Principal component analysis\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "Before we start, let's import the data science libraries into Python.\n"
      ],
      "id": "2f0a7929"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "id": "ec8a7b71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use specific functions from the **pandas**, **matplotlib**, **seaborn**, and **sklearn** libraries in Python.\n",
        "\n",
        "## Types of learning\n",
        "\n",
        "</br></br>\n",
        "\n",
        "In data science, there are two main types of learning:\n",
        "\n",
        "-   [Supervised learning. In which we have multiple predictors and one response. The goal is to predict the response using the predictor values.]{style=\"color:gray;\"}\n",
        "\n",
        "-   [Unsupervised learning]{style=\"color:green;\"}. In which we have only multiple predictors. The goal is to discover patterns in your data.\n",
        "\n",
        "## Unsupervised learning methods\n",
        "\n",
        "</br></br>\n",
        "\n",
        "-   [**Clustering Methods** aim to find subgroups with similar data in the database.]{style=\"color:gray;\"}\n",
        "\n",
        "-   [**Principal Component Analysis**]{style=\"color:#017373;\"} seeks an alternative representation of the data to make it easier to understand when there are many predictors in the database.\n",
        "\n",
        "Here we will use these methods on predictors $X_1, X_2, \\ldots, X_p,$ which are *numerical*.\n",
        "\n",
        "# Dispersion in one or more dimensions\n",
        "\n",
        "## Dispersion in one dimension\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The concept of principal components requires an understanding of the dispersion or variability of the data.\n",
        "\n",
        "Suppose we have data for a **single predictor**.\n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/unadispersion.png){fig-align=\"center\"}\n",
        "\n",
        "## Dispersion in two dimensions\n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/dosdispersion1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/dosdispersion2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Capturing dispersion\n",
        "\n",
        "In some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/capturedispersion1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Capturing dispersion\n",
        "\n",
        "In some cases, we can capture the spread of data in two dimensions (predictors) using a single dimension.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/capturedispersion2.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "</br>\n",
        "\n",
        "A single predictor $X_2$ captures much of the spread in the data.\n",
        "\n",
        "![](images/capturedispersion3.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Let's see another example\n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/otroejemplo1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## Let's see another example\n",
        "\n",
        "</br>\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/otroejemplo1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "A single predictor captures much of the dispersion in the data. In this case, the new predictor has the form $Z_1 = a X_1 + b X_2 + c.$\n",
        "\n",
        "![](images/otroejemplo2.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Alternatively, we can use two alternative dimensions to capture the dispersion.\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/alternativas1.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/alternativas3.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## A new coordinate system\n",
        "\n",
        "::::::: center\n",
        ":::::: columns\n",
        ":::: {.column width=\"50%\"}\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "-   The new coordinate axis is given by two new predictors, $Z_1$ and $Z_2$. Both are given by linear equations of the new predictors.\n",
        "\n",
        "-   The first axis, $Z_1$, captures a large portion of the dispersion, while $Z_2$ captures a small portion from another angle.\n",
        "\n",
        "-   The new axes, $Z_1$ and $Z_2$, are called [***principal components***]{style=\"color:darkgray;\"}.\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/alternativas3.png)\n",
        ":::\n",
        "::::::\n",
        ":::::::\n",
        "\n",
        "# Principal Component Analysis\n",
        "\n",
        "## Dimension Reduction\n",
        "\n",
        "</br></br>\n",
        "\n",
        "[**Principal Components Analysis (PCA)**]{style=\"color:#017373;\"} helps us reduce the dimension of the data.\n",
        "\n",
        "::: incremental\n",
        "-   It creates a new coordinate axis in two (or more) dimensions.\n",
        "\n",
        "-   Technically, it creates new predictors by combining highly [*correlated*]{.underline} predictors. The new predictors are uncorrelated.\n",
        ":::\n",
        "\n",
        "## Setup\n",
        "\n",
        "</br>\n",
        "\n",
        "**Step 1**. We start with a database with $n$ observations and $p$ predictors.\n",
        "\n",
        "| **Predictor 1** | **Predictor 2** | **Predictor 3** |\n",
        "|-----------------|-----------------|-----------------|\n",
        "| 15              | 14              | 5               |\n",
        "| 2               | 1               | 6               |\n",
        "| 10              | 3               | 17              |\n",
        "| 8               | 18              | 9               |\n",
        "| 12              | 16              | 11              |\n",
        "\n",
        "## \n",
        "\n",
        "**Step 2**. We standardize each predictor individually.\n",
        "\n",
        "$${\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }}$$\n",
        "\n",
        "|              | Predictor 1 | Predictor 2 | Predictor 3 |\n",
        "|--------------|-------------|-------------|-------------|\n",
        "|              | 1.15        | 0.46        | -0.96       |\n",
        "|              | -1.52       | -1.20       | -0.75       |\n",
        "|              | 0.12        | -0.95       | 1.55        |\n",
        "|              | -0.29       | 0.97        | -0.13       |\n",
        "|              | 0.53        | 0.72        | 0.29        |\n",
        "| **Sum**      | *0*         | *0*         | *0*         |\n",
        "| **Variance** | *1*         | *1*         | *1*         |\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "**Step 3**. We assume that the standardized database is an $n\\times p$ matrix $\\mathbf{X}$.\n",
        "\n",
        "$$\\mathbf{X} = \\begin{pmatrix}\n",
        "1.15    &   0.46    &   -0.96   \\\\\n",
        "-1.52   &   -1.20   &   -0.75   \\\\\n",
        "0.12    &   -0.95   &   1.55    \\\\\n",
        "-0.29   &   0.97    &   -0.13   \\\\\n",
        "0.53    &   0.72    &   0.29    \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "## Algorithm\n",
        "\n",
        "</br>\n",
        "\n",
        "The PCA algorithm has its origins in linear algebra.\n",
        "\n",
        "</br>\n",
        "\n",
        "Its basic idea is:\n",
        "\n",
        "1.  Create a matrix $\\mathbf{C}$ with the correlations between the predictors of the matrix $\\mathbf{X}$.\n",
        "\n",
        "2.  Split the matrix $\\mathbf{C}$ into three parts, which give us the new coordinate axis and the importance of each axis.\n",
        "\n",
        "## Correlation matrix\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Continuing with our example, the correlation matrix contains the correlations between two columns of $\\mathbf{X}$.\n",
        "\n",
        "![](images/correlations.png)\n",
        "\n",
        "## Partitioning the correlation matrix\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The $\\mathbf{C}$ matrix is partitioned using the [*eigenvalue and eigenvector decomposition method*]{style=\"color:#017373;\"}.\n",
        "\n",
        "![](images/descomposition1.png)\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "![](images/descomposition2.png)\n",
        "\n",
        "-   The columns of $\\mathbf{B}$ define the axes of the new coordinate system. These axes are called [***principal components***]{style=\"color:green;\"}.\n",
        "\n",
        "-   The diagonal values in $\\mathbf{A}$ define the individual importance of each principal component (axis).\n",
        "\n",
        "## Proportion of the dispersion explained by the component\n",
        "\n",
        ":::::::: center\n",
        "::::::: columns\n",
        ":::: {.column width=\"50%\"}\n",
        "</br></br>\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "$$\\mathbf{A} = \\begin{pmatrix}\n",
        "1.60     &  0.00    &   0.00    \\\\\n",
        "0.00     &  1.07     &  0.00    \\\\\n",
        "0.00     &  0.00     &  0.33    \\\\\n",
        "\\end{pmatrix}$$\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::: {.column width=\"50%\"}\n",
        "</br></br>\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "The proportion of the dispersion in the data that is captured by the first component is $\\frac{a_{1,1}}{p} = \\frac{1.60}{3} = 0.53$.\n",
        ":::\n",
        "::::\n",
        ":::::::\n",
        "::::::::\n",
        "\n",
        "## \n",
        "\n",
        ":::::::: center\n",
        "::::::: columns\n",
        ":::: {.column width=\"50%\"}\n",
        "</br></br>\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "$$\\mathbf{A} = \\begin{pmatrix}\n",
        "1.60     &  0.00    &   0.00    \\\\\n",
        "0.00     &  1.07     &  0.00    \\\\\n",
        "0.00     &  0.00     &  0.33    \\\\\n",
        "\\end{pmatrix}$$\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::: {.column width=\"50%\"}\n",
        "</br></br>\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "The proportion captured by the second component is $\\frac{a_{2,2}}{p} = \\frac{1.07}{3} = 0.36$.\n",
        "\n",
        "The proportion captured by the third component is $\\frac{a_{3,3}}{p} = \\frac{0.33}{3} = 0.11.$\n",
        ":::\n",
        "::::\n",
        ":::::::\n",
        "::::::::\n",
        "\n",
        "## Comments\n",
        "\n",
        "</br>\n",
        "\n",
        "Principal components can be used to approximate a matrix.\n",
        "\n",
        "For example, we can approximate the matrix $\\mathbf{C}$ by setting the third component equal to zero.\n",
        "\n",
        "::: {style=\"font-size: 60%;\"}\n",
        "$$\\begin{pmatrix}\n",
        "-0.68   &   0.35    &   0.00    \\\\\n",
        "-0.72   &   -0.13   &   0.00    \\\\\n",
        "0.16    &   0.93    &   0.00\\\\\n",
        "\\end{pmatrix} \\begin{pmatrix}\n",
        "1.60     &  0.00    &   0.00    \\\\\n",
        "0.00     &  1.07     &  0.00    \\\\\n",
        "0.00     &  0.00     &  0.00    \\\\\n",
        "\\end{pmatrix} \\begin{pmatrix}\n",
        "-0.68   &   -0.72   &   0.16    \\\\\n",
        "0.35    &   -0.13   &   0.93    \\\\\n",
        "0.00    &   0.00    &   0.00    \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "0.86    &   0.73    &   0.18    \\\\\n",
        "0.73    &   0.85    &   -0.30   \\\\\n",
        "0.18    &   -0.30   &   0.96    \\\\\n",
        "\\end{pmatrix}$$\n",
        ":::\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}\n",
        "$$\\approx \\begin{pmatrix}\n",
        "1.00    &   0.58    &   0.11    \\\\\n",
        "0.58    &   1.00    &   -0.23   \\\\\n",
        "0.11    &   -0.23   &   1.00    \\\\\n",
        "\\end{pmatrix} = \\mathbf{C}$$\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::::::: center\n",
        ":::::: columns\n",
        ":::: {.column width=\"40%\"}\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "-   Approximations are useful for storing large matrices.\n",
        "\n",
        "-   This is because we only need to store the largest eigenvalues and their corresponding eigenvectors to recover a high-quality approximation of the entire matrix.\n",
        "\n",
        "-   This is the idea behind **image compression**.\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](images/clipboard-2295232858.png)\n",
        ":::\n",
        "::::::\n",
        ":::::::\n",
        "\n",
        "## Example 1\n",
        "\n",
        "</br>\n",
        "\n",
        "Consider a database of the 100 most popular songs on TikTok. The data is in the file “TikTok 2020 reduced.xlsx”. There are observations of several predictors, such as:\n",
        "\n",
        "-   Danceability describes how suitable a track is for dancing based on a combination of musical elements.\n",
        "\n",
        "-   Energy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity.\n",
        "\n",
        "-   The overall volume of a track in decibels (dB). Loudness values are averaged across the entire track.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "Other predictors are:\n",
        "\n",
        "-   Speech detects the presence of spoken words in a track. The more exclusively speech-like the recording is.\n",
        "\n",
        "-   A confidence measure from 0 to 1 about whether the track is acoustic.\n",
        "\n",
        "-   Detects the presence of an audience in the recording.\n",
        "\n",
        "-   A measure from 0 to 1 that describes the musical positivity a track conveys.\n",
        "\n",
        "## The data\n"
      ],
      "id": "dda3f43d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "\n",
        "tiktok_data = pd.read_excel(\"TikTok_Songs_2020_Reduced.xlsx\")\n",
        "tiktok_data.head()"
      ],
      "id": "3464e38d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize the data\n",
        "\n",
        "</br>\n",
        "\n",
        "Remember that PCA works with distances, so we must standardize the quantitative predictors to have an accurate analysis.\n"
      ],
      "id": "102ea28d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "\n",
        "# Select the predictors\n",
        "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
        "            'acousticness', 'liveness', 'valence', 'tempo']\n",
        "X_tiktok = tiktok_data.filter(features)  \n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "Xs_tiktok = scaler.fit_transform(X_tiktok)"
      ],
      "id": "528e50a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA in Python\n",
        "\n",
        "</br></br>\n",
        "\n",
        "We tell Python that we want to apply PCA using the function `PCA()` from **sklearn**. Next, we run the algorithm using `.fit_transform()`.\n"
      ],
      "id": "398146d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "\n",
        "pca = PCA()\n",
        "PCA_tiktok = pca.fit_transform(Xs_tiktok)"
      ],
      "id": "0801d91b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   The **Screen or Summary Plot** tells you the variability captured by each component. This variability is given by the *Eigenvalue*. From 1 to 8 components.\n",
        "\n",
        "-   The first component covers most of the data dispersion.\n",
        "\n",
        "-   This graph is used to define the total number of components to use.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "c116b9e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: false\n",
        "#| fig-align: center\n",
        "\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(range(1, len(explained_var) + 1), explained_var, marker='o', linestyle='-')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xticks(range(1, len(explained_var) + 1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4cad1274",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "The code to generate a scree plot is below.\n"
      ],
      "id": "b9fd39ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: true\n",
        "#| fig-align: center\n",
        "\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(range(1, len(explained_var) + 1), explained_var, \n",
        "         marker='o', linestyle='-')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xticks(range(1, len(explained_var) + 1))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "8ac5d3e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Biplot\n",
        "\n",
        ":::::: center\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   Displays the graphical observations on the new coordinate axis given by the first two components.\n",
        "-   Helps visualize data for three or more predictors using a two-dimensional scatter plot.\n",
        "-   A red line indicates the growth direction of the labeled variable.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "3d486ee6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: false\n",
        "#| fig-align: center\n",
        "\n",
        "# Create a DataFrame with the PCA results\n",
        "pca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\n",
        "\n",
        "# Create biplot of first two principal components\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\n",
        "\n",
        "# Add variable vectors\n",
        "loadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\n",
        "for i, feature in enumerate(features):\n",
        "    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n",
        "              color='red', alpha=0.5, head_width=0.05)\n",
        "    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Biplot')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.show()"
      ],
      "id": "b3ad35e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::::\n",
        "::::::\n",
        "\n",
        "## \n",
        "\n",
        "The code to generate the biplot is lenghty but it can be broken into three steps.\n",
        "\n",
        "Step 1. Create a DataFrame with the PCA results\n"
      ],
      "id": "0cb9fae9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "#| fig-align: center\n",
        "\n",
        "pca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\n",
        "pca_df.head()"
      ],
      "id": "f1000e36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Step 2. Create biplot of first two principal components\n"
      ],
      "id": "7026ecaa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "plt.figure(figsize=(10, 5.5))\n",
        "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Biplot')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.show()"
      ],
      "id": "c5fd6ca6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Step 3. Add more information to the biplot.\n"
      ],
      "id": "8959a38b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "#| fig-align: center\n",
        "#| code-fold: true\n",
        "\n",
        "plt.figure(figsize=(10, 5.5))\n",
        "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Biplot')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Add variable vectors\n",
        "loadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\n",
        "for i, feature in enumerate(features):\n",
        "    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n",
        "              color='red', alpha=0.5, head_width=0.05)\n",
        "    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "id": "f7fa044e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "With some extra lines of code, we label the points in the plot.\n"
      ],
      "id": "315c7372"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| echo: true\n",
        "#| fig-align: center\n",
        "#| code-fold: true\n",
        "\n",
        "pca_df = pd.DataFrame(PCA_tiktok, columns=[f'PC{i+1}' for i in range(PCA_tiktok.shape[1])])\n",
        "pca_df = (pca_df\n",
        "          .assign(songs = tiktok_data['track_name'])\n",
        "          )\n",
        "\n",
        "plt.figure(figsize=(10, 5.5))\n",
        "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], alpha=0.7)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Biplot')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Add labels for each song\n",
        "for i in range(pca_df.shape[0]):\n",
        "    plt.text(pca_df['PC1'][i] + 0.1, pca_df['PC2'][i] + 0.1,\n",
        "             pca_df['songs'][i], fontsize=8, alpha=0.7)\n",
        "\n",
        "\n",
        "# Add variable vectors\n",
        "loadings = pca.components_.T[:, :2]  # loadings for PC1 and PC2\n",
        "for i, feature in enumerate(features):\n",
        "    plt.arrow(0, 0, loadings[i, 0]*3, loadings[i, 1]*3,\n",
        "              color='red', alpha=0.5, head_width=0.05)\n",
        "    plt.text(loadings[i, 0]*3.2, loadings[i, 1]*3.2, feature, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "id": "1e97624c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN2004B/)"
      ],
      "id": "512c9b74"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}